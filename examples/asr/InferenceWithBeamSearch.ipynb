{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this is NEMO's \"core\" package\n",
    "import nemo\n",
    "# this is NEMO's ASR collection of speech recognition related neural modules\n",
    "import nemo_asr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path to the data on which you want to run inference\n",
    "inference_manifest = \"<path to json manifest>\"\n",
    "\n",
    "# Import Jasper model definition\n",
    "# Note that we are using a much larger 15x5 model now instead of 12x1\n",
    "from ruamel.yaml import YAML\n",
    "yaml = YAML(typ=\"safe\")\n",
    "with open(\"<nemo_root>/examples/asr/configs/quartznet15x5.yaml\") as f:\n",
    "    jasper_model_definition = yaml.load(f)\n",
    "labels = jasper_model_definition['labels']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First step is to instantiate a NeuralModuleFactory\n",
    "# If torch is installed without CUDA and Apex CPU will be used\n",
    "# and training is impractically slow even for this dataset\n",
    "from nemo.core import DeviceType\n",
    "import torch\n",
    "nf = nemo.core.NeuralModuleFactory(placement=DeviceType.GPU if torch.cuda.is_available() else DeviceType.CPU)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate necessary neural modules\n",
    "data_layer = nemo_asr.AudioToTextDataLayer(\n",
    "    shuffle=False,\n",
    "    manifest_filepath=inference_manifest,\n",
    "    labels=labels, batch_size=64)\n",
    "data_preprocessor = nemo_asr.AudioPreprocessing()\n",
    "jasper_encoder = nemo_asr.JasperEncoder(\n",
    "    feat_in=64,\n",
    "    **jasper_model_definition['JasperEncoder'])\n",
    "jasper_decoder = nemo_asr.JasperDecoderForCTC(feat_in=1024,\n",
    "                                              num_classes=len(labels))\n",
    "greedy_decoder = nemo_asr.GreedyCTCDecoder()\n",
    "\n",
    "# Define inference DAG\n",
    "audio_signal, audio_signal_len, transcripts, transcripts_len = data_layer()\n",
    "processed_signal, processed_signal_len = data_preprocessor(input_signal=audio_signal,\n",
    "                                                           length=audio_signal_len)\n",
    "encoded, encoded_len = jasper_encoder(audio_signal=processed_signal, length=processed_signal_len)\n",
    "log_probs = jasper_decoder(encoder_output=encoded)\n",
    "predictions = greedy_decoder(log_probs=log_probs)\n",
    "\n",
    "eval_tensors=[predictions, transcripts, transcripts_len]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download checkpoint from here: https://ngc.nvidia.com/catalog/models/nvidia:quartznet15x5\n",
    "import os\n",
    "# Instantiate BeamSearch NM\n",
    "beam_search_with_lm = nemo_asr.BeamSearchDecoderWithLM(\n",
    "    vocab=labels,\n",
    "    beam_width=128,\n",
    "    alpha=2.2,\n",
    "    beta=0.5,\n",
    "    lm_path=\"<path_to_lm>/6-gram.binary\",\n",
    "    num_cpus=max(os.cpu_count(), 1))\n",
    "beam_predictions = beam_search_with_lm(log_probs=log_probs, log_probs_length=encoded_len)\n",
    "eval_tensors.append(beam_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nemo_asr.helpers import post_process_predictions, \\\n",
    "                             post_process_transcripts, word_error_rate\n",
    "\n",
    "evaluated_tensors = neural_factory.infer(\n",
    "    tensors=eval_tensors,\n",
    "    checkpoint_dir=\"<path_to_checkpoint>\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hypotheses = post_process_predictions(evaluated_tensors[3], labels=labels)\n",
    "beam_hypotheses = [] \n",
    "for i in evaluated_tensors[-1]:\n",
    "    for j in i:\n",
    "        beam_hypotheses.append(j[0][1])\n",
    "references = post_process_transcripts(evaluated_tensors[1], labels=labels, \n",
    "                                      transcript_len_list=evaluated_tensors[2])\n",
    "wer = word_error_rate(hypotheses=beam_hypotheses, references=references)\n",
    "\n",
    "print(\"BEAM WER {:.2f}\".format(wer*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
