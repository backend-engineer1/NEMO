model = "RNN-LM"

[optimization]
batch_size = 128  # >=256 have lower utilization and higher conv. time
optimizer = "novograd"
smoothing_coef = 0.0  # is it even working for chars?
warmup_epochs = 1
[optimization.params]
num_epochs = 25
lr = 0.02
weight_decay = 1e-5
larc = false
larc_eta = 1e-3
luc = false
luc_eta = 1e-3

[decoder]
hidden_size = 512
attention_method = "general"
attention_type = "none"
in_dropout = 0.2
gru_dropout = 0.2
teacher_forcing = 0.6
curriculum_learning = 0.75
rnn_type = "gru"
n_layers = 2
tie_emb_out_weights = true

[target]
labels = [
    " ", "'",
    "a", "b", "c", "d", "e", "f", "g", "h", "i", "j", "k", "l", "m",
    "n", "o", "p", "q", "r", "s", "t", "u", "v", "w", "x", "y", "z"
]