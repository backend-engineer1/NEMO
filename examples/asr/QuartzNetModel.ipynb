{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NeMo Models and JarvisModels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import NeMo and ASR collection\n",
    "import nemo\n",
    "import nemo.collections.asr as nemo_asr\n",
    "nf = nemo.core.NeuralModuleFactory(placement=nemo.core.DeviceType.CPU)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A *NeMoModel* is a kind of NeuralModule which contains other neural modules inside it.\n",
    "NeMoModel can have other NeuralModules inside and their mode, and topology of connections can\n",
    "depend on the mode (training, inference, etc.)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NeMoModel instantiation - method 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Because NeMoModel is a NeuralModule, regular constructor-based initialization applies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#First, load the config from YAML file\n",
    "from ruamel.yaml import YAML\n",
    "yaml = YAML(typ=\"safe\")\n",
    "with open(\"configs/jasper_an4.yaml\") as file:\n",
    "    model_definition = yaml.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qn_model = nemo.collections.asr.models.QuartzNet(\n",
    "            preprocessor_params=model_definition['AudioToMelSpectrogramPreprocessor'],\n",
    "            encoder_params=model_definition['JasperEncoder'],\n",
    "            decoder_params=model_definition['JasperDecoderForCTC'],\n",
    "        )\n",
    "print(qn_model.num_weights)\n",
    "print(qn_model.input_ports)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Because NeMoModel is a NeuralModule, regular config import/export work"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qn_model.export_to_config(\"qn.yaml\")\n",
    "qn_model2 = nemo.collections.asr.models.QuartzNet.import_from_config(config_file=\"qn.yaml\")\n",
    "print(qn_model2.num_weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NeMoModel instantiation - method 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List all available models from NGC\n",
    "for checkpoint in nemo.collections.asr.models.QuartzNet.list_pretrained_models():\n",
    "    print(checkpoint.pretrained_model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Automagically go to NGC and instantiate a model and weights\n",
    "pre_trained_qn_model = nemo_asr.models.QuartzNet.from_pretrained(model_info=\"QuartzNet15x5-En-BASE\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Export model to \".nemo\" format"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Export to \".nemo\" file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pre_trained_qn_model.export('aaaa.nemo')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_instance = nemo_asr.models.QuartzNet.from_pretrained(model_info='aaaa.nemo')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\".nemo\" file is an arxiv which contains the following:\n",
    "\n",
    "* weights per module\n",
    "* hyperparameters (e.g. constructor arguments) for all modules\n",
    "* topology (e.g. NeuralGraph) description for inference and for trainig modes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NeMoModels can be used just as any other Neural Module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_manifest = \"/Users/okuchaiev/Data/an4_dataset/an4_train.json\"\n",
    "val_manifest = \"/Users/okuchaiev/Data/an4_dataset/an4_val.json\"\n",
    "labels = model_definition['labels']\n",
    "data_layer = nemo_asr.AudioToTextDataLayer(manifest_filepath=train_manifest, labels=labels, batch_size=16)\n",
    "data_layerE = nemo_asr.AudioToTextDataLayer(manifest_filepath=val_manifest, labels=labels, batch_size=16)\n",
    "ctc_loss = nemo_asr.CTCLossNM(num_classes=len(labels))\n",
    "greedy_decoder = nemo_asr.GreedyCTCDecoder()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "audio_signal, audio_signal_len, transcript, transcript_len = data_layer()\n",
    "log_probs, encoded_len = pre_trained_qn_model(input_signal=audio_signal, length=audio_signal_len)\n",
    "predictions = greedy_decoder(log_probs=log_probs)\n",
    "loss = ctc_loss(log_probs=log_probs, targets=transcript,\n",
    "                input_length=encoded_len, target_length=transcript_len)\n",
    "\n",
    "\n",
    "# # Evaluation\n",
    "# audio_signal, audio_signal_len, transcript, transcript_len = data_layerE()\n",
    "# log_probs, encoded_len = pre_trained_qn_model(input_signal=audio_signal, length=audio_signal_len)\n",
    "# predictions = greedy_decoder(log_probs=log_probs)\n",
    "# lossE = ctc_loss(log_probs=log_probs, targets=transcript,\n",
    "#                 input_length=encoded_len, target_length=transcript_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# START TRAINING \n",
    "tensors_to_evaluate=[predictions, transcript, transcript_len]\n",
    "from functools import partial\n",
    "from nemo.collections.asr.helpers import monitor_asr_train_progress\n",
    "train_callback = nemo.core.SimpleLossLoggerCallback(\n",
    "    tensors=[loss]+tensors_to_evaluate,\n",
    "    print_func=partial(monitor_asr_train_progress, labels=labels))\n",
    "nf.train(tensors_to_optimize=[loss],\n",
    "                callbacks=[train_callback],\n",
    "                optimizer=\"novograd\",\n",
    "                optimization_params={\"num_epochs\": 30, \"lr\": 1e-2,\n",
    "                                    \"weight_decay\": 1e-3})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## OTHER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pre_trained_qn_model.transcribe('myaudio.wav')\n",
    "pre_trained_qn_model.transcribe_from_microphone()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
