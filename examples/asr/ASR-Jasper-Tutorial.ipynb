{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NeMo ASR Tutorial\n",
    "\n",
    "## Preliminary Setup\n",
    "This notebook was designed to run in the Docker container defined in the root of the NeMo git repo.  To set up this environment, first **clone NeMo**:\n",
    "```\n",
    "git clone https://github.com/NVIDIA/NeMo.git\n",
    "```\n",
    "Next, **download a dataset** for training.  We like to keep this *outside* the NeMo tree to avoid a huge Docker build context.  In this example, we use `$HOME/nemo-data/LibriSpeech`.  Adjust paths as needed.\n",
    "```\n",
    "cd NeMo\n",
    "mkdir -p $HOME/nemo-data/LibriSpeech\n",
    "python scripts/get_librispeech_data.py \\\n",
    "    --data_root=$HOME/nemo-data/LibriSpeech \\\n",
    "    --data_set=dev_clean,train_clean_100\n",
    "# This requires 26GB of disk space and takes a while, go grab a coffee...\n",
    "```\n",
    "Now **build the Docker image**:\n",
    "```\n",
    "docker build -t nemo-demo .\n",
    "```\n",
    "And then **run the container**, mapping in the data directory used above.  For example:\n",
    "```\n",
    "docker run --runtime=nvidia --rm -it --ipc=host \\\n",
    "    -v $HOME/nemo-data/LibriSpeech:/workspace/nemo/data \\\n",
    "    nemo-demo\n",
    "```\n",
    "Once inside running Docker container, just use the start-jupyter.sh script to start JupyterLab.  Then using the JupyterLab file browser, open examples/asr/NeMo-ASR-Tutorial-blog.ipynb.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "This Automatic Speech Recognition (ASR) tutorial is focused on the Jasper model [[2]](#ref2). Jasper is CTC-based [[1]](#ref1) end-to-end model. The model is called “end-to-end” because it transcribes speech samples without any additional alignment information. CTC allows finding an alignment between audio and text. CTC-ASR training pipeline consists of the following blocks:\n",
    "1. audio preprocessing (feature extraction): signal normalization, windowing, (log) spectrogram (or mel scale spectrogram, or MFCC)\n",
    "2. neural acoustic model (which predicts a probability distribution P_t(c) over vocabulary characters c per each time step t given input features per each timestep)\n",
    "3. CTC loss function\n",
    "\n",
    "![CTC training pipeline](https://raw.githubusercontent.com/NVIDIA/NeMo/master/docs/sources/source/asr/jasper.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting the data\n",
    "We will be using the open-source LibriSpeech [[3]](#ref3) dataset</a>.  A script to download and prepare the LibriSpeech dataset is included in the <nemo_git_repo_root>/scripts directory.  For example:\n",
    "```\n",
    "$ mkdir /path/to/data\n",
    "$ python <nemo_git_repo_root>/scripts/get_librispeech_data.py \\\n",
    "    --data_root=/path/to/data \\\n",
    "    --data_set=dev_clean,train_clean_100\n",
    "```\n",
    "\n",
    "\n",
    "> **A word of caution** - *LibriSpeech is a large dataset.  Using the\n",
    "> `--data_set=dev_clean,train_clean_100` requires at least 26GB\n",
    "> of disk space, and `--data_set=ALL` at least 110GB.  In the example\n",
    "> above, the `/path/to/data` directory should probably be outside the\n",
    "> nemo git root directory to avoid a huge Docker build context.\n",
    "> Probably also best to run this ahead of time, outside the notebook,\n",
    "> and map it into your running docker container via\n",
    "> `docker run -v /path/to/data:/data ...`.*\n",
    "\n",
    "\n",
    "After download and conversion, your data folder should contain 2 json files:\n",
    "* `dev_clean.json`\n",
    "* `train_clean_100.json`\n",
    "\n",
    "In the tutorial we will use `train_clean_100.json` for training and `dev_clean.json` for evaluation. Each line in json file describes a training sample - `audio_filepath` contains path to the wav file, `duration` it’s duration in seconds, and `text` is it’s transcript."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training\n",
    "We will train a small model from the Jasper family [[2]](#ref2). Jasper (“Just Another SPeech Recognizer”) is a deep time delay neural network (TDNN) comprising of blocks of 1D-convolutional layers. Jasper family of models are denoted as Jasper_[BxR] where B is the number of blocks, and R - the number of convolutional sub-blocks within a block. Each sub-block contains a 1-D convolution, batch normalization, ReLU, and dropout:\n",
    "\n",
    "![Jasper Model](https://raw.githubusercontent.com/NVIDIA/NeMo/master/docs/sources/source/asr/jasper.png)\n",
    "\n",
    "In the tutorial we will be using model [12x1] and will be using separable convolutions. In the code that follows, we'll run both training (on train_clean_100.json) and evaluation (on dev_clean.json) on single GPU."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup\n",
    "\n",
    "> **NOTE** - *The following code blocks assume you are running in a Docker\n",
    "> container based on the example `<nemo git root>/Dockerfile`, with\n",
    "> `/workspace/nemo` being the path to <nemo git root> inside the\n",
    "> container.  We also assume you have mounted the top level of the\n",
    "> LibriSpeech dataset in `/workspace/nemo/data` in the container.*\n",
    ">    \n",
    "> *If you are running with a different configuration, please adjust\n",
    "> paths accordingly in the code blocks below.*\n",
    "\n",
    "    \n",
    "First we import the NeMo core and ASR packages and define our datasets and model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NeMo's \"core\" package\n",
    "import nemo\n",
    "# NeMo's ASR collection\n",
    "import nemo_asr\n",
    "\n",
    "# Path to our training manifest.  Here we assume we've mapped it into a /nemo-data data directory\n",
    "train_dataset = \"/workspace/nemo/data/train_clean_100.json\"\n",
    "# Path to our validation manifest\n",
    "eval_datasets = \"/workspace/nemo/data/dev_clean.json\"\n",
    "\n",
    "# To read the Jasper Model definition yaml\n",
    "from ruamel.yaml import YAML\n",
    "\n",
    "# Here we will be using separable convolutions\n",
    "# with 12 blocks (k=12 repeated once r=1 from the picture above)\n",
    "yaml = YAML(typ=\"safe\")\n",
    "# Change path to ./configs/jasper12x1SEP.yaml once NeMo-ASR-Tutorial.ipynb is moved into the examples folder.\n",
    "with open(\"/workspace/nemo/examples/asr/configs/jasper12x1SEP.yaml\") as f:\n",
    "    jasper_model_definition = yaml.load(f)\n",
    "labels = jasper_model_definition['labels']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can create a NeuralFactory to manage training and begin instantiating the Neural Modules required for the Jasper model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a Neural Factory\n",
    "# It creates log files and tensorboard writers for us among other functions\n",
    "nf = nemo.core.NeuralModuleFactory(\n",
    "    log_dir='jasper12x1SEP',\n",
    "    create_tb_writer=True)\n",
    "tb_writer = nf.tb_writer\n",
    "logger = nf.logger\n",
    "\n",
    "# Instantiate neural modules\n",
    "data_layer = nemo_asr.AudioToTextDataLayer(\n",
    "    manifest_filepath=train_dataset,\n",
    "    labels=labels, batch_size=32)\n",
    "\n",
    "data_layer_val = nemo_asr.AudioToTextDataLayer(\n",
    "    manifest_filepath=eval_datasets,\n",
    "    labels=labels, batch_size=32, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "...and audio preprocessors and encoders/decoders for the Jasper model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_preprocessor = nemo_asr.AudioPreprocessing()\n",
    "spec_augment = nemo_asr.SpectrogramAugmentation(rect_masks=5)\n",
    "\n",
    "jasper_encoder = nemo_asr.JasperEncoder(\n",
    "    feat_in=64,\n",
    "    **jasper_model_definition['JasperEncoder'])\n",
    "jasper_decoder = nemo_asr.JasperDecoderForCTC(\n",
    "    feat_in=1024, num_classes=len(labels))\n",
    "ctc_loss = nemo_asr.CTCLossNM(num_classes=len(labels))\n",
    "greedy_decoder = nemo_asr.GreedyCTCDecoder()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we define the DAG for training the Jasper model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training DAG (Model)\n",
    "audio_signal, audio_signal_len, transcript, transcript_len = data_layer()\n",
    "processed_signal, processed_signal_len = data_preprocessor(\n",
    "    input_signal=audio_signal, length=audio_signal_len)\n",
    "aug_signal = spec_augment(input_spec=processed_signal)\n",
    "encoded, encoded_len = jasper_encoder(\n",
    "    audio_signal=aug_signal, length=processed_signal_len)\n",
    "log_probs = jasper_decoder(encoder_output=encoded)\n",
    "predictions = greedy_decoder(log_probs=log_probs)\n",
    "loss = ctc_loss(\n",
    "    log_probs=log_probs, targets=transcript,\n",
    "    input_length=encoded_len, target_length=transcript_len)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And similarly for the validation model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Validation DAG (Model)\n",
    "# We need to instantiate additional data layer neural module\n",
    "# for validation data\n",
    "audio_signal_v, audio_signal_len_v, transcript_v, transcript_len_v = data_layer_val()\n",
    "processed_signal_v, processed_signal_len_v = data_preprocessor(\n",
    "    input_signal=audio_signal_v, length=audio_signal_len_v)\n",
    "# Note that we are not using data-augmentation in validation DAG\n",
    "encoded_v, encoded_len_v = jasper_encoder(\n",
    "    audio_signal=processed_signal_v, length=processed_signal_len_v)\n",
    "log_probs_v = jasper_decoder(encoder_output=encoded_v)\n",
    "predictions_v = greedy_decoder(log_probs=log_probs_v)\n",
    "loss_v = ctc_loss(\n",
    "    log_probs=log_probs_v, targets=transcript_v,\n",
    "    input_length=encoded_len_v, target_length=transcript_len_v)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also need some helper functions to monitor training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# These helper functions are needed to print and compute various metrics\n",
    "# such as word error rate and log them into tensorboard\n",
    "# they are domain-specific and are provided by NeMo's collections\n",
    "from nemo_asr.helpers import monitor_asr_train_progress, \\\n",
    "    process_evaluation_batch, process_evaluation_epoch\n",
    "\n",
    "from functools import partial\n",
    "# Callback to track loss and print predictions during training\n",
    "train_callback = nemo.core.SimpleLossLoggerCallback(\n",
    "    tb_writer=tb_writer,\n",
    "    # Define the tensors that you want SimpleLossLoggerCallback to\n",
    "    # operate on\n",
    "    # Here we want to print our loss, and our word error rate which\n",
    "    # is a function of our predictions, transcript, and transcript_len\n",
    "    tensors=[loss, predictions, transcript, transcript_len],\n",
    "    # To print logs to screen, define a print_func\n",
    "    print_func=partial(\n",
    "        monitor_asr_train_progress,\n",
    "        labels=labels,\n",
    "        logger=logger\n",
    "    ))\n",
    "\n",
    "saver_callback = nemo.core.CheckpointCallback(\n",
    "    folder=\"./\",\n",
    "    # Set how often we want to save checkpoints\n",
    "    step_freq=100)\n",
    "\n",
    "# PRO TIP: while you can only have 1 train DAG, you can have as many\n",
    "# val DAGs and callbacks as you want. This is useful if you want to monitor\n",
    "# progress on more than one val dataset at once (say LibriSpeech dev clean\n",
    "# and dev other)\n",
    "eval_callback = nemo.core.EvaluatorCallback(\n",
    "    eval_tensors=[loss_v, predictions_v, transcript_v, transcript_len_v],\n",
    "    # how to process evaluation batch - e.g. compute WER\n",
    "    user_iter_callback=partial(\n",
    "        process_evaluation_batch,\n",
    "        labels=labels\n",
    "        ),\n",
    "    # how to aggregate statistics (e.g. WER) for the evaluation epoch\n",
    "    user_epochs_done_callback=partial(\n",
    "        process_evaluation_epoch, tag=\"DEV-CLEAN\", logger=logger\n",
    "        ),\n",
    "    eval_step=500,\n",
    "    tb_writer=tb_writer)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **NOTE** - *One potential gotcha is the audio_filepath encoded\n",
    "> in the `train_clean_100.json` and `dev_clean.json` manifest files.\n",
    "> These will contain the path used when originally executing the\n",
    "> `get_librispeech_data.py` script which is likely different from\n",
    "> the path mapped into the docker container.*\n",
    ">\n",
    "> *In our example Dockerfile, we have `WORKDIR` set to the nemo git\n",
    "> root in `/workspace/nemo`.  When running the container, we map the\n",
    "> data directory into `/workspace/nemo/data`.*\n",
    ">\n",
    "> *Assuming we used `get_librispeech_data.py --data_root=/path/to/data`,\n",
    "> we need to replace the existing /path/to/data prefix with the\n",
    "> container path as follows:*\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's look at the first entry in the train_clean_100.json file to see the path:\n",
    "!head -n1 /workspace/nemo/data/train_clean_100.json\n",
    "# Now replace this path with the path inside the container:\n",
    "!sed -i 's,/path/to/data,/workspace/nemo/data,g' /workspace/nemo/data/train_clean_100.json\n",
    "!sed -i 's,/path/to/data,/workspace/nemo/data,g' /workspace/nemo/data/dev_clean.json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can train the model! (Training 50 epochs requires approximately 4 hours on RTX8000.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run training using your Neural Factory\n",
    "# Once this \"action\" is called data starts flowing along train and eval DAGs\n",
    "# and computations start to happen\n",
    "nf.train(\n",
    "    # Specify the loss to optimize for\n",
    "    tensors_to_optimize=[loss],\n",
    "    # Specify which callbacks you want to run\n",
    "    callbacks=[train_callback, eval_callback, saver_callback],\n",
    "    # Specify what optimizer to use\n",
    "    optimizer=\"novograd\",\n",
    "    # Specify optimizer parameters such as num_epochs and lr\n",
    "    optimization_params={\n",
    "        \"num_epochs\": 50, \"lr\": 0.02, \"weight_decay\": 1e-4\n",
    "        }\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To improve word error rates:\n",
    "* Train longer\n",
    "* Train on more data\n",
    "* Use a larger model\n",
    "* Train on several GPUs and use mixed precision (on NVIDIA Volta and Turing GPUs)\n",
    "* Start with pre-trained checkpoints"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mixed Precision Training\n",
    "Mixed precision and distributed training in NeMo is based on <a href=\"https://github.com/NVIDIA/apex\">NVIDIA’s APEX library</a>. This is installed with NVIDIA's NGC Pytorch container with an example of updating in the example Dockerfile.\n",
    "\n",
    "> **Note** -  _Because mixed precision requires Tensor Cores it\n",
    "> only works on NVIDIA Volta and Turing based GPUs._\n",
    "\n",
    "To train with mixed-precision all you need is to set `optimization_level` parameter of `nemo.core.NeuralModuleFactory` to `nemo.core.Optimization.mxprO1`.  For example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "local_rank = None\n",
    "nf = nemo.core.NeuralModuleFactory(\n",
    "    backend=nemo.core.Backend.PyTorch,\n",
    "    optimization_level=nemo.core.Optimization.mxprO1,\n",
    "    local_rank=local_rank,\n",
    "    placement=nemo.core.DeviceType.AllGpu,\n",
    "    cudnn_benchmark=True)\n",
    "# Here we define some additional features of the NeuralFactor that\n",
    "# enable multi-GPU training.  We'll discuss these more below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note, if you completed the training steps above, you'll have\n",
    "# checkpoints saved in the working directory.  We'll need to\n",
    "# remove these to restart training.\n",
    "! rm JasperEncoder-STEP-*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we can re-run training, this time only a few epochs to test.\n",
    "nf.train(\n",
    "    # Specify the loss to optimize for\n",
    "    tensors_to_optimize=[loss],\n",
    "    # Specify which callbacks you want to run\n",
    "    callbacks=[train_callback, eval_callback, saver_callback],\n",
    "    # Specify what optimizer to use\n",
    "    optimizer=\"novograd\",\n",
    "    # Specify optimizer parameters such as num_epochs and lr\n",
    "    optimization_params={\n",
    "        \"num_epochs\": 5, \"lr\": 0.02, \"weight_decay\": 1e-4\n",
    "        }\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the above training run, you will notice some additional output describing the optimization level used:\n",
    "```\n",
    "Defaults for this optimization level are:\n",
    "enabled                : True\n",
    "opt_level              : O1\n",
    "cast_model_type        : None\n",
    "patch_torch_functions  : True\n",
    "keep_batchnorm_fp32    : None\n",
    "master_weights         : None\n",
    "loss_scale             : dynamic\n",
    "```\n",
    "You will also see output signaling Gradient overflow:\n",
    "```\n",
    "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0\n",
    "```\n",
    "This is expected behavior, and example of how NVIDIA's APEX extension tracks gradients and scales loss so that gradients are representable in mixed precision."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multi-GPU Training\n",
    "Enabling multi-GPU training with NeMo is easy:\n",
    "1. First set placement to `nemo.core.DeviceType.AllGpu` in NeuralModuleFactory and in your Neural Modules\n",
    "2. Have your script accept `local_rank` argument and do not set it yourself: `parser.add_argument(“–local_rank”, default=None, type=int)`\n",
    "3. Use the `torch.distributed.launch` package to run your script.\n",
    "\n",
    "An example of this can be seen in the `<nemo git root>/examples/asr/jasper.py` script.\n",
    "\n",
    "Unfortunately we can't launch this interactively in a notebook because of the way torch.distributed spawns python processes.  We can instead use a JupyterLab Terminal (File -> New -> Terminal) to launch multi-GPU training directly.  In the example below `--nproc_per_node` should be set to the number of GPUs on the node.  The arguments to the jasper.py script mirror those used in the example above.  The jasper.py script uses these arguments to define the NeuralFactory and NeuralModules in the same way as above, with multiple processes spawned to run on multiple GPUs.  In the example below, we use two GPUs to train the Jasper 15x5 model on the same LibriSpeech train_clean_100 dataset:\n",
    "\n",
    "```\n",
    "python -m torch.distributed.launch --nproc_per_node=2 \\\n",
    "    /workspace/nemo/examples/asr/jasper.py \\\n",
    "    --batch_size=64 \\\n",
    "    --num_epochs=100 \\\n",
    "    --lr=0.015 \\\n",
    "    --warmup_steps=8000 \\\n",
    "    --weight_decay=0.001 \\\n",
    "    --train_dataset=/workspace/nemo/data/train_clean_100.json \\\n",
    "    --eval_datasets=/workspace/nemo/data/dev_clean.json \\\n",
    "    --model_config=/workspace/nemo/examples/asr/configs/jasper15x5SEP.yaml \\\n",
    "    --exp_name=MultiNodeExperiment\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References\n",
    "<a name=\"ref1\"></a>Alex Graves, Santiago Fernández, Faustino Gomez, and Jürgen Schmidhuber. Connectionist temporal classification: labelling unsegmented sequence data with recurrent neural networks. In _Proceedings of the 23rd international conference on Machine learning_, 369–376. ACM, 2006.\n",
    "\n",
    "<a name=\"ref2\"></a>Jason Li, Vitaly Lavrukhin, Boris Ginsburg, Ryan Leary, Oleksii Kuchaiev, Jonathan M Cohen, Huyen Nguyen, and Ravi Teja Gadde. Jasper: an end-to-end convolutional neural acoustic model. _arXiv preprint arXiv:1904.03288_, 2019.\n",
    "\n",
    "<a name=\"ref3\"></a>Vassil Panayotov, Guoguo Chen, Daniel Povey, and Sanjeev Khudanpur. Librispeech: an asr corpus based on public domain audio books. In _Acoustics, Speech and Signal Processing (ICASSP), 2015 IEEE International Conference on_, 5206–5210. IEEE, 2015.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
