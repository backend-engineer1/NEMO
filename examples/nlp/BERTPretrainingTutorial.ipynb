{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This example is for demonstration purposes\n",
    "# It shows how to pre-train BERT from scratch using your own tokenizer model\n",
    "# To build tokenizer model do:\n",
    "# python tests/data/create_vocab.py --train_path wikitext-2/train.txt\n",
    "# Please refer to the corresponding NLP tutorial on NeMo documentation\n",
    "\n",
    "import math\n",
    "import os\n",
    "\n",
    "import nemo\n",
    "from nemo.utils.lr_policies import CosineAnnealing\n",
    "\n",
    "import nemo_nlp\n",
    "from nemo_nlp import NemoBertTokenizer, SentencePieceTokenizer\n",
    "from nemo_nlp.callbacks.bert_pretraining import eval_iter_callback, \\\n",
    "    eval_epochs_done_callback\n",
    "\n",
    "BATCHES_PER_STEP = 1\n",
    "BATCH_SIZE = 64\n",
    "BATCH_SIZE_EVAL = 16\n",
    "CHECKPOINT_DIR = \"bert_pretraining_checkpoints\"\n",
    "D_MODEL = 768\n",
    "D_INNER = 3072\n",
    "HIDDEN_ACT = \"gelu\"\n",
    "LEARNING_RATE = 1e-2\n",
    "LR_WARMUP_PROPORTION = 0.05\n",
    "MASK_PROBABILITY = 0.15\n",
    "MAX_SEQ_LENGTH = 128\n",
    "NUM_EPOCHS = 10\n",
    "NUM_HEADS = 12\n",
    "NUM_LAYERS = 12\n",
    "OPTIMIZER = \"novograd\"\n",
    "WEIGHT_DECAY = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate neural factory with supported backend\n",
    "neural_factory = nemo.core.NeuralModuleFactory(\n",
    "    backend=nemo.core.Backend.PyTorch,\n",
    "\n",
    "    # If you're training with multiple GPUs, you should handle this value with\n",
    "    # something like argparse. See examples/nlp/bert_pretraining.py for an example.\n",
    "    local_rank=None,\n",
    "\n",
    "    # If you're training with mixed precision, this should be set to mxprO1 or mxprO2.\n",
    "    # See https://nvidia.github.io/apex/amp.html#opt-levels for more details.\n",
    "    optimization_level=nemo.core.Optimization.mxprO0,\n",
    "\n",
    "    # If you're training with multiple GPUs, this should be set to\n",
    "    # nemo.core.DeviceType.AllGpu\n",
    "    placement=nemo.core.DeviceType.GPU)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenizer = SentencePieceTokenizer(model_path=\"<PATH TO tokenizer.model>\")\n",
    "tokenizer = SentencePieceTokenizer(model_path=\"/home/okuchaiev/repos/gitlab-master/nemo/tokenizer.model\")\n",
    "tokenizer.add_special_tokens([\"[MASK]\", \"[CLS]\", \"[SEP]\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_model = nemo_nlp.huggingface.BERT(\n",
    "    vocab_size=tokenizer.vocab_size,\n",
    "    num_hidden_layers=NUM_LAYERS,\n",
    "    hidden_size=D_MODEL,\n",
    "    num_attention_heads=NUM_HEADS,\n",
    "    intermediate_size=D_INNER,\n",
    "    max_position_embeddings=MAX_SEQ_LENGTH,\n",
    "    hidden_act=HIDDEN_ACT,\n",
    "    factory=neural_factory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlm_log_softmax = nemo_nlp.TransformerLogSoftmaxNM(\n",
    "    vocab_size=tokenizer.vocab_size,\n",
    "    d_model=D_MODEL,\n",
    "    factory=neural_factory)\n",
    "mlm_loss = nemo_nlp.MaskedLanguageModelingLossNM(factory=neural_factory)\n",
    "\n",
    "mlm_log_softmax.log_softmax.dense.weight = \\\n",
    "    bert_model.bert.embeddings.word_embeddings.weight\n",
    "\n",
    "nsp_log_softmax = nemo_nlp.SentenceClassificationLogSoftmaxNM(\n",
    "    d_model=D_MODEL,\n",
    "    num_classes=2,\n",
    "    factory=neural_factory)\n",
    "nsp_loss = nemo_nlp.NextSentencePredictionLossNM(factory=neural_factory)\n",
    "\n",
    "bert_loss = nemo_nlp.LossAggregatorNM(\n",
    "    num_inputs=2,\n",
    "    factory=neural_factory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_layer = nemo_nlp.BertPretrainingDataLayer(\n",
    "    tokenizer=tokenizer,\n",
    "    dataset=os.path.join(\"/home/okuchaiev/repos/gitlab-master/nemo/wikitext-2\", \"train.txt\"),\n",
    "    name=\"train\",\n",
    "    max_seq_length=MAX_SEQ_LENGTH,\n",
    "    mask_probability=MASK_PROBABILITY,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    factory=neural_factory)\n",
    "\n",
    "test_data_layer = nemo_nlp.BertPretrainingDataLayer(\n",
    "    tokenizer=tokenizer,\n",
    "    dataset=os.path.join(\"/home/okuchaiev/repos/gitlab-master/nemo/wikitext-2\", \"test.txt\"),\n",
    "    name=\"test\",\n",
    "    max_seq_length=MAX_SEQ_LENGTH,\n",
    "    mask_probability=MASK_PROBABILITY,\n",
    "    batch_size=BATCH_SIZE_EVAL,\n",
    "    factory=neural_factory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_ids, input_type_ids, input_mask, \\\n",
    "    output_ids, output_mask, nsp_labels = train_data_layer()\n",
    "\n",
    "hidden_states = bert_model(input_ids=input_ids,\n",
    "                           token_type_ids=input_type_ids,\n",
    "                           attention_mask=input_mask)\n",
    "\n",
    "train_mlm_log_probs = mlm_log_softmax(hidden_states=hidden_states)\n",
    "train_mlm_loss = mlm_loss(log_probs=train_mlm_log_probs,\n",
    "                          output_ids=output_ids,\n",
    "                          output_mask=output_mask)\n",
    "\n",
    "train_nsp_log_probs = nsp_log_softmax(hidden_states=hidden_states)\n",
    "train_nsp_loss = nsp_loss(log_probs=train_nsp_log_probs, labels=nsp_labels)\n",
    "train_loss = bert_loss(loss_1=train_mlm_loss, loss_2=train_nsp_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_ids_, input_type_ids_, input_mask_, \\\n",
    "    output_ids_, output_mask_, nsp_labels_ = test_data_layer()\n",
    "\n",
    "hidden_states_ = bert_model(input_ids=input_ids_,\n",
    "                            token_type_ids=input_type_ids_,\n",
    "                            attention_mask=input_mask_)\n",
    "\n",
    "test_mlm_log_probs = mlm_log_softmax(hidden_states=hidden_states_)\n",
    "test_mlm_loss = mlm_loss(log_probs=test_mlm_log_probs,\n",
    "                         output_ids=output_ids_,\n",
    "                         output_mask=output_mask_)\n",
    "\n",
    "test_nsp_log_probs = nsp_log_softmax(hidden_states=hidden_states_)\n",
    "test_nsp_loss = nsp_loss(log_probs=test_nsp_log_probs, labels=nsp_labels_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "callback_loss = nemo.core.SimpleLossLoggerCallback(\n",
    "    tensors=[train_loss],\n",
    "    print_func=lambda x: print(\"Loss: {:.3f}\".format(x[0].item())))\n",
    "\n",
    "train_data_size = len(train_data_layer)\n",
    "\n",
    "# If you're training on multiple GPUs, this should be\n",
    "# train_data_size / (batch_size * batches_per_step * num_gpus)\n",
    "steps_per_epoch = int(train_data_size / (BATCHES_PER_STEP * BATCH_SIZE))\n",
    "\n",
    "callback_test = nemo.core.EvaluatorCallback(\n",
    "    eval_tensors=[test_mlm_loss, test_nsp_loss],\n",
    "    user_iter_callback=eval_iter_callback,\n",
    "    user_epochs_done_callback=eval_epochs_done_callback,\n",
    "    eval_step=steps_per_epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_policy = CosineAnnealing(NUM_EPOCHS * steps_per_epoch,\n",
    "                            warmup_ratio=LR_WARMUP_PROPORTION)\n",
    "neural_factory.train(tensors_to_optimize=[train_loss],\n",
    "                lr_policy=lr_policy,\n",
    "                callbacks=[callback_loss, callback_test],\n",
    "                #callbacks=[callback_loss],\n",
    "                batches_per_step=BATCHES_PER_STEP,\n",
    "                optimizer=OPTIMIZER,\n",
    "                optimization_params={\n",
    "                    \"batch_size\": BATCH_SIZE,\n",
    "                    \"num_epochs\": NUM_EPOCHS,\n",
    "                    \"lr\": LEARNING_RATE,\n",
    "                    \"weight_decay\": WEIGHT_DECAY,\n",
    "                    \"betas\": (0.95, 0.98),\n",
    "                    \"grad_norm_clip\": None\n",
    "                })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
