{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This example is for demonstration purposes\n",
    "# It shows how to pre-train BERT from scratch using your own tokenizer model\n",
    "# To build tokenizer model do:\n",
    "# python tests/data/create_vocab.py --train_path wikitext-2/train.txt\n",
    "# Please refer to the corresponding NLP tutorial on NeMo documentation\n",
    "\n",
    "import math\n",
    "import os\n",
    "\n",
    "import nemo\n",
    "from nemo.utils.lr_policies import CosineAnnealing\n",
    "\n",
    "import nemo_nlp\n",
    "from nemo_nlp import NemoBertTokenizer, SentencePieceTokenizer\n",
    "from nemo_nlp.utils.callbacks.bert_pretraining import eval_iter_callback, \\\n",
    "    eval_epochs_done_callback\n",
    "\n",
    "BATCHES_PER_STEP = 1\n",
    "BATCH_SIZE = 64\n",
    "BATCH_SIZE_EVAL = 16\n",
    "CHECKPOINT_DIR = \"bert_pretraining_checkpoints\"\n",
    "D_MODEL = 768\n",
    "D_INNER = 3072\n",
    "HIDDEN_ACT = \"gelu\"\n",
    "LEARNING_RATE = 1e-2\n",
    "LR_WARMUP_PROPORTION = 0.05\n",
    "MASK_PROBABILITY = 0.15\n",
    "MAX_SEQ_LENGTH = 128\n",
    "NUM_EPOCHS = 10\n",
    "NUM_HEADS = 12\n",
    "NUM_LAYERS = 12\n",
    "OPTIMIZER = \"novograd\"\n",
    "WEIGHT_DECAY = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate neural factory with supported backend\n",
    "neural_factory = nemo.core.NeuralModuleFactory(\n",
    "    backend=nemo.core.Backend.PyTorch,\n",
    "\n",
    "    # If you're training with multiple GPUs, you should handle this value with\n",
    "    # something like argparse. See examples/nlp/bert_pretraining.py for an example.\n",
    "    local_rank=None,\n",
    "\n",
    "    # If you're training with mixed precision, this should be set to mxprO1 or mxprO2.\n",
    "    # See https://nvidia.github.io/apex/amp.html#opt-levels for more details.\n",
    "    optimization_level=nemo.core.Optimization.mxprO0,\n",
    "\n",
    "    # If you're training with multiple GPUs, this should be set to\n",
    "    # nemo.core.DeviceType.AllGpu\n",
    "    placement=nemo.core.DeviceType.GPU)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = SentencePieceTokenizer(model_path=\"data/lm/wikitext-2/bert/tokenizer.model\")\n",
    "tokenizer.add_special_tokens([\"[MASK]\", \"[CLS]\", \"[SEP]\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_model = nemo_nlp.huggingface.BERT(\n",
    "    vocab_size=tokenizer.vocab_size,\n",
    "    num_hidden_layers=NUM_LAYERS,\n",
    "    hidden_size=D_MODEL,\n",
    "    num_attention_heads=NUM_HEADS,\n",
    "    intermediate_size=D_INNER,\n",
    "    max_position_embeddings=MAX_SEQ_LENGTH,\n",
    "    hidden_act=HIDDEN_ACT,\n",
    "    factory=neural_factory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlm_log_softmax = nemo_nlp.TransformerLogSoftmaxNM(\n",
    "    vocab_size=tokenizer.vocab_size,\n",
    "    d_model=D_MODEL,\n",
    "    factory=neural_factory)\n",
    "mlm_loss = nemo_nlp.MaskedLanguageModelingLossNM(factory=neural_factory)\n",
    "\n",
    "mlm_log_softmax.log_softmax.dense.weight = \\\n",
    "    bert_model.bert.embeddings.word_embeddings.weight\n",
    "\n",
    "nsp_log_softmax = nemo_nlp.SentenceClassificationLogSoftmaxNM(\n",
    "    d_model=D_MODEL,\n",
    "    num_classes=2,\n",
    "    factory=neural_factory)\n",
    "nsp_loss = nemo_nlp.NextSentencePredictionLossNM(factory=neural_factory)\n",
    "\n",
    "bert_loss = nemo_nlp.LossAggregatorNM(\n",
    "    num_inputs=2,\n",
    "    factory=neural_factory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00,  7.62it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 63.67it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Used 1314411 tokens of total 2051913\n",
      "Used 148262 tokens of total 241211\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "train_data_layer = nemo_nlp.BertPretrainingDataLayer(\n",
    "    tokenizer=tokenizer,\n",
    "    dataset=os.path.join(\"data/lm/wikitext-2\", \"train.txt\"),\n",
    "    name=\"train\",\n",
    "    max_seq_length=MAX_SEQ_LENGTH,\n",
    "    mask_probability=MASK_PROBABILITY,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    factory=neural_factory)\n",
    "\n",
    "test_data_layer = nemo_nlp.BertPretrainingDataLayer(\n",
    "    tokenizer=tokenizer,\n",
    "    dataset=os.path.join(\"data/lm/wikitext-2\", \"test.txt\"),\n",
    "    name=\"test\",\n",
    "    max_seq_length=MAX_SEQ_LENGTH,\n",
    "    mask_probability=MASK_PROBABILITY,\n",
    "    batch_size=BATCH_SIZE_EVAL,\n",
    "    factory=neural_factory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_ids, input_type_ids, input_mask, \\\n",
    "    output_ids, output_mask, nsp_labels = train_data_layer()\n",
    "\n",
    "hidden_states = bert_model(input_ids=input_ids,\n",
    "                           token_type_ids=input_type_ids,\n",
    "                           attention_mask=input_mask)\n",
    "\n",
    "train_mlm_log_probs = mlm_log_softmax(hidden_states=hidden_states)\n",
    "train_mlm_loss = mlm_loss(log_probs=train_mlm_log_probs,\n",
    "                          output_ids=output_ids,\n",
    "                          output_mask=output_mask)\n",
    "\n",
    "train_nsp_log_probs = nsp_log_softmax(hidden_states=hidden_states)\n",
    "train_nsp_loss = nsp_loss(log_probs=train_nsp_log_probs, labels=nsp_labels)\n",
    "train_loss = bert_loss(loss_1=train_mlm_loss, loss_2=train_nsp_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_ids_, input_type_ids_, input_mask_, \\\n",
    "    output_ids_, output_mask_, nsp_labels_ = test_data_layer()\n",
    "\n",
    "hidden_states_ = bert_model(input_ids=input_ids_,\n",
    "                            token_type_ids=input_type_ids_,\n",
    "                            attention_mask=input_mask_)\n",
    "\n",
    "test_mlm_log_probs = mlm_log_softmax(hidden_states=hidden_states_)\n",
    "test_mlm_loss = mlm_loss(log_probs=test_mlm_log_probs,\n",
    "                         output_ids=output_ids_,\n",
    "                         output_mask=output_mask_)\n",
    "\n",
    "test_nsp_log_probs = nsp_log_softmax(hidden_states=hidden_states_)\n",
    "test_nsp_loss = nsp_loss(log_probs=test_nsp_log_probs, labels=nsp_labels_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "callback_loss = nemo.core.SimpleLossLoggerCallback(\n",
    "    tensors=[train_loss],\n",
    "    print_func=lambda x: print(\"Loss: {:.3f}\".format(x[0].item())))\n",
    "\n",
    "train_data_size = len(train_data_layer)\n",
    "\n",
    "# If you're training on multiple GPUs, this should be\n",
    "# train_data_size / (batch_size * batches_per_step * num_gpus)\n",
    "steps_per_epoch = int(train_data_size / (BATCHES_PER_STEP * BATCH_SIZE))\n",
    "\n",
    "callback_test = nemo.core.EvaluatorCallback(\n",
    "    eval_tensors=[test_mlm_loss, test_nsp_loss],\n",
    "    user_iter_callback=eval_iter_callback,\n",
    "    user_epochs_done_callback=eval_epochs_done_callback,\n",
    "    eval_step=steps_per_epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-09-23 16:22:02,491 - WARNING - Data Layer does not have any weights to return. This get_weights call returns None.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected optimization level O0:  Pure FP32 training.\n",
      "\n",
      "Defaults for this optimization level are:\n",
      "enabled                : True\n",
      "opt_level              : O0\n",
      "cast_model_type        : torch.float32\n",
      "patch_torch_functions  : False\n",
      "keep_batchnorm_fp32    : None\n",
      "master_weights         : False\n",
      "loss_scale             : 1.0\n",
      "Processing user overrides (additional kwargs that are not None)...\n",
      "After processing overrides, optimization options are:\n",
      "enabled                : True\n",
      "opt_level              : O0\n",
      "cast_model_type        : torch.float32\n",
      "patch_torch_functions  : False\n",
      "keep_batchnorm_fp32    : None\n",
      "master_weights         : False\n",
      "loss_scale             : 1.0\n",
      "Starting .....\n",
      "Starting epoch 0\n",
      "Step: 0\n",
      "Loss: 11.202\n",
      "Step time: 0.7863831520080566 seconds\n",
      "Doing Evaluation ..............................\n",
      "Dev MLM perplexity: 28715.501\n",
      "Dev NSP perplexity: 2.697\n",
      "Evaluation time: 3.663609027862549 seconds\n",
      "Step: 25\n",
      "Loss: 8.740\n",
      "Step time: 0.7746455669403076 seconds\n",
      "Step: 50\n",
      "Loss: 8.046\n",
      "Step time: 0.6674389839172363 seconds\n",
      "Step: 75\n",
      "Loss: 7.897\n",
      "Step time: 0.7494850158691406 seconds\n",
      "Step: 100\n",
      "Loss: 7.889\n",
      "Step time: 0.6734147071838379 seconds\n",
      "Doing Evaluation ..............................\n",
      "Dev MLM perplexity: 863.598\n",
      "Dev NSP perplexity: 2.015\n",
      "Evaluation time: 3.5442614555358887 seconds\n",
      "Finished epoch 0 in 86.36702585220337\n",
      "Starting epoch 1\n",
      "Step: 125\n",
      "Loss: 8.003\n",
      "Step time: 0.8014047145843506 seconds\n",
      "Step: 150\n",
      "Loss: 7.871\n",
      "Step time: 0.7708852291107178 seconds\n",
      "Step: 175\n",
      "Loss: 7.785\n",
      "Step time: 0.8721175193786621 seconds\n",
      "Step: 200\n",
      "Loss: 8.043\n",
      "Step time: 0.7606015205383301 seconds\n",
      "Doing Evaluation ..............................\n",
      "Dev MLM perplexity: 783.26\n",
      "Dev NSP perplexity: 2.026\n",
      "Evaluation time: 3.755350351333618 seconds\n",
      "Finished epoch 1 in 88.4552903175354\n",
      "Starting epoch 2\n",
      "Step: 225\n",
      "Loss: 7.593\n",
      "Step time: 0.7387840747833252 seconds\n",
      "Step: 250\n",
      "Loss: 7.694\n",
      "Step time: 0.7432963848114014 seconds\n",
      "Step: 275\n",
      "Loss: 7.593\n",
      "Step time: 0.7494339942932129 seconds\n",
      "Step: 300\n",
      "Loss: 7.739\n",
      "Step time: 0.7615630626678467 seconds\n",
      "Doing Evaluation ..............................\n",
      "Dev MLM perplexity: 708.151\n",
      "Dev NSP perplexity: 2.009\n",
      "Evaluation time: 3.994788885116577 seconds\n",
      "Finished epoch 2 in 88.8535668849945\n",
      "Starting epoch 3\n",
      "Step: 325\n",
      "Loss: 7.630\n",
      "Step time: 0.7386500835418701 seconds\n",
      "Step: 350\n",
      "Loss: 7.837\n",
      "Step time: 0.7364590167999268 seconds\n",
      "Step: 375\n",
      "Loss: 7.706\n",
      "Step time: 0.7686624526977539 seconds\n",
      "Step: 400\n",
      "Loss: 7.832\n",
      "Step time: 0.8648722171783447 seconds\n",
      "Step: 425\n",
      "Loss: 7.763\n",
      "Step time: 0.8091261386871338 seconds\n",
      "Doing Evaluation ..............................\n",
      "Dev MLM perplexity: 650.07\n",
      "Dev NSP perplexity: 2.002\n",
      "Evaluation time: 3.6503849029541016 seconds\n",
      "Finished epoch 3 in 89.41587972640991\n",
      "Starting epoch 4\n",
      "Step: 450\n",
      "Loss: 7.732\n",
      "Step time: 0.6903843879699707 seconds\n",
      "Step: 475\n",
      "Loss: 7.591\n",
      "Step time: 0.7484607696533203 seconds\n",
      "Step: 500\n",
      "Loss: 7.604\n",
      "Step time: 0.7217869758605957 seconds\n",
      "Step: 525\n",
      "Loss: 7.567\n",
      "Step time: 0.7727603912353516 seconds\n",
      "Doing Evaluation ..............................\n",
      "Dev MLM perplexity: 730.825\n",
      "Dev NSP perplexity: 2.003\n",
      "Evaluation time: 3.979888677597046 seconds\n",
      "Finished epoch 4 in 85.8669924736023\n",
      "Starting epoch 5\n",
      "Step: 550\n",
      "Loss: 7.968\n",
      "Step time: 0.7359879016876221 seconds\n",
      "Step: 575\n",
      "Loss: 7.629\n",
      "Step time: 0.7514605522155762 seconds\n",
      "Step: 600\n",
      "Loss: 7.750\n",
      "Step time: 0.8256654739379883 seconds\n",
      "Step: 625\n",
      "Loss: 7.722\n",
      "Step time: 0.7764778137207031 seconds\n",
      "Doing Evaluation ..............................\n",
      "Dev MLM perplexity: 652.046\n",
      "Dev NSP perplexity: 2.038\n",
      "Evaluation time: 3.4465320110321045 seconds\n",
      "Finished epoch 5 in 87.78663039207458\n",
      "Starting epoch 6\n",
      "Step: 650\n",
      "Loss: 7.684\n",
      "Step time: 0.8063247203826904 seconds\n",
      "Step: 675\n",
      "Loss: 7.598\n",
      "Step time: 0.7592158317565918 seconds\n",
      "Step: 700\n",
      "Loss: 7.718\n",
      "Step time: 0.6886806488037109 seconds\n",
      "Step: 725\n",
      "Loss: 7.440\n",
      "Step time: 0.8108744621276855 seconds\n",
      "Doing Evaluation ..............................\n",
      "Dev MLM perplexity: 655.077\n",
      "Dev NSP perplexity: 1.993\n",
      "Evaluation time: 3.714364767074585 seconds\n",
      "Step: 750\n",
      "Loss: 7.481\n",
      "Step time: 0.7753477096557617 seconds\n",
      "Finished epoch 6 in 86.34933543205261\n",
      "Starting epoch 7\n",
      "Step: 775\n",
      "Loss: 7.710\n",
      "Step time: 0.7958974838256836 seconds\n",
      "Step: 800\n",
      "Loss: 7.668\n",
      "Step time: 0.716982364654541 seconds\n",
      "Step: 825\n",
      "Loss: 7.615\n",
      "Step time: 0.7516562938690186 seconds\n",
      "Step: 850\n",
      "Loss: 7.652\n",
      "Step time: 0.7231900691986084 seconds\n",
      "Doing Evaluation ..............................\n",
      "Dev MLM perplexity: 666.324\n",
      "Dev NSP perplexity: 2.009\n",
      "Evaluation time: 3.5358762741088867 seconds\n",
      "Finished epoch 7 in 83.49787712097168\n",
      "Starting epoch 8\n",
      "Step: 875\n",
      "Loss: 7.579\n",
      "Step time: 0.7056005001068115 seconds\n",
      "Step: 900\n",
      "Loss: 7.652\n",
      "Step time: 0.715315580368042 seconds\n",
      "Step: 925\n",
      "Loss: 7.519\n",
      "Step time: 0.7068846225738525 seconds\n",
      "Step: 950\n",
      "Loss: 7.590\n",
      "Step time: 0.7088096141815186 seconds\n",
      "Doing Evaluation ..............................\n",
      "Dev MLM perplexity: 642.418\n",
      "Dev NSP perplexity: 2.012\n",
      "Evaluation time: 3.523393154144287 seconds\n",
      "Finished epoch 8 in 81.84094309806824\n",
      "Starting epoch 9\n",
      "Step: 975\n",
      "Loss: 7.624\n",
      "Step time: 0.6993868350982666 seconds\n",
      "Step: 1000\n",
      "Loss: 7.568\n",
      "Step time: 0.6980130672454834 seconds\n",
      "Step: 1025\n",
      "Loss: 7.648\n",
      "Step time: 0.9143342971801758 seconds\n",
      "Step: 1050\n",
      "Loss: 7.801\n",
      "Step time: 0.6960704326629639 seconds\n",
      "Doing Evaluation ..............................\n",
      "Dev MLM perplexity: 665.085\n",
      "Dev NSP perplexity: 2.013\n",
      "Evaluation time: 3.520737648010254 seconds\n",
      "Step: 1075\n",
      "Loss: 7.620\n",
      "Step time: 0.71848464012146 seconds\n",
      "Finished epoch 9 in 83.29717564582825\n",
      "Done in 861.7323451042175\n",
      "Final Evaluation ..............................\n",
      "Dev MLM perplexity: 682.171\n",
      "Dev NSP perplexity: 2.015\n",
      "Evaluation time: 3.5061471462249756 seconds\n"
     ]
    }
   ],
   "source": [
    "lr_policy = CosineAnnealing(NUM_EPOCHS * steps_per_epoch,\n",
    "                            warmup_ratio=LR_WARMUP_PROPORTION)\n",
    "neural_factory.train(tensors_to_optimize=[train_loss],\n",
    "                lr_policy=lr_policy,\n",
    "                callbacks=[callback_loss, callback_test],\n",
    "                #callbacks=[callback_loss],\n",
    "                batches_per_step=BATCHES_PER_STEP,\n",
    "                optimizer=OPTIMIZER,\n",
    "                optimization_params={\n",
    "                    \"batch_size\": BATCH_SIZE,\n",
    "                    \"num_epochs\": NUM_EPOCHS,\n",
    "                    \"lr\": LEARNING_RATE,\n",
    "                    \"weight_decay\": WEIGHT_DECAY,\n",
    "                    \"betas\": (0.95, 0.98),\n",
    "                    \"grad_norm_clip\": None\n",
    "                })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
