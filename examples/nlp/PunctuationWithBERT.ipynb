{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "import nemo\n",
    "from nemo.utils.lr_policies import WarmupAnnealing\n",
    "\n",
    "import nemo_nlp\n",
    "from nemo_nlp import NemoBertTokenizer\n",
    "from nemo_nlp.utils.callbacks.token_classification import \\\n",
    "    eval_iter_callback, eval_epochs_done_callback\n",
    "\n",
    "DATA_DIR = \"PATH_TO_WHERE_THE_DATA_IS\"\n",
    "WORK_DIR = \"PATH_TO_WHERE_TO_STORE_CHECKPOINTS_AND_LOGS\"\n",
    "PRETRAINED_BERT_MODEL = \"bert-base-cased\"\n",
    "\n",
    "# model parameters\n",
    "BATCHES_PER_STEP = 1\n",
    "BATCH_SIZE = 32\n",
    "CLASSIFICATION_DROPOUT = 0.1\n",
    "MAX_SEQ_LENGTH = 128\n",
    "NUM_EPOCHS = 3\n",
    "LEARNING_RATE = 0.00005\n",
    "LR_WARMUP_PROPORTION = 0.1\n",
    "OPTIMIZER = \"adam\"\n",
    "STEP_FREQ=200 # determines how often loss will be printed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Download and preprocess the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mkdir: cannot create directory ‘PATH_TO_WHERE_THE_DATA_IS’: File exists\n",
      "--2019-12-10 14:19:27--  https://downloads.tatoeba.org/exports/sentences.csv\n",
      "Resolving downloads.tatoeba.org (downloads.tatoeba.org)... 94.130.77.194\n",
      "Connecting to downloads.tatoeba.org (downloads.tatoeba.org)|94.130.77.194|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 440346898 (420M) [application/octet-stream]\n",
      "Saving to: ‘PATH_TO_WHERE_THE_DATA_IS/sentences.csv’\n",
      "\n",
      "PATH_TO_WHERE_THE_D 100%[===================>] 419.95M  18.0MB/s    in 24s     \n",
      "\n",
      "2019-12-10 14:19:52 (17.3 MB/s) - ‘PATH_TO_WHERE_THE_DATA_IS/sentences.csv’ saved [440346898/440346898]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# create DATA_DIR and download Tatoeba sentences\n",
    "! mkdir $DATA_DIR\n",
    "! wget -nc -O $DATA_DIR/sentences.csv https://downloads.tatoeba.org/exports/sentences.csv\n",
    "# extract English sentences\n",
    "! grep -P \"\\teng\\t\" $DATA_DIR/sentences.csv > $DATA_DIR/english_sentences.csv\n",
    "! rm $DATA_DIR/sentences.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To preprocess the data run NeMo/scripts/get_tatoeba_data.py\n",
    "# This can take a few minutes\n",
    "\n",
    "! python ../../scripts/get_tatoeba_data.py --num_lines_to_combine 5 --num_samples 15000 --data_dir $DATA_DIR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate neural factory with supported backend\n",
    "nf = nemo.core.NeuralModuleFactory(\n",
    "    backend=nemo.core.Backend.PyTorch,\n",
    "\n",
    "    # If you're training with multiple GPUs, you should handle this value with\n",
    "    # something like argparse. See examples/nlp/token_classification.py for an example.\n",
    "    local_rank=None,\n",
    "\n",
    "    # If you're training with mixed precision, this should be set to mxprO1 or mxprO2.\n",
    "    # See https://nvidia.github.io/apex/amp.html#opt-levels for more details.\n",
    "    optimization_level=\"O0\",\n",
    "    \n",
    "    # Define path to the directory you want to store your results\n",
    "    log_dir=WORK_DIR,\n",
    "\n",
    "    # If you're training with multiple GPUs, this should be set to\n",
    "    # nemo.core.DeviceType.AllGpu\n",
    "    placement=nemo.core.DeviceType.GPU)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If you're using a standard BERT model, you should do it like this. To see the full\n",
    "# list of BERT model names, check out nemo_nlp.huggingface.BERT.list_pretrained_models()\n",
    "tokenizer = NemoBertTokenizer(pretrained_model=PRETRAINED_BERT_MODEL)\n",
    "bert_model = nemo_nlp.huggingface.BERT(pretrained_model_name=PRETRAINED_BERT_MODEL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Describe training DAG\n",
    "train_data_layer = nemo_nlp.BertTokenClassificationDataLayer(\n",
    "        tokenizer=tokenizer,\n",
    "        text_file=os.path.join(DATA_DIR, 'text_train.txt'),\n",
    "        label_file=os.path.join(DATA_DIR, 'labels_train.txt'),\n",
    "        max_seq_length=MAX_SEQ_LENGTH,\n",
    "        batch_size=BATCH_SIZE,\n",
    "        pad_label=NONE_LABEL)\n",
    "\n",
    "label_ids = train_data_layer.dataset.label_ids\n",
    "num_classes = len(label_ids)\n",
    "\n",
    "hidden_size = bert_model.local_parameters[\"hidden_size\"]\n",
    "classifier = nemo_nlp.TokenClassifier(hidden_size=hidden_size,\n",
    "                                          num_classes=num_classes,\n",
    "                                          dropout=CLASSIFICATION_DROPOUT)\n",
    "\n",
    "task_loss = nemo_nlp.TokenClassificationLoss(d_model=hidden_size,\n",
    "                                            num_classes=len(label_ids),\n",
    "                                            dropout=CLASSIFICATION_DROPOUT)\n",
    "\n",
    "input_ids, input_type_ids, input_mask, loss_mask, _, labels = train_data_layer()\n",
    "\n",
    "hidden_states = bert_model(input_ids=input_ids,\n",
    "                           token_type_ids=input_type_ids,\n",
    "                           attention_mask=input_mask)\n",
    "\n",
    "logits = classifier(hidden_states=hidden_states)\n",
    "loss = task_loss(logits=logits, labels=labels, loss_mask=loss_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Describe evaluation DAG\n",
    "eval_data_layer = nemo_nlp.BertTokenClassificationDataLayer(\n",
    "        tokenizer=tokenizer,\n",
    "        text_file=os.path.join(DATA_DIR, 'text_dev.txt'),\n",
    "        label_file=os.path.join(DATA_DIR, 'labels_dev.txt'),\n",
    "        max_seq_length=MAX_SEQ_LENGTH,\n",
    "        batch_size=BATCH_SIZE,\n",
    "        pad_label=NONE_LABEL,\n",
    "        label_ids=label_ids)\n",
    "\n",
    "eval_input_ids, eval_input_type_ids, eval_input_mask, _, eval_subtokens_mask, eval_labels \\\n",
    "    = eval_data_layer()\n",
    "\n",
    "hidden_states = bert_model(\n",
    "    input_ids=eval_input_ids,\n",
    "    token_type_ids=eval_input_type_ids,\n",
    "    attention_mask=eval_input_mask)\n",
    "\n",
    "eval_logits = classifier(hidden_states=hidden_states)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create callbacks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "callback_train = nemo.core.SimpleLossLoggerCallback(\n",
    "    tensors=[loss],\n",
    "    print_func=lambda x: print(\"Loss: {:.3f}\".format(x[0].item())),\n",
    "    step_freq=STEP_FREQ)\n",
    "\n",
    "train_data_size = len(train_data_layer)\n",
    "\n",
    "# If you're training on multiple GPUs, this should be\n",
    "# train_data_size / (batch_size * batches_per_step * num_gpus)\n",
    "steps_per_epoch = int(train_data_size / (BATCHES_PER_STEP * BATCH_SIZE))\n",
    "\n",
    "# Callback to evaluate the model\n",
    "callback_eval = nemo.core.EvaluatorCallback(\n",
    "    eval_tensors=[eval_logits, eval_labels, eval_subtokens_mask],\n",
    "    user_iter_callback=lambda x, y: eval_iter_callback(x, y),\n",
    "    user_epochs_done_callback=lambda x: eval_epochs_done_callback(x, label_ids),\n",
    "    eval_step=steps_per_epoch)\n",
    "\n",
    "# Callback to store checkpoints\n",
    "ckpt_callback = nemo.core.CheckpointCallback(\n",
    "    folder=nf.checkpoint_dir,\n",
    "    epoch_freq=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_policy = WarmupAnnealing(NUM_EPOCHS * steps_per_epoch,\n",
    "                            warmup_ratio=LR_WARMUP_PROPORTION)\n",
    "\n",
    "nf.train(tensors_to_optimize=[loss],\n",
    "         callbacks=[callback_train, callback_eval, ckpt_callback],\n",
    "         lr_policy=lr_policy,\n",
    "         batches_per_step=BATCHES_PER_STEP,\n",
    "         optimizer=OPTIMIZER,\n",
    "         optimization_params={\"num_epochs\": NUM_EPOCHS,\n",
    "                              \"lr\": LEARNING_RATE})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the list of queiries for inference\n",
    "queries = ['we bought four shirts from the nvidia gear store in santa clara', \n",
    "           'tom sam and i are going to travel do you want to join',\n",
    "           'nvidia is a company',\n",
    "           'can i help you',\n",
    "           'we bought four shirts one mug and ten thousand titan rtx graphics cards the more you buy the more you save']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "infer_data_layer = nemo_nlp.BertTokenClassificationInferDataLayer(queries=queries,\n",
    "                                                                  tokenizer=tokenizer,\n",
    "                                                                  max_seq_length=MAX_SEQ_LENGTH,\n",
    "                                                                  batch_size=1)\n",
    "input_ids, input_type_ids, input_mask, _, subtokens_mask = infer_data_layer()\n",
    "\n",
    "hidden_states = bert_model(input_ids=input_ids,\n",
    "                                      token_type_ids=input_type_ids,\n",
    "                                      attention_mask=input_mask)\n",
    "logits = classifier(hidden_states=hidden_states)\n",
    "\n",
    "evaluated_tensors = nf.infer(tensors=[logits, subtokens_mask], checkpoint_dir=WORK_DIR + '/checkpoints')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# helper functions\n",
    "def concatenate(lists):\n",
    "    return np.concatenate([t.cpu() for t in lists])\n",
    "\n",
    "def get_preds(logits):\n",
    "    return np.argmax(logits, 1)\n",
    "\n",
    "ids_to_labels = {label_ids[k]: k for k in label_ids}\n",
    "\n",
    "logits, subtokens_mask = [concatenate(tensors) for tensors in evaluated_tensors]\n",
    "\n",
    "preds = np.argmax(logits, axis=2)\n",
    "\n",
    "for i, query in enumerate(queries):\n",
    "    nf.logger.info(f'Query: {query}')\n",
    "\n",
    "    pred = preds[i][subtokens_mask[i] > 0.5]\n",
    "    words = query.strip().split()\n",
    "    if len(pred) != len(words):\n",
    "        raise ValueError('Pred and words must be of the same length')\n",
    "\n",
    "    output = ''\n",
    "    for j, word in enumerate(words):\n",
    "        label = ids_to_labels[pred[j]]\n",
    "    \n",
    "        if label != NONE_LABEL:\n",
    "            if 'U' in label:\n",
    "                word = word.capitalize()\n",
    "            if label[0] != 'O':\n",
    "                word += label[0]\n",
    "            \n",
    "        output += word\n",
    "        output += ' '\n",
    "    nf.logger.info(f'Combined: {output.strip()}\\n')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
