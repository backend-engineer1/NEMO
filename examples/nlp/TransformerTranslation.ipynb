{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NeMo core\n",
    "import nemo\n",
    "# NeMo NLP collection\n",
    "import nemo_nlp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "nf = nemo.core.NeuralModuleFactory()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_dir = \"/home/okuchaiev/repos/gitlab-master/nemo/tests/data/\"\n",
    "tokenizer = nemo_nlp.data.SentencePieceTokenizer(dataset_dir+\"m_common.model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizing dataset ...\n",
      "Tokenizing dataset ...\n"
     ]
    }
   ],
   "source": [
    "train_dl = nemo_nlp.data.TranslationDataLayer(\n",
    "        tokenizer_src=tokenizer,\n",
    "        tokenizer_tgt=tokenizer,\n",
    "        dataset_src=dataset_dir+\"en_de/train.de\",\n",
    "        dataset_tgt=dataset_dir+\"en_de/train.en\",\n",
    "        tokens_in_batch=4096,\n",
    "        clean=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizing dataset ...\n",
      "Tokenizing dataset ...\n"
     ]
    }
   ],
   "source": [
    "eval_dl = nemo_nlp.data.TranslationDataLayer(\n",
    "        tokenizer_src=tokenizer,\n",
    "        tokenizer_tgt=tokenizer,\n",
    "        dataset_src=dataset_dir+\"en_de/test.de\",\n",
    "        dataset_tgt=dataset_dir+\"en_de/test.en\",\n",
    "        tokens_in_batch=4096,\n",
    "        clean=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:When constructing TransformerEncoderNM. The base NeuralModule class received the following unused arguments:\n",
      "WARNING:root:dict_keys(['d_model', 'd_inner', 'num_layers', 'num_attn_heads', 'fully_connected_dropout', 'vocab_size', 'max_seq_length', 'embedding_dropout'])\n",
      "WARNING:root:When constructing TransformerDecoderNM. The base NeuralModule class received the following unused arguments:\n",
      "WARNING:root:dict_keys(['d_model', 'd_inner', 'num_layers', 'num_attn_heads', 'fully_connected_dropout', 'vocab_size', 'max_seq_length', 'embedding_dropout'])\n",
      "WARNING:root:When constructing TransformerLogSoftmaxNM. The base NeuralModule class received the following unused arguments:\n",
      "WARNING:root:dict_keys(['vocab_size', 'd_model'])\n",
      "WARNING:root:When constructing BeamSearchTranslatorNM. The base NeuralModule class received the following unused arguments:\n",
      "WARNING:root:dict_keys(['encoder', 'max_seq_length', 'beam_size', 'length_penalty', 'bos_token', 'pad_token', 'eos_token'])\n",
      "WARNING:root:When constructing PaddedSmoothedCrossEntropyLossNM. The base NeuralModule class received the following unused arguments:\n",
      "WARNING:root:dict_keys(['pad_id', 'smoothing'])\n",
      "WARNING:root:When constructing PaddedSmoothedCrossEntropyLossNM. The base NeuralModule class received the following unused arguments:\n",
      "WARNING:root:dict_keys(['pad_id', 'smoothing'])\n"
     ]
    }
   ],
   "source": [
    "d_model=512\n",
    "d_inner=2048\n",
    "num_layers=6\n",
    "num_heads=8\n",
    "dp = 0.1\n",
    "max_seq_len = 256\n",
    "\n",
    "t_encoder = nemo_nlp.TransformerEncoderNM(\n",
    "        d_model=d_model,\n",
    "        d_inner=d_inner,\n",
    "        num_layers=num_layers,\n",
    "        num_attn_heads=num_heads,\n",
    "        fully_connected_dropout=dp,\n",
    "        vocab_size=tokenizer.vocab_size,\n",
    "        max_seq_length=max_seq_len,\n",
    "        embedding_dropout=dp)\n",
    "\n",
    "t_decoder = nemo_nlp.TransformerDecoderNM(\n",
    "        d_model=d_model,\n",
    "        d_inner=d_inner,\n",
    "        num_layers=num_layers,\n",
    "        num_attn_heads=num_heads,\n",
    "        fully_connected_dropout=dp,\n",
    "        vocab_size=tokenizer.vocab_size,\n",
    "        max_seq_length=max_seq_len,\n",
    "        embedding_dropout=dp)\n",
    "\n",
    "t_log_softmax = nemo_nlp.TransformerLogSoftmaxNM(\n",
    "    vocab_size=tokenizer.vocab_size,\n",
    "    d_model=d_model)\n",
    "\n",
    "beam_translator = nemo_nlp.BeamSearchTranslatorNM(\n",
    "    encoder=t_encoder,\n",
    "    decoder=t_decoder,\n",
    "    log_softmax=t_log_softmax,\n",
    "    max_seq_length=max_seq_len,\n",
    "    beam_size=4,\n",
    "    length_penalty=0.0,\n",
    "    bos_token=2, pad_token=0, eos_token=1)\n",
    "\n",
    "t_loss = nemo_nlp.PaddedSmoothedCrossEntropyLossNM(pad_id=0, smoothing=0.1)\n",
    "t_loss_eval = nemo_nlp.PaddedSmoothedCrossEntropyLossNM(pad_id=0, smoothing=0.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "bert = nemo_nlp.huggingface.BERT(vocab_size=tokenizer.vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tie weights of softmax layer and embedding layers from encoder and decoder\n",
    "t_log_softmax.tie_weights_with(\n",
    "    t_encoder,\n",
    "    weight_names=[\"log_softmax.dense.weight\"],\n",
    "    name2name_and_transform={\n",
    "        \"log_softmax.dense.weight\": (\"embeddings.word_embedding.weight\", 0)\n",
    "    }\n",
    ")\n",
    "t_decoder.tie_weights_with(\n",
    "    t_encoder,\n",
    "    weight_names=[\"embeddings.word_embedding.weight\"],\n",
    "    name2name_and_transform={\n",
    "        \"embeddings.word_embedding.weight\": (\n",
    "            \"embeddings.word_embedding.weight\", 0)\n",
    "    }\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training pipeline\n",
    "src, src_mask, tgt, tgt_mask, labels, sent_ids = train_dl()\n",
    "src_hiddens = t_encoder(input_ids=src, input_mask_src=src_mask)\n",
    "tgt_hiddens = t_decoder(input_ids_tgt=tgt,\n",
    "                        hidden_states_src=src_hiddens,\n",
    "                        input_mask_src=src_mask,\n",
    "                        input_mask_tgt=tgt_mask)\n",
    "log_softmax = t_log_softmax(hidden_states=tgt_hiddens)\n",
    "train_loss = t_loss(log_probs=log_softmax, target_ids=labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluation pipeline with beam search on top of the model output\n",
    "src_, src_mask_, tgt_, tgt_mask_, labels_, sent_ids_ = eval_dl()\n",
    "src_hiddens_ = t_encoder(input_ids=src_, input_mask_src=src_mask_)\n",
    "tgt_hiddens_ = t_decoder(input_ids_tgt=tgt_,\n",
    "                         hidden_states_src=src_hiddens_,\n",
    "                         input_mask_src=src_mask_,\n",
    "                         input_mask_tgt=tgt_mask_)\n",
    "log_softmax_ = t_log_softmax(hidden_states=tgt_hiddens_)\n",
    "eval_loss = t_loss_eval(log_probs=log_softmax_, target_ids=labels_)\n",
    "beam_trans = beam_translator(\n",
    "    hidden_states_src=src_hiddens_, input_mask_src=src_mask_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# callback which prints training loss once in a while\n",
    "callback = nemo.core.SimpleLossLoggerCallback(\n",
    "    tensor_list2string=lambda x: str(x[0].item()),\n",
    "    step_frequency=100)\n",
    "# callback which calculates evaluation loss without label smoothing\n",
    "# and SacreBLEU score between outputs of beam search and reference translations\n",
    "from nemo_nlp.callbacks.translation import eval_iter_callback, \\\n",
    "    eval_epochs_done_callback\n",
    "callback_dev = nemo.core.EvaluatorCallback(\n",
    "    eval_tensors=[tgt_, eval_loss, beam_trans, sent_ids_],\n",
    "    user_iter_callback=lambda x, y: eval_iter_callback(x, y, tokenizer),\n",
    "    user_epochs_done_callback=lambda x: eval_epochs_done_callback(x),\n",
    "    eval_step=50)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nemo.utils.lr_policies import CosineAnnealing\n",
    "lr_policy = CosineAnnealing(10000, warmup_steps=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Data Layer does not have any weights to return. This get_weights call returns None.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting .....\n",
      "Starting epoch 0\n",
      "Step: 0\n",
      "Train Loss: 10.435689926147461\n",
      "Step time: 0.37077879905700684 seconds\n",
      "Doing Evaluation .................................\n",
      "Ground truth: \"The greatest enemies of the press freedom are not evil and wicked politicians, but bad journalists depending on profit, blackmail and extortion\" he said.\n",
      "\n",
      "Translation:  ChinaChinaChinaChina BronChinaChina BronChinaChinaChina BronChina Bron Bron BronChina Bron Bron Bron Bron Bron Bron BronChina Bron Bron Bron Bron Bron Bron Bron Bron Bron Bronstruction Bron unreststruction Bron unreststruction Bron unrest unreststruction Bron\n",
      "\n",
      "Ground truth: A bus full of white people, who sing songs in a black language - this degree of recognition brings not only morale and joy, but some grim-faced border soldiers even shed a few tears.\n",
      "\n",
      "Translation:  Support Support Bron Support Bron Support Bron Support Bron analy analy analy analy analy analy analy analy analy analy analy analy analy analy analy analy analy analy analy analy Support Support Support Support Support Support Support Bron Support Support Support Support Support Support Bron Support Bron Support\n",
      "\n",
      "Ground truth: \"At least they don't drop cigarette ends on your magnificent carpet, like some beautiful pop music celebrities do,\" says Baroness Fiona Thyssen-Bornemisza.\n",
      "\n",
      "Translation:  uageuageuage cosmet cosmet cosmetuage cosmet cosmet cosmet cosmet cosmet cosmet cosmet cosmet cosmet cosmet cosmet cosmet cosmet cosmet cosmet cosmet cosmet cosmet cosmet cosmet cosmet cosmet cosmet cosmet cosmet cosmet cosmet cosmet cosmet cosmet cosmet cosmet\n",
      "\n",
      "------------------------------------------------------------\n",
      "     Validation loss: 10.432\n",
      "Validation FAIR BLEU: 0.002066030687978538\n",
      "Validation SacreBLEU: 0.002140152460370101\n",
      "------------------------------------------------------------\n",
      "Evaluation time: 25.10103988647461 seconds\n",
      "Finished epoch 0 in 32.28424882888794\n",
      "Starting epoch 1\n",
      "Doing Evaluation .................................\n",
      "Ground truth: In Europe there are diverse opinions regarding the Palestinian initiative.\n",
      "\n",
      "Translation:  the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the\n",
      "\n",
      "Ground truth: Pierre Noizat, also author of an educational book on this currency, has a lot of faith in the potential of this technology as a transaction network.\n",
      "\n",
      "Translation:  the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the\n",
      "\n",
      "Ground truth: I remember it perfectly.\n",
      "\n",
      "Translation:  the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the\n",
      "\n",
      "------------------------------------------------------------\n",
      "     Validation loss: 9.372\n",
      "Validation FAIR BLEU: 0.009671795387711133\n",
      "Validation SacreBLEU: 0.009671795387711133\n",
      "------------------------------------------------------------\n",
      "Evaluation time: 25.914916276931763 seconds\n",
      "Finished epoch 1 in 32.997509479522705\n",
      "Starting epoch 2\n",
      "Step: 100\n",
      "Train Loss: 8.465082168579102\n",
      "Step time: 0.13677358627319336 seconds\n",
      "Doing Evaluation .................................\n",
      "Ground truth: Hydro-laboratory of CPK - mandatory phase of training for a flight.\n",
      "\n",
      "Translation:  ,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,\n",
      "\n",
      "Ground truth: \"Miss Wyatt,\" said the don, Harry Pitt (now deceased), \"please translate the first paragraph.\"\n",
      "\n",
      "Translation:  ,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,\n",
      "\n",
      "Ground truth: The thoughts of the young engineer (born 1905) left his audience cold, and years later Sternfeld remembered that only Dr. Jan Gadomski had shown an interest in his work.\n",
      "\n",
      "Translation:  ,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,\n",
      "\n",
      "------------------------------------------------------------\n",
      "     Validation loss: 8.38\n",
      "Validation FAIR BLEU: 0.0098490831473666\n",
      "Validation SacreBLEU: 0.0098490831473666\n",
      "------------------------------------------------------------\n",
      "Evaluation time: 25.69144582748413 seconds\n",
      "Finished epoch 2 in 32.70426535606384\n",
      "Starting epoch 3\n",
      "Doing Evaluation .................................\n",
      "Ground truth: Snowploughs were brought out by falling snow overnight, the Sumperk region, according to highway maintenance, got around three centimetres of snow.\n",
      "\n",
      "Translation:  \n",
      "\n",
      "Ground truth: Stefan Spermon, initially a sceptic of the IT sector, has already ventured into the new library.\n",
      "\n",
      "Translation:  \n",
      "\n",
      "Ground truth: One particularly special feature are the black book shelves, which simultaneously act as wall cladding, parapets and railings for the stairway.\n",
      "\n",
      "Translation:  the,,,,,,,,,,, the,,,, the,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,, the\n",
      "\n",
      "------------------------------------------------------------\n",
      "     Validation loss: 8.086\n",
      "Validation FAIR BLEU: 0.03201967601955866\n",
      "Validation SacreBLEU: 0.033122595361421925\n",
      "------------------------------------------------------------\n",
      "Evaluation time: 17.9498450756073 seconds\n",
      "Finished epoch 3 in 25.15358328819275\n",
      "Starting epoch 4\n",
      "Step: 200\n",
      "Train Loss: 7.524139881134033\n",
      "Step time: 0.14318251609802246 seconds\n",
      "Doing Evaluation .................................\n",
      "Ground truth: \"He was born in 1805, here in the region, and died in 1973,\" explains Fatulayeva.\n",
      "\n",
      "Translation:  , the,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,\n",
      "\n",
      "Ground truth: Just imagine...\n",
      "\n",
      "Translation:  ,, the,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,\n",
      "\n",
      "Ground truth: What is not quite so apparent is whether these were the people, who had chosen their path independently, or whether behind their throne stood someone who directed their actions towards a pre-calculated goal.\n",
      "\n",
      "Translation:  , the,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,\n",
      "\n",
      "------------------------------------------------------------\n",
      "     Validation loss: 7.669\n",
      "Validation FAIR BLEU: 0.04129106265434148\n",
      "Validation SacreBLEU: 0.04129106265434148\n",
      "------------------------------------------------------------\n",
      "Evaluation time: 25.831955671310425 seconds\n",
      "Finished epoch 4 in 32.54558563232422\n",
      "Starting epoch 5\n",
      "Doing Evaluation .................................\n",
      "Ground truth: However, the decision opens the door for the leading glass manufacturer in Mexico to appeal to the Supreme Court of the United States, claiming three inconsistencies.\n",
      "\n",
      "Translation:  .\n",
      "\n",
      "Ground truth: From my debut at the Clairefontaine INF pre-training centre to my transfer to Saint-Etienne, I've always moved step by step.\n",
      "\n",
      "Translation:  .\n",
      "\n",
      "Ground truth: The work week starts on Sunday morning.\n",
      "\n",
      "Translation:  .\n",
      "\n",
      "------------------------------------------------------------\n",
      "     Validation loss: 7.496\n",
      "Validation FAIR BLEU: 0.0\n",
      "Validation SacreBLEU: 0.0\n",
      "------------------------------------------------------------\n",
      "Evaluation time: 16.397782802581787 seconds\n",
      "Finished epoch 5 in 23.29615306854248\n",
      "Starting epoch 6\n",
      "Step: 300\n",
      "Train Loss: 7.1792097091674805\n",
      "Step time: 0.16086292266845703 seconds\n",
      "Doing Evaluation .................................\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-12-3009fd12feb6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     13\u001b[0m                 \u001b[0mtensors_to_evaluate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m                 \u001b[0mlr_policy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlr_policy\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m                 callbacks=[callback, callback_dev])\n\u001b[0m",
      "\u001b[0;32m~/repos/gitlab-master/nemo/nemo/nemo/backends/pytorch/actions.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, tensors_to_optimize, tensors_to_evaluate, callbacks, lr_policy, batches_per_step, stop_on_nan_loss)\u001b[0m\n\u001b[1;32m    841\u001b[0m                         \u001b[0mregistered_tensors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mregistered_tensors\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    842\u001b[0m                     )\n\u001b[0;32m--> 843\u001b[0;31m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_perform_on_iteration_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    844\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    845\u001b[0m             \u001b[0;31m# End of epoch for loop\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/repos/gitlab-master/nemo/nemo/nemo/core/neural_factory.py\u001b[0m in \u001b[0;36m_perform_on_iteration_end\u001b[0;34m(self, callbacks)\u001b[0m\n\u001b[1;32m    146\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mcallback\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    147\u001b[0m                 \u001b[0mcallback\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_local_rank\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlocal_rank\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 148\u001b[0;31m                 \u001b[0mcallback\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_iteration_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    149\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    150\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_perform_on_action_start\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/repos/gitlab-master/nemo/nemo/nemo/core/callbacks.py\u001b[0m in \u001b[0;36mon_iteration_end\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    343\u001b[0m                     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Doing Evaluation .................................\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    344\u001b[0m                 \u001b[0mstart_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 345\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compute_callback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_eval_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    346\u001b[0m                 \u001b[0melapsed_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mstart_time\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    347\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_local_rank\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_local_rank\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/repos/gitlab-master/nemo/nemo/nemo/backends/pytorch/actions.py\u001b[0m in \u001b[0;36m_eval\u001b[0;34m(self, tensors_2_evaluate, callback, step, verbose)\u001b[0m\n\u001b[1;32m    452\u001b[0m                     \u001b[0mcall_chain\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcall_chain\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    453\u001b[0m                     \u001b[0mregistered_tensors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mregistered_e_tensors\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 454\u001b[0;31m                     \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mModelMode\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meval\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    455\u001b[0m                 )\n\u001b[1;32m    456\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/repos/gitlab-master/nemo/nemo/nemo/backends/pytorch/actions.py\u001b[0m in \u001b[0;36m__nm_graph_forward_pass\u001b[0;34m(self, call_chain, registered_tensors, mode)\u001b[0m\n\u001b[1;32m    285\u001b[0m                 \u001b[0mnew_tensors\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mcall_set\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    286\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 287\u001b[0;31m                 \u001b[0mnew_tensors\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mforce_pt\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mcall_set\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    288\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    289\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnew_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mList\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/repos/gitlab-master/nemo/nemo/nemo/backends/pytorch/nm.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, force_pt, *input, **kwargs)\u001b[0m\n\u001b[1;32m     45\u001b[0m         \u001b[0mpt_call\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mforce_pt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mpt_call\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 47\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mModule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     48\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mNeuralModule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/py37/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    491\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    492\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 493\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    494\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    495\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/repos/gitlab-master/nemo/collections/nemo_nlp/nemo_nlp/transformer_nm.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states_src, input_mask_src)\u001b[0m\n\u001b[1;32m    344\u001b[0m         output_ids = self.generator(\n\u001b[1;32m    345\u001b[0m             \u001b[0mencoder_hidden_states\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mhidden_states_src\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 346\u001b[0;31m             encoder_input_mask=input_mask_src)\n\u001b[0m\u001b[1;32m    347\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0moutput_ids\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    348\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/py37/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    491\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    492\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 493\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    494\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    495\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/py37/lib/python3.7/site-packages/torch/autograd/grad_mode.py\u001b[0m in \u001b[0;36mdecorate_no_grad\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     41\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mdecorate_no_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 43\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     44\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mdecorate_no_grad\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/repos/gitlab-master/nemo/collections/nemo_nlp/nemo_nlp/transformer/generators.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, decoder_input_ids, encoder_hidden_states, encoder_input_mask)\u001b[0m\n\u001b[1;32m    198\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    199\u001b[0m             \u001b[0;31m# if all hypotheses end with </s>, interrupt hypotheses search\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 200\u001b[0;31m             \u001b[0;32mif\u001b[0m \u001b[0mpad_profile\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mbatch_size\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbeam_size\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    201\u001b[0m                 \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    202\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# define and launch training algorithm (optimizer)\n",
    "optimizer = nf.get_trainer(\n",
    "    params={\n",
    "        \"optimizer_kind\": \"novograd\",\n",
    "        \"optimization_params\":{\n",
    "            \"num_epochs\": 40,\n",
    "            \"lr\": 0.01,\n",
    "            \"weight_decay\": 0.0\n",
    "        }\n",
    "    }\n",
    ")\n",
    "optimizer.train(tensors_to_optimize=[train_loss],\n",
    "                tensors_to_evaluate=[],\n",
    "                lr_policy=lr_policy,\n",
    "                callbacks=[callback, callback_dev])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
