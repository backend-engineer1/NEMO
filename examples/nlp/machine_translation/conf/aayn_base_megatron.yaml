name: MegatronNMT
do_training: True # set to False if only preprocessing data
do_testing: False # set to True to run evaluation on test data after training

trainer:
  devices: 1
  num_nodes: 1
  precision: 16 # Should be set to 16 for O1 and O2, default is 16 as PT ignores it when am_level is O0
  accelerator: gpu
  enable_checkpointing: False
  logger: False
  replace_sampler_ddp: False
  max_steps: 400000 # consumed_samples = global_step * micro_batch_size * data_parallel_size * accumulate_grad_batches
  log_every_n_steps: 10
  val_check_interval: 1000
  accumulate_grad_batches: 1
  gradient_clip_val: 1.0

exp_manager:
  explicit_log_dir: null
  exp_dir: null
  name: megatron_nmt
  create_wandb_logger: False
  wandb_logger_kwargs:
    project: null
    name: null
  resume_if_exists: True
  resume_ignore_no_checkpoint: True
  create_checkpoint_callback: True
  checkpoint_callback_params:
    monitor: val_loss
    save_top_k: 10
    mode: min
    save_best_model: True
    always_save_nemo: False # saves nemo file during validation, not implemented for model parallel
    model_parallel_size: ${multiply:${model.tensor_model_parallel_size}, ${model.pipeline_model_parallel_size}}

model:
  # NMT Params
  multilingual: False
  label_smoothing: 0.1 # TODO: Implement this.
  shared_tokenizer: True # train tokenizer model across src and tgt train data
  preproc_out_dir: null # path to store data preprocessing outputs
  src_language: 'en'
  tgt_language: 'de'
  max_generation_delta: 20 # Maximum decoder sequence length is encoder sequence length + this parameter.
  micro_batch_size: 0 # This is just here to keep compute_consumed_samples() happy.

  # Megatron params
  tensor_model_parallel_size: 1
  pipeline_model_parallel_size: 1
  seq_length: 512
  max_position_embeddings: ${.seq_length}
  num_layers: 12
  hidden_size: 768
  ffn_hidden_size: 3072 # Transformer FFN hidden size. Usually 4 * hidden_size.
  num_attention_heads: 12
  init_method_std: 0.02 # Standard deviation of the zero mean normal distribution used for weight initialization.')
  hidden_dropout: 0.1 # Dropout probability for hidden state transformer.
  attention_dropout: 0.1 # Dropout probability in the attention layer.
  kv_channels: null # Projection weights dimension in multi-head attention. Set to hidden_size // num_attention_heads if null
  apply_query_key_layer_scaling: True # scale Q * K^T by 1 / layer-number.
  layernorm_epsilon: 1e-5
  persist_layer_norm: True # Use of persistent fused layer norm kernel.
  gradient_as_bucket_view: True # Allocate gradients in a contiguous bucket to save memory (less fragmentation and buffer memory)
  encoder_arch: 'transformer'
  decoder_arch: 'transformer'
  activation: 'gelu'
  masked_softmax_fusion: False # This needs to be False to avoid an issue with the fused softmax kernel with variable sequence lengths.
  bias_gelu_fusion: True

  train_ds:
    src_file_name: null
    tgt_file_name: null
    use_tarred_dataset: True # if true tar_file_name and meta_file_name will be used (or created automatically) 
    # config for preprocessing training data and creating a tarred datset automatically
    tar_file_prefix: parallel # prefix for tar file names
    tar_files: null # if data has already been preprocessed (rest of config ignored)
    metadata_file: null # metadata for tarred dataset
    lines_per_dataset_fragment: 1000000 # Number of lines to consider for bucketing and padding
    num_batches_per_tarfile: 100 # Number of batches (pickle files) within each tarfile
    tar_shuffle_n: 100 # How many samples to look ahead and load to be shuffled
    shard_strategy: scatter # tarred dataset shard distribution strategy
    n_preproc_jobs: -2 # number of processes to use for data preprocessing (-2 means all but 2)
    tokens_in_batch: 512
    clean: true
    max_seq_length: 512
    shuffle: true
    num_samples: -1
    drop_last: false
    pin_memory: false
    num_workers: 8
    concat_sampling_technique: temperature # only used with ConcatTranslationDataset 
    concat_sampling_temperature: 5 # only used with ConcatTranslationDataset 
    concat_sampling_probabilities: null # only used with ConcatTranslationDataset 
    reverse_lang_direction: false

  validation_ds:
    src_file_name: ???
    tgt_file_name: ???
    tokens_in_batch: 512
    clean: false
    max_seq_length: 512
    shuffle: false
    num_samples: -1
    drop_last: false
    pin_memory: false
    num_workers: 8

  test_ds:
    src_file_name: ???
    tgt_file_name: ???
    tokens_in_batch: 512
    clean: false
    max_seq_length: 512
    shuffle: false
    num_samples: -1
    drop_last: false
    pin_memory: false
    num_workers: 8

  optim:
    name: fused_adam
    lr: 0.001
    betas:
      - 0.9
      - 0.98
    weight_decay: 0.0
    sched:
      name: InverseSquareRootAnnealing
      min_lr: 0.0
      last_epoch: -1
      warmup_ratio: 0.1

  encoder_tokenizer:
    library: yttm
    tokenizer_model: null
    vocab_size: null # vocab size for training bpe
    bpe_dropout: null
    vocab_file: null
    special_tokens: null
    training_sample_size: null # valid for sentencepiece tokenizer
    r2l: false

  decoder_tokenizer:
    library: yttm
    tokenizer_model: null
    vocab_size: null # vocab size for training bpe
    bpe_dropout: null
    vocab_file: null
    special_tokens: null
    training_sample_size: null # valid for sentencepiece tokenizer
    r2l: false
