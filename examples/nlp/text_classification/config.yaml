# Sentence classification with pretrained BERT models
batch_size: 32

# IO
work_dir: ./outputs
checkpoint_dir: null

# Data
data_dir: /raid/data/nlp/SST-2 #/path/to/data
train_file_prefix: train
eval_file_prefix: dev
shuffle: true # shuffle training data
num_train_samples: -1
num_val_samples: -1
num_workers: 2
pin_memory: false
use_cache: true

# Language Model
pretrained_model_name: roberta-base
bert_checkpoint: null
bert_config: null
# TODO: we can specialize config based on other config values at runtime:
# https://hydra.cc/docs/next/patterns/specializing_config
tokenizer: nemobert # only used if using custom checkpoint
tokenizer_model: null # only used if tokenizer is sentencepiece
do_lower_case: null # true for uncased models, false for cased models
max_seq_length: 36

# Classifier
num_output_layers: 2
fc_dropout: 0.1
class_balancing: null # or weighted_loss

# Pytorch Lightning
pl:
  trainer:
    gpus: 1 # the number of gpus, 0 for CPU
    max_epochs: 100
    max_steps: null # precedence over max_epochs
    accumulate_grad_batches: 1 # accumulates grads every k batches
    amp_level: O0 # O1/O2 for mixed precision
    distributed_backend: ddp

# Validation
save_epoch_freq: 1
save_step_freq: -1
loss_step_freq: 25
eval_epoch_freq: 1

# Optimizer
optim:
  name: adam
  lr: 2e-5
  args:
    name: auto
    params:
      weight_decay: 0.01
