{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inference Pipeline\n",
    "\n",
    "For inference we instantiate the same neural modules but now we will be using the checkpoints that we just learned."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nemo.core import NeuralModuleFactory\n",
    "from nemo.collections.nlp.data.datasets.text_classification_dataset import BertTextClassificationDataset\n",
    "from nemo.collections.nlp.nm.data_layers.text_classification_datalayer import BertTextClassificationDataLayer\n",
    "from nemo.collections.nlp.nm.trainables.common.huggingface.bert_nm import BERT\n",
    "from nemo.collections.nlp.nm.trainables.common.sequence_classification_nm import SequenceClassifier\n",
    "from pytorch_transformers import BertTokenizer\n",
    "import torch.nn.functional as f\n",
    "import torch\n",
    "\n",
    "import json\n",
    "import math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "pd.options.display.max_colwidth = -1\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.decomposition import PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pretrained_bert_model = 'bert-base-uncased'\n",
    "#pretrained_bert_model = 'bert-large-uncased'\n",
    "\n",
    "log_dir = 'logs/' + pretrained_bert_model\n",
    "checkpoint_dir = log_dir + '/checkpoints'\n",
    "bert_model_config_path = log_dir + '/' + pretrained_bert_model + '_config.json'\n",
    "inference_log_dir = log_dir + '/inference'\n",
    "data_dir = 'data/SST-2/split'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls -lh $checkpoint_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nf = NeuralModuleFactory(log_dir=inference_log_dir,\n",
    "                                   optimization_level='O1')\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained(pretrained_bert_model)\n",
    "\n",
    "bert = BERT(config_filename=bert_model_config_path)\n",
    "\n",
    "bert_hidden_size = bert.local_parameters['hidden_size']\n",
    "\n",
    "mlp = SequenceClassifier(hidden_size=bert_hidden_size,\n",
    "                                  num_classes=2,\n",
    "                                  num_layers=2,\n",
    "                                  log_softmax=False,\n",
    "                                  dropout=0.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bert.restore_from(checkpoint_dir + '/BERT-EPOCH-3.pt')\n",
    "mlp.restore_from(checkpoint_dir + '/SequenceClassifier-EPOCH-3.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_seq_length = 64\n",
    "\n",
    "if pretrained_bert_model == 'bert-base-uncased':\n",
    "    batch_size = 256\n",
    "if pretrained_bert_model == 'bert-large-uncased':\n",
    "    batch_size = 64\n",
    "    \n",
    "test_data = BertTextClassificationDataLayer(\n",
    "    input_file=data_dir + '/test.tsv',\n",
    "    tokenizer=tokenizer,\n",
    "    max_seq_length=max_seq_length,\n",
    "    shuffle=False,\n",
    "    num_samples=-1, # lower for dev, -1 for all dataset\n",
    "    batch_size=64\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_input, test_token_types, test_attn_mask, _ = test_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_embeddings = bert(input_ids=test_input,\n",
    "                        token_type_ids=test_token_types,\n",
    "                        attention_mask=test_attn_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_logits = mlp(hidden_states=test_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "test_logits_tensors = nf.infer(tensors=[test_logits])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_probs = f.softmax(torch.cat(test_logits_tensors[0])).numpy()[:, 1] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df = pd.read_csv(data_dir + '/test.tsv', sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df['prob'] = test_probs "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df.to_csv(inference_log_dir + '/test_inference.tsv', sep='\\t', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_classification(data_path):\n",
    "    df = pd.read_csv(data_path, sep='\\t')\n",
    "    sample = df.sample()\n",
    "    sentence = sample.sentence.values[0]\n",
    "    prob = sample.prob.values[0]\n",
    "    result = f'{sentence} | {prob}'\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_classification(inference_log_dir + '/test_inference.tsv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_samples = 10\n",
    "for _ in range(num_samples):\n",
    "    print(sample_classification(inference_log_dir + '/test_inference.tsv'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BERT nails it:\n",
    "the film is just a big , gorgeous , mind-blowing , breath-taking mess . | 0.2738656\n",
    "\n",
    "a sensual performance from abbass buoys the flimsy story , but her inner journey is largely unexplored and we 're left wondering about this exotic-looking woman whose emotional depths are only hinted at . | 0.48260054"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classify my sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classify_sentence(nf, tokenizer, bert, mlp, sentence):\n",
    "    sentence = sentence.lower()\n",
    "    tmp_file = \"data/tmp_sentence.tsv\"\n",
    "    with open(tmp_file, 'w+') as tmp_tsv:\n",
    "        header = 'sentence\\tlabel\\n'\n",
    "        line = sentence + '\\t0\\n'\n",
    "        tmp_tsv.writelines([header, line])\n",
    "\n",
    "    tmp_data = BertTextClassificationDataLayer(\n",
    "        input_file=tmp_file,\n",
    "        tokenizer=tokenizer,\n",
    "        max_seq_length=128,\n",
    "        shuffle=False,\n",
    "        num_samples=-1, # lower for dev, -1 for all dataset\n",
    "        batch_size=1\n",
    "    )\n",
    "    tmp_input, tmp_token_types, tmp_attn_mask, _ = tmp_data()\n",
    "    tmp_embeddings = bert(input_ids=tmp_input,\n",
    "                            token_type_ids=tmp_token_types,\n",
    "                            attention_mask=tmp_attn_mask)\n",
    "    tmp_logits = mlp(hidden_states=tmp_embeddings)\n",
    "    tmp_logits_tensors = nf.infer(tensors=[tmp_logits, tmp_embeddings])\n",
    "    tmp_probs = f.softmax(torch.cat(tmp_logits_tensors[0])).numpy()[:, 1] \n",
    "    print(f'{sentence} | {tmp_probs[0]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sentence = 'point break is the best movie of all time'\n",
    "#sentence = 'the movie was a wonderful exercise in understanding the struggles of native americans'\n",
    "#sentence = 'the performance of diego luna had me excited and annoyed at the same time'\n",
    "sentence = 'matt damon is the only good thing about this film'\n",
    "classify_sentence(nf, tokenizer, bert, mlp, sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Understanding and Visualizing BERT Embeddings\n",
    "\n",
    "Now that we've fine-tuned our BERT model, let's see if the word embeddings have changed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = 'data/positive_negative.tsv'\n",
    "# positive negative spectrum\n",
    "spectrum_data = BertTextClassificationDataLayer(\n",
    "    input_file=data_path,\n",
    "    tokenizer=tokenizer,\n",
    "    max_seq_length=max_seq_length,\n",
    "    shuffle=False,\n",
    "    num_samples=-1, # lower for dev, -1 for all dataset\n",
    "    batch_size=batch_size,\n",
    "    dataset_type=BertTextClassificationDataset\n",
    ")\n",
    "\n",
    "spectrum_input, spectrum_token_types, spectrum_attn_mask, spectrum_labels = spectrum_data()\n",
    "\n",
    "spectrum_embeddings = bert(input_ids=spectrum_input,\n",
    "                        token_type_ids=spectrum_token_types,\n",
    "                        attention_mask=spectrum_attn_mask)\n",
    "\n",
    "spectrum_embeddings_tensors = nf.infer(tensors=[spectrum_embeddings])\n",
    "\n",
    "plt.figure(figsize=(100,100))\n",
    "plt.imshow(spectrum_embeddings_tensors[0][0][:,0,:].numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spectrum_df = pd.read_csv(data_path, delimiter='\\t')\n",
    "\n",
    "spectrum_activations = spectrum_embeddings_tensors[0][0][:,0,:].numpy()\n",
    "tsne_spectrum = TSNE(n_components=2, perplexity=10, verbose=1, learning_rate=2,\n",
    "                     random_state=123).fit_transform(spectrum_activations)\n",
    "\n",
    "fig = plt.figure(figsize=(10,10))\n",
    "plt.plot(tsne_spectrum[0:11, 0], tsne_spectrum[0:11, 1], 'rx')\n",
    "plt.plot(tsne_spectrum[11:, 0], tsne_spectrum[11:, 1], 'bo')\n",
    "for (x,y, label) in zip(tsne_spectrum[0:, 0], tsne_spectrum[0:, 1], spectrum_df.sentence.values.tolist() ):\n",
    "    plt.annotate(label, # this is the text\n",
    "                 (x,y), # this is the point to label\n",
    "                 textcoords=\"offset points\", # how to position the text\n",
    "                 xytext=(0,10), # distance from text to points (x,y)\n",
    "                 ha='center') # horizontal alignment can be left, right or center"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spectrum_activations = spectrum_embeddings_tensors[0][0][:,0,:].numpy()\n",
    "pca_spectrum = PCA(n_components=2).fit_transform(spectrum_activations)\n",
    "\n",
    "fig = plt.figure(figsize=(10,10))\n",
    "ax = fig.add_subplot(111)\n",
    "ax.plot(pca_spectrum[0:11, 0], pca_spectrum[0:11, 1], 'rx')\n",
    "ax.plot(pca_spectrum[11:, 0], pca_spectrum[11:, 1], 'bo')\n",
    "for (x,y, label) in zip(pca_spectrum[0:, 0], pca_spectrum[0:, 1], spectrum_df.sentence.values.tolist() ):\n",
    "    plt.annotate(label, # this is the text\n",
    "                 (x,y), # this is the point to label\n",
    "                 textcoords=\"offset points\", # how to position the text\n",
    "                 xytext=(0,10), # distance from text to points (x,y)\n",
    "                 ha='center') # horizontal alignment can be left, right or center"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
