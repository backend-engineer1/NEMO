{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "kUlQMiPZxfS_",
    "outputId": "cee17d53-c44c-4821-ebeb-4fa347c316b2"
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "You can run either this notebook locally (if you have all the dependencies and a GPU) or on Google Colab.\n",
    "\n",
    "Instructions for setting up Colab are as follows:\n",
    "1. Open a new Python 3 notebook.\n",
    "2. Import this notebook from GitHub (File -> Upload Notebook -> \"GITHUB\" tab -> copy/paste GitHub URL)\n",
    "3. Connect to an instance with a GPU (Runtime -> Change runtime type -> select \"GPU\" for hardware accelerator)\n",
    "4. Run this cell to set up dependencies.\n",
    "\"\"\"\n",
    "import os\n",
    "# If you're using Google Colab and not running locally, run this cell.\n",
    "!pip install wget\n",
    "!apt-get install sox\n",
    "# !pip install nemo_toolkit[asr]==0.10.0b10\n",
    "!git clone https://github.com/NVIDIA/NeMo.git\n",
    "os.chdir('NeMo')\n",
    "!bash reinstall.sh\n",
    "!pip install unidecode"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "VgTR8CMlxu3p"
   },
   "source": [
    "# **SPEAKER RECOGNITION** \n",
    "\n",
    "Speaker Recognition (SR) is an broad research area which solves two major tasks: speaker identification (who is speaking?) and speaker verification (is the speaker who she claims to be?). In this work, we focmus on the far-field, text-independent speaker recognition when the identity of the speaker is based on how speech is spoken, not necessarily in what is being said. Typically such SR systems operate on unconstrained speech utterances, \n",
    "which are converted into vectors of fixed length, called speaker embeddings. Speaker embeddings are also used in automatic speech recognition (ASR) and speech synthesis.\n",
    "\n",
    "As the goal of most speaker related systems is to get good speaker level embeddings that could help distinguish from other speakers, we shall first train these embeddings in end-to-end manner optimizing the [QuatzNet](https://arxiv.org/abs/1910.10261) based encoder model on cross-entropy loss. We modify the original quartznet based decoder to get these fixed size embeddings irrespective of the length of the input audio. We employ mean and variance based statistics pooling method to grab these embeddings."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "KzzOC5rpx9y6"
   },
   "source": [
    "In this tutorial we shall first train these embeddings on speaker related datasets and then get speaker embeddings from a pretrained network for a new dataset. Since Google Colab has very slow read-write speeds I'll be demonstarting this tutorial using [an4](http://www.speech.cs.cmu.edu/databases/an4/). \n",
    "\n",
    "Instead if you'd like to try on a bigger dataset like [hi-mia](https://arxiv.org/abs/1912.01231) use the [get_hi-mia-data.py](https://github.com/NVIDIA/NeMo/blob/master/scripts/get_hi-mia_data.py) script to download the necessary files, extract them, also re-sample to 16Khz if any of these samples are not at 16Khz. We do also provide scripts to score these embeddings for a speaker-verification task like hi-mia dataset. To do that follow this detailed [tutorial](https://nvidia.github.io/NeMo/) or [notebook](https://github.com/NVIDIA/NeMo/blob/master/examples/speaker_recognition/notebooks/Speaker_Recognition_hi-mia.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 119
    },
    "colab_type": "code",
    "id": "UO_hAhMx0rwv",
    "outputId": "493bd23a-d07a-46db-e634-d38a09f70ef3"
   },
   "outputs": [],
   "source": [
    "print(os.getcwd())\n",
    "data_dir = 'data'\n",
    "!mkdir $data_dir\n",
    "import glob\n",
    "import subprocess\n",
    "import tarfile\n",
    "import wget\n",
    "\n",
    "# Download the dataset. This will take a few moments...\n",
    "print(\"******\")\n",
    "if not os.path.exists(data_dir + '/an4_sphere.tar.gz'):\n",
    "    an4_url = 'http://www.speech.cs.cmu.edu/databases/an4/an4_sphere.tar.gz'\n",
    "    an4_path = wget.download(an4_url, data_dir)\n",
    "    print(f\"Dataset downloaded at: {an4_path}\")\n",
    "else:\n",
    "    print(\"Tarfile already exists.\")\n",
    "    an4_path = data_dir + '/an4_sphere.tar.gz'\n",
    "\n",
    "# Untar and convert .sph to .wav (using sox)\n",
    "tar = tarfile.open(an4_path)\n",
    "tar.extractall(path=data_dir)\n",
    "\n",
    "print(\"Converting .sph to .wav...\")\n",
    "sph_list = glob.glob(data_dir + '/an4/**/*.sph', recursive=True)\n",
    "for sph_path in sph_list:\n",
    "    wav_path = sph_path[:-4] + '.wav'\n",
    "    cmd = [\"sox\", sph_path, wav_path]\n",
    "    subprocess.run(cmd)\n",
    "print(\"Finished conversion.\\n******\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "LEKDkOSimsKr"
   },
   "source": [
    "Since an4 is not designed for speaker recognition, this facilitates the oppurtunity to demostrate how you can generate manifest files that are necessary for training. These methods can be applied to any dataset to get similar training manifest files. \n",
    "\n",
    "First get a scp file(s) which has all the wav files with absolute path for each of train,dev and test set. This can be easily done by the `find` bash command"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "0e6nuOFN8Pfv",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!find $PWD/data/an4/wav/an4_clstk  -iname \"*.wav\" > data/an4/wav/an4_clstk/train_all.scp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "7168Z9eXn4st"
   },
   "source": [
    "Let's look at the first 3 lines of scp file for train. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 68
    },
    "colab_type": "code",
    "id": "SQupCVpZIvtL",
    "outputId": "e45cf645-42fc-4f4f-bd94-964848e04145"
   },
   "outputs": [],
   "source": [
    "!head -n 3 $data_dir/an4/wav/an4_clstk/train_all.scp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "cN09z0XFoDjN"
   },
   "source": [
    "Since we created the scp file for train, we use `scp_to_manifest.py` to convert this scp file to a manifest file and then optionally split the files to train \\& dev for evaluating the models while training by using the `--split` flag. So as you guessed we wouldn't be needing the `--split` option for test folder. \n",
    "Accordingly please mention the `id` number, which is the field num seperated by `/` to be considered as speaker label "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 85
    },
    "colab_type": "code",
    "id": "fNXZwNexIkAo",
    "outputId": "ca06c4be-c0f6-4ec7-8198-a26347ea4b1e"
   },
   "outputs": [],
   "source": [
    "!python scripts/scp_to_manifest.py --scp $data_dir/an4/wav/an4_clstk/train_all.scp --id -2 --out $data_dir/an4/wav/an4_clstk/all_manifest.json --split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "dxUL_g77oned"
   },
   "source": [
    "Generate the scp for test folder and then convert to a manifest type. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "QShlVwEIO64D",
    "outputId": "291d7dce-e202-4062-9eee-e43224084cb5"
   },
   "outputs": [],
   "source": [
    "!find $PWD/data/an4/wav/an4test_clstk  -iname \"*.wav\" > data/an4/wav/an4test_clstk/test_all.scp\n",
    "!python scripts/scp_to_manifest.py --scp data/an4/wav/an4test_clstk/test_all.scp --id -2 --out data/an4/wav/an4test_clstk/test.json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "F4rBMntjpPph"
   },
   "source": [
    "Import necessary packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 187
    },
    "colab_type": "code",
    "id": "4mSWNvdZPIwR",
    "outputId": "83455882-4924-4d18-afd3-d2c8ee8ed78d"
   },
   "outputs": [],
   "source": [
    "from ruamel.yaml import YAML\n",
    "\n",
    "import nemo\n",
    "import nemo.collections.asr as nemo_asr\n",
    "import copy\n",
    "from functools import partial"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "CeKfJQ-YpTOv"
   },
   "source": [
    "# Building Training and Evaluation DAGs with NeMo\n",
    "Building a model using NeMo consists of \n",
    "\n",
    "1.  Instantiating the neural modules we need\n",
    "2.  specifying the DAG by linking them together.\n",
    "\n",
    "In NeMo, the training and inference pipelines are managed by a NeuralModuleFactory, which takes care of checkpointing, callbacks, and logs, along with other details in training and inference. We set its log_dir argument to specify where our model logs and outputs will be written, and can set other training and inference settings in its constructor. For instance, if we were resuming training from a checkpoint, we would set the argument checkpoint_dir=`<path_to_checkpoint>`.\n",
    "\n",
    "Along with logs in NeMo, you can optionally view the tensorboard logs with the create_tb_writer=True argument to the NeuralModuleFactory. By default all the tensorboard log files will be stored in {log_dir}/tensorboard, but you can change this with the tensorboard_dir argument. One can load tensorboard logs through tensorboard by running tensorboard --logdir=`<path_to_tensorboard dir>` in the terminal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "uyn2xrR7R1K_"
   },
   "outputs": [],
   "source": [
    "exp_name = 'quartznet3x1_an4'\n",
    "work_dir = './myExps/'\n",
    "neural_factory = nemo.core.NeuralModuleFactory(\n",
    "    log_dir=work_dir+\"/as4_logdir/\",\n",
    "    checkpoint_dir=\"./myExps/checkpoints/\" + exp_name,\n",
    "    create_tb_writer=True,\n",
    "    random_seed=42,\n",
    "    tensorboard_dir=work_dir+'/tensorboard/',\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "k-juqc40p8KN"
   },
   "source": [
    "Now that we have our neural module factory, we can specify our **neural modules and instantiate them**. Here, we load the parameters for each module from the configuration file. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "mC-KPOy-rpLA",
    "outputId": "1d902505-6e35-4eb8-aebf-8401c1bdd39c"
   },
   "outputs": [],
   "source": [
    "logging = nemo.logging\n",
    "yaml = YAML(typ=\"safe\")\n",
    "with open('../configs/quartznet_spkr_3x1x512_xvector.yaml') as f:\n",
    "    spkr_params = yaml.load(f)\n",
    "\n",
    "sample_rate = spkr_params[\"sample_rate\"]\n",
    "time_length = spkr_params.get(\"time_length\", 8)\n",
    "logging.info(\"max time length considered for each file is {} sec\".format(time_length))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "5VgzNS1lrrqS"
   },
   "source": [
    "Instantiating train data_layer using config arguments. `labels = None` automatically creates output labels from manifest files, if you would like to pass those speaker names you can use the labels option. So while instantiating eval data_layer, we can use pass labels to the class in order to match same the speaker output labels as we have in the training data layer. This comes in handy while training on multiple datasets with more than one manifest file. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 153
    },
    "colab_type": "code",
    "id": "dC9QOenNPoUs",
    "outputId": "786aac99-57f6-4066-e9a3-4908dc6e3d7a"
   },
   "outputs": [],
   "source": [
    "train_dl_params = copy.deepcopy(spkr_params[\"AudioToSpeechLabelDataLayer\"])\n",
    "train_dl_params.update(spkr_params[\"AudioToSpeechLabelDataLayer\"][\"train\"])\n",
    "del train_dl_params[\"train\"]\n",
    "del train_dl_params[\"eval\"]\n",
    "\n",
    "batch_size=64\n",
    "data_layer_train = nemo_asr.AudioToSpeechLabelDataLayer(\n",
    "        manifest_filepath='../data/an4/wav/an4_clstk/train.json',\n",
    "        labels=None,\n",
    "        batch_size=batch_size,\n",
    "        time_length=time_length,\n",
    "        **train_dl_params,\n",
    "    )\n",
    "\n",
    "eval_dl_params = copy.deepcopy(spkr_params[\"AudioToSpeechLabelDataLayer\"])\n",
    "eval_dl_params.update(spkr_params[\"AudioToSpeechLabelDataLayer\"][\"eval\"])\n",
    "del eval_dl_params[\"train\"]\n",
    "del eval_dl_params[\"eval\"]\n",
    "\n",
    "data_layer_eval = nemo_asr.AudioToSpeechLabelDataLayer(\n",
    "    manifest_filepath=\"../data/an4/wav/an4_clstk/dev.json\",\n",
    "    labels=data_layer_train.labels,\n",
    "    batch_size=batch_size,\n",
    "    time_length=time_length,\n",
    "    **eval_dl_params,\n",
    ")\n",
    "\n",
    "data_preprocessor = nemo_asr.AudioToMelSpectrogramPreprocessor(\n",
    "        sample_rate=sample_rate, **spkr_params[\"AudioToMelSpectrogramPreprocessor\"],\n",
    "    )\n",
    "encoder = nemo_asr.JasperEncoder(**spkr_params[\"JasperEncoder\"],)\n",
    "\n",
    "decoder = nemo_asr.JasperDecoderForSpkrClass(\n",
    "        feat_in=spkr_params[\"JasperEncoder\"][\"jasper\"][-1][\"filters\"],\n",
    "        num_classes=data_layer_train.num_classes,\n",
    "        pool_mode=spkr_params[\"JasperDecoderForSpkrClass\"]['pool_mode'],\n",
    "        emb_sizes=spkr_params[\"JasperDecoderForSpkrClass\"][\"emb_sizes\"].split(\",\"),\n",
    "    )\n",
    "\n",
    "xent_loss = nemo_asr.CrossEntropyLossNM(weight=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "9bAP70DqsXGY"
   },
   "source": [
    "The next step is to assemble our training DAG by specifying the inputs to each neural module."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 224
    },
    "colab_type": "code",
    "id": "1raBGmd5Vshl",
    "outputId": "33a128f4-193f-4913-9c82-fd27610dfb9a"
   },
   "outputs": [],
   "source": [
    "audio_signal, audio_signal_len, label, label_len = data_layer_train()\n",
    "processed_signal, processed_signal_len = data_preprocessor(input_signal=audio_signal, length=audio_signal_len)\n",
    "encoded, encoded_len = encoder(audio_signal=processed_signal, length=processed_signal_len)\n",
    "logits, _ = decoder(encoder_output=encoded)\n",
    "loss = xent_loss(logits=logits, labels=label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "uwnZT8ycsYMa"
   },
   "source": [
    "We would like to be able to evaluate our model on the dev set, as well, so let's set up the evaluation DAG.\n",
    "\n",
    "Our evaluation DAG will reuse most of the parts of the training DAG with the exception of the data layer, since we are loading the evaluation data from a different file but evaluating on the same model. Note that if we were using data augmentation in training, we would also leave that out in the evaluation DAG."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 224
    },
    "colab_type": "code",
    "id": "sPPyiNtLWDyf",
    "outputId": "3c37a7dd-b85c-4d29-edfe-cfd0cd16abe4"
   },
   "outputs": [],
   "source": [
    "audio_signal_test, audio_len_test, label_test, _ = data_layer_eval()\n",
    "processed_signal_test, processed_len_test = data_preprocessor(\n",
    "            input_signal=audio_signal_test, length=audio_len_test\n",
    "        )\n",
    "encoded_test, encoded_len_test = encoder(audio_signal=processed_signal_test, length=processed_len_test)\n",
    "logits_test, _ = decoder(encoder_output=encoded_test)\n",
    "loss_test = xent_loss(logits=logits_test, labels=label_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "8m7dz1-usp1S"
   },
   "source": [
    "# Creating CallBacks\n",
    "\n",
    "We would like to be able to monitor our model while it's training, so we use callbacks. In general, callbacks are functions that are called at specific intervals over the course of training or inference, such as at the start or end of every n iterations, epochs, etc. The callbacks we'll be using for this are the SimpleLossLoggerCallback, which reports the training loss (or another metric of your choosing, such as \\% accuracy for speaker recognition tasks), and the EvaluatorCallback, which regularly evaluates the model on the dev set. Both of these callbacks require you to pass in the tensors to be evaluated--these would be the final outputs of the training and eval DAGs above.\n",
    "\n",
    "Another useful callback is the CheckpointCallback, for saving checkpoints at set intervals. We create one here just to demonstrate how it works."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "LFlXnbRaWTVl"
   },
   "outputs": [],
   "source": [
    "from nemo.collections.asr.helpers import (\n",
    "    monitor_classification_training_progress,\n",
    "    process_classification_evaluation_batch,\n",
    "    process_classification_evaluation_epoch,\n",
    ")\n",
    "from nemo.utils.lr_policies import CosineAnnealing\n",
    "\n",
    "train_callback = nemo.core.SimpleLossLoggerCallback(\n",
    "        tensors=[loss, logits, label],\n",
    "        print_func=partial(monitor_classification_training_progress, eval_metric=[1]),\n",
    "        step_freq=40,\n",
    "        get_tb_values=lambda x: [(\"train_loss\", x[0])],\n",
    "        tb_writer=neural_factory.tb_writer,\n",
    "    )\n",
    "\n",
    "callbacks = [train_callback]\n",
    "\n",
    "chpt_callback = nemo.core.CheckpointCallback(\n",
    "            folder=\"./myExps/checkpoints/\" + exp_name,\n",
    "            load_from_folder=\"./myExps/checkpoints/\" + exp_name,\n",
    "            step_freq=100,\n",
    "        )\n",
    "callbacks.append(chpt_callback)\n",
    "\n",
    "tagname = \"an4_dev\"\n",
    "eval_callback = nemo.core.EvaluatorCallback(\n",
    "            eval_tensors=[loss_test, logits_test, label_test],\n",
    "            user_iter_callback=partial(process_classification_evaluation_batch, top_k=1),\n",
    "            user_epochs_done_callback=partial(process_classification_evaluation_epoch, tag=tagname),\n",
    "            eval_step=100,  # How often we evaluate the model on the test set\n",
    "            tb_writer=neural_factory.tb_writer,\n",
    "        )\n",
    "\n",
    "callbacks.append(eval_callback)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "a8EFjLsWs_jM"
   },
   "source": [
    "Now that we have our model and callbacks set up, how do we run it?\n",
    "\n",
    "Once we create our neural factory and the callbacks for the information that we want to see, we can start training by simply calling the train function on the tensors we want to optimize and our callbacks! Since this notebook is for you to get started, by an4 as dataset is small it would quickly get higher accuracies. For better models use bigger datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "xHTEtz7yXVMK",
    "outputId": "bd53ae06-cd0d-4291-da66-1af3079cbd86"
   },
   "outputs": [],
   "source": [
    "# train model\n",
    "num_epochs=100\n",
    "N = len(data_layer_train)\n",
    "steps_per_epoch = N // batch_size\n",
    "\n",
    "logging.info(\"Number of steps per epoch {}\".format(steps_per_epoch))\n",
    "\n",
    "neural_factory.train(\n",
    "        tensors_to_optimize=[loss],\n",
    "        callbacks=callbacks,\n",
    "        lr_policy=CosineAnnealing(\n",
    "            num_epochs * steps_per_epoch, warmup_steps=0.1 * num_epochs * steps_per_epoch,\n",
    "        ),\n",
    "        optimizer=\"novograd\",\n",
    "        optimization_params={\n",
    "            \"num_epochs\": num_epochs,\n",
    "            \"lr\": 0.01,\n",
    "            \"betas\": (0.95, 0.5),\n",
    "            \"weight_decay\": 0.001,\n",
    "            \"grad_norm_clip\": None,\n",
    "        }\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "BB6s19pmxGfX"
   },
   "source": [
    "Now that we trained our embeddings, we shall extract these embeddings using our pretrained checkpoint present at `checkpoint_dir`. As we can see from the neural architecture, we extract the embeddings after the `emb1` layer. \n",
    "![Speaker Recognition Layers](./speaker_reco.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "oSIDu6jkym66"
   },
   "source": [
    "Now use the test manifest to get the embeddings. As we saw before, let's create a new `data_layer` for test. Use previously instiated models and attach the DAGs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 258
    },
    "colab_type": "code",
    "id": "5JqUVbKDY32a",
    "outputId": "dd835e02-8882-4287-9639-c249ac3dfc94"
   },
   "outputs": [],
   "source": [
    "eval_dl_params = copy.deepcopy(spkr_params[\"AudioToSpeechLabelDataLayer\"])\n",
    "eval_dl_params.update(spkr_params[\"AudioToSpeechLabelDataLayer\"][\"eval\"])\n",
    "del eval_dl_params[\"train\"]\n",
    "del eval_dl_params[\"eval\"]\n",
    "eval_dl_params['shuffle'] = False  # To grab  the file names without changing data_layer\n",
    "\n",
    "test_dataset = '../data/an4/wav/an4test_clstk/test.json'\n",
    "data_layer_test = nemo_asr.AudioToSpeechLabelDataLayer(\n",
    "        manifest_filepath=test_dataset,\n",
    "        labels=None,\n",
    "        batch_size=batch_size,\n",
    "        **eval_dl_params,\n",
    "    )\n",
    "\n",
    "audio_signal_test, audio_len_test, label_test, _ = data_layer_test()\n",
    "processed_signal_test, processed_len_test = data_preprocessor(\n",
    "    input_signal=audio_signal_test, length=audio_len_test)\n",
    "encoded_test, _ = encoder(audio_signal=processed_signal_test, length=processed_len_test)\n",
    "_, embeddings = decoder(encoder_output=encoded_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "dwEifkD9zfpl"
   },
   "source": [
    "Now get the embeddings using neural_factor infer command, that just does forward pass of all our modules. And save our embeddings in `<work_dir>/embeddings`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 153
    },
    "colab_type": "code",
    "id": "wGxYiFpJze5h",
    "outputId": "dbbc7204-28bc-43e9-b6aa-f3f757f5d4b5"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import json\n",
    "eval_tensors = neural_factory.infer(tensors=[embeddings, label_test], checkpoint_dir=\"./myExps/checkpoints/\" + exp_name)\n",
    "    # inf_loss , inf_emb, inf_logits, inf_label = eval_tensors\n",
    "inf_emb, inf_label = eval_tensors\n",
    "whole_embs = []\n",
    "whole_labels = []\n",
    "manifest = open(test_dataset, 'r').readlines()\n",
    "\n",
    "for line in manifest:\n",
    "    line = line.strip()\n",
    "    dic = json.loads(line)\n",
    "    filename = dic['audio_filepath'].split('/')[-1]\n",
    "    whole_labels.append(filename)\n",
    "\n",
    "for idx in range(len(inf_label)):\n",
    "    whole_embs.extend(inf_emb[idx].numpy())\n",
    "\n",
    "embedding_dir = './myExps/embeddings/'\n",
    "if not os.path.exists(embedding_dir):\n",
    "    os.mkdir(embedding_dir)\n",
    "\n",
    "filename = os.path.basename(test_dataset).split('.')[0]\n",
    "name = embedding_dir + filename\n",
    "\n",
    "np.save(name + '.npy', np.asarray(whole_embs))\n",
    "np.save(name + '_labels.npy', np.asarray(whole_labels))\n",
    "logging.info(\"Saved embedding files to {}\".format(embedding_dir))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "SKKVIb7e6vel",
    "outputId": "a3fa3703-da6c-4a07-c20c-c83df11a8f25"
   },
   "outputs": [],
   "source": [
    "ls myExps/embeddings/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "A_7S4Yja7A8V"
   },
   "source": [
    "Since an4 doesn't have trails files to demonstrate cosine and PLDA scoring. Tutorial for that can be found at\n",
    "[hi-mia notebook](https://github.com/NVIDIA/NeMo/blob/master/examples/speaker_recognition/notebooks/Speaker_Recognition_an4.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Speaker_Recognition_dataset.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
