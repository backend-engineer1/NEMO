model: "TalkNet Durs Predictor (LJSpeech)"
sample_rate: &sample_rate 22050
pad16: &pad16 false  # True is not supported at the moment (just too complicated for different losses).
labels: [
  # Space
    ' ',
  # string.ascii_lowercase
    'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm',
    'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z',
  # Punctuation
    ',', '.', '!', '?', ';', ':', '-', '/',
    '"', "'", '(', ')', '[', ']', '{', '}',
]

TalkNetDataLayer_train:
    sample_rate: *sample_rate
    normalize_transcripts: true
    trim_silence: false
    drop_last: true
    shuffle: true
    sampler_type: 'default'  # 'super-smart' makes sense for distributed training.
    bd_aug: false

TalkNetDataLayer_eval:
    sample_rate: *sample_rate
    normalize_transcripts: true
    trim_silence: false
    drop_last: false  # Mind the BN.
    shuffle: false
    sampler_type: 'all'  # Eval/Test are too small for distributed evaluation.
    bd_aug: false

TalkNet:
    d_char: 64
    pad16: *pad16

# QN-like arch
dropout: &dropout 0.1  # Having dropout more than 0.1 doesn't make sense for LJSpeech.
separable: &separable true  # Non-separable one greatly increase number of weights.
JasperEncoder:
    activation: "relu"
    conv_mask: false  # Mind the MaskedConv1d implementation: it's too slow for training and inference.

    # (Number of Layers) x (Number of Repeats) x (Number of Filters) x (Kernel Size)
    jasper:
        # First 3x3 Conv
        -   filters: 256
            repeat: 1
            kernel: [3]
            stride: [1]
            dilation: [1]
            dropout: *dropout
            residual: true
            separable: *separable

        # M: 5 x 5 x 256 x [5, 7, 9, 11, 13]
        -   filters: 256
            repeat: 5
            kernel: [5]
            stride: [1]
            dilation: [1]
            dropout: *dropout
            residual: true
            separable: *separable

        -   filters: 256
            repeat: 5
            kernel: [7]
            stride: [1]
            dilation: [1]
            dropout: *dropout
            residual: true
            separable: *separable

        -   filters: 256
            repeat: 5
            kernel: [9]
            stride: [1]
            dilation: [1]
            dropout: *dropout
            residual: true
            separable: *separable

        -   filters: 256
            repeat: 5
            kernel: [11]
            stride: [1]
            dilation: [1]
            dropout: *dropout
            residual: true
            separable: *separable

        -   filters: 256
            repeat: 5
            kernel: [13]
            stride: [1]
            dilation: [1]
            dropout: *dropout
            residual: true
            separable: *separable

        # Last 1x1 Conv
        -   filters: 512
            repeat: 1
            kernel: [1]
            stride: [1]
            dilation: [1]
            dropout: *dropout
            residual: true

TalkNetDursLoss:
    method: 'xe-steps'  # ['l2-log', 'l2', 'dmld-log', 'dmld', 'xe', 'xe-steps']
    num_classes: 32  # 'n_classes' for dmld or xe (32 covers 98%)
    dmld_hidden: 5  # 1/3 of d_hidden for dmld (log2(num_classes))
    reduction: 'all'
    max_dur: 500
    xe_steps_coef: 1.5
    pad16: *pad16
