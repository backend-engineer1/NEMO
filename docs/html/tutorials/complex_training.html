

<!DOCTYPE html>
<!--[if IE 8]><html class="no-js lt-ie9" lang="en" > <![endif]-->
<!--[if gt IE 8]><!--> <html class="no-js" lang="en" > <!--<![endif]-->
<head>
  <meta charset="utf-8">
  
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  
  <title>Complex Training Pipelines (GAN Example) &mdash; nemo 0.8.1 documentation</title>
  

  
  
  
  

  
  <script type="text/javascript" src="../_static/js/modernizr.min.js"></script>
  
    
      <script type="text/javascript" id="documentation_options" data-url_root="../" src="../_static/documentation_options.js"></script>
        <script type="text/javascript" src="../_static/jquery.js"></script>
        <script type="text/javascript" src="../_static/underscore.js"></script>
        <script type="text/javascript" src="../_static/doctools.js"></script>
        <script type="text/javascript" src="../_static/language_data.js"></script>
    
    <script type="text/javascript" src="../_static/js/theme.js"></script>

    

  
  <link rel="stylesheet" href="../_static/css/theme.css" type="text/css" />
  <link rel="stylesheet" href="../_static/pygments.css" type="text/css" />
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="Fast Training" href="../training.html" />
    <link rel="prev" title="Callbacks" href="callbacks.html" /> 
</head>

<body class="wy-body-for-nav">

   
  <div class="wy-grid-for-nav">
    
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >
          

          
            <a href="../index.html" class="icon icon-home"> nemo
          

          
          </a>

          
            
            
              <div class="version">
                0.8.1
              </div>
            
          

          
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>

          
        </div>

        <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
          
            
            
              
            
            
              <ul class="current">
<li class="toctree-l1"><a class="reference internal" href="../index.html">Introduction</a></li>
<li class="toctree-l1"><a class="reference internal" href="../installation.html">Installation</a></li>
<li class="toctree-l1 current"><a class="reference internal" href="intro.html">Getting started</a><ul class="current">
<li class="toctree-l2"><a class="reference internal" href="program_model.html">Programming Model</a></li>
<li class="toctree-l2"><a class="reference internal" href="examples.html">Examples</a></li>
<li class="toctree-l2"><a class="reference internal" href="neuraltypes.html">Neural Types</a></li>
<li class="toctree-l2"><a class="reference internal" href="custommodules.html">How to build Neural Module</a></li>
<li class="toctree-l2"><a class="reference internal" href="weightsharing.html">Weight Sharing between Modules</a></li>
<li class="toctree-l2"><a class="reference internal" href="callbacks.html">Callbacks</a></li>
<li class="toctree-l2 current"><a class="current reference internal" href="#">Complex Training Pipelines (GAN Example)</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#multiple-losses">Multiple Losses</a></li>
<li class="toctree-l3"><a class="reference internal" href="#multiple-optimizers-and-multiple-losses">Multiple Optimizers and Multiple Losses</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../training.html">Fast Training</a></li>
<li class="toctree-l1"><a class="reference internal" href="../asr/intro.html">Speech Recognition</a></li>
<li class="toctree-l1"><a class="reference internal" href="../nlp/intro.html">Natural Language Processing</a></li>
<li class="toctree-l1"><a class="reference internal" href="../collections/modules.html">NeMo Collections API</a></li>
<li class="toctree-l1"><a class="reference internal" href="../api-docs/modules.html">NeMo API</a></li>
<li class="toctree-l1"><a class="reference internal" href="../chinese/intro.html">Mandarin Support</a></li>
</ul>

            
          
        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">

      
      <nav class="wy-nav-top" aria-label="top navigation">
        
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../index.html">nemo</a>
        
      </nav>


      <div class="wy-nav-content">
        
        <div class="rst-content">
        
          















<div role="navigation" aria-label="breadcrumbs navigation">

  <ul class="wy-breadcrumbs">
    
      <li><a href="../index.html">Docs</a> &raquo;</li>
        
          <li><a href="intro.html">Getting started</a> &raquo;</li>
        
      <li>Complex Training Pipelines (GAN Example)</li>
    
    
      <li class="wy-breadcrumbs-aside">
        
            
            <a href="../_sources/tutorials/complex_training.rst.txt" rel="nofollow"> View page source</a>
          
        
      </li>
    
  </ul>

  
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
            
  <div class="section" id="complex-training-pipelines-gan-example">
<h1>Complex Training Pipelines (GAN Example)<a class="headerlink" href="#complex-training-pipelines-gan-example" title="Permalink to this headline">¶</a></h1>
<p>So far, training examples have utilized one optimizer to optimize one loss
across all Trainable Neural Modules. NeMo further extends to uses cases that
require multiple losses and multiple optimizers.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>All of our pipelines only support one datalayer.</p>
</div>
<div class="section" id="multiple-losses">
<h2>Multiple Losses<a class="headerlink" href="#multiple-losses" title="Permalink to this headline">¶</a></h2>
<p>Taking our Hello World example from earlier. Let’s say that we now want to
optimize for both a square error loss and a l1 loss. We can pass both the
square error loss tensor and the l1 loss tensor to
<code class="xref py py-meth docutils literal notranslate"><span class="pre">NeuralFactory.train()</span></code>.
An example is shown below.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1">### Same as previous example ###</span>
<span class="kn">import</span> <span class="nn">nemo</span>

<span class="c1"># instantiate Neural Factory with supported backend</span>
<span class="n">nf</span> <span class="o">=</span> <span class="n">nemo</span><span class="o">.</span><span class="n">core</span><span class="o">.</span><span class="n">NeuralModuleFactory</span><span class="p">()</span>

<span class="c1"># instantiate necessary neural modules</span>
<span class="n">dl</span> <span class="o">=</span> <span class="n">nemo</span><span class="o">.</span><span class="n">tutorials</span><span class="o">.</span><span class="n">RealFunctionDataLayer</span><span class="p">(</span>
    <span class="n">n</span><span class="o">=</span><span class="mi">10000</span><span class="p">,</span> <span class="n">batch_size</span><span class="o">=</span><span class="mi">128</span><span class="p">)</span>
<span class="n">fx</span> <span class="o">=</span> <span class="n">nemo</span><span class="o">.</span><span class="n">tutorials</span><span class="o">.</span><span class="n">TaylorNet</span><span class="p">(</span><span class="n">dim</span><span class="o">=</span><span class="mi">4</span><span class="p">)</span>
<span class="n">mse_loss</span> <span class="o">=</span> <span class="n">nemo</span><span class="o">.</span><span class="n">tutorials</span><span class="o">.</span><span class="n">MSELoss</span><span class="p">()</span>

<span class="c1"># describe activation&#39;s flow</span>
<span class="n">x</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="n">dl</span><span class="p">()</span>
<span class="n">p</span> <span class="o">=</span> <span class="n">fx</span><span class="p">(</span><span class="n">x</span><span class="o">=</span><span class="n">x</span><span class="p">)</span>
<span class="n">mse_loss_tensor</span> <span class="o">=</span> <span class="n">mse_loss</span><span class="p">(</span><span class="n">predictions</span><span class="o">=</span><span class="n">p</span><span class="p">,</span> <span class="n">target</span><span class="o">=</span><span class="n">y</span><span class="p">)</span>

<span class="c1">### New code starts here ###</span>
<span class="c1"># We define our new LossNM and as well as our new loss tensor</span>
<span class="n">l1_loss</span> <span class="o">=</span> <span class="n">nemo</span><span class="o">.</span><span class="n">tutorials</span><span class="o">.</span><span class="n">L1Loss</span><span class="p">()</span>
<span class="n">l1_loss_tensor</span> <span class="o">=</span> <span class="n">l1_loss</span><span class="p">(</span><span class="n">predictions</span><span class="o">=</span><span class="n">p</span><span class="p">,</span> <span class="n">target</span><span class="o">=</span><span class="n">y</span><span class="p">)</span>

<span class="c1"># SimpleLossLoggerCallback will print loss values to console.</span>
<span class="c1"># Update printing function to add both losses</span>
<span class="n">callback</span> <span class="o">=</span> <span class="n">nemo</span><span class="o">.</span><span class="n">core</span><span class="o">.</span><span class="n">SimpleLossLoggerCallback</span><span class="p">(</span>
    <span class="n">tensors</span><span class="o">=</span><span class="p">[</span><span class="n">l1_loss_tensor</span><span class="p">,</span> <span class="n">mse_loss_tensor</span><span class="p">],</span>
    <span class="n">print_func</span><span class="o">=</span><span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="k">print</span><span class="p">(</span>
        <span class="n">f</span><span class="s1">&#39;Train Loss: {str(x[0].item() + x[1].item())}&#39;</span><span class="p">)</span>
<span class="p">)</span>

<span class="c1"># Invoke &quot;train&quot; action with both loss tensors</span>
<span class="n">nf</span><span class="o">.</span><span class="n">train</span><span class="p">([</span><span class="n">mse_loss_tensor</span><span class="p">,</span> <span class="n">l1_loss_tensor</span><span class="p">],</span> <span class="n">callbacks</span><span class="o">=</span><span class="p">[</span><span class="n">callback</span><span class="p">],</span>
         <span class="n">optimization_params</span><span class="o">=</span><span class="p">{</span><span class="s2">&quot;num_epochs&quot;</span><span class="p">:</span> <span class="mi">3</span><span class="p">,</span> <span class="s2">&quot;lr&quot;</span><span class="p">:</span> <span class="mf">0.0003</span><span class="p">},</span>
         <span class="n">optimizer</span><span class="o">=</span><span class="s2">&quot;sgd&quot;</span><span class="p">)</span>
</pre></div>
</div>
<p>We can further extend this to optimize one loss at a time. Let’s say that
instead of computing derivatives and gradients with respect to
mse_loss + l1_loss, we want to first compute gradients with respect to
mse_loss, do a weight update, and then compute gradients with respect to
l1_loss, and do another weight update. Here we have to define our own training
loop.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1">### Same as previous example ###</span>
<span class="kn">import</span> <span class="nn">nemo</span>

<span class="c1"># instantiate Neural Factory with supported backend</span>
<span class="n">nf</span> <span class="o">=</span> <span class="n">nemo</span><span class="o">.</span><span class="n">core</span><span class="o">.</span><span class="n">NeuralModuleFactory</span><span class="p">()</span>

<span class="c1"># instantiate necessary neural modules</span>
<span class="n">dl</span> <span class="o">=</span> <span class="n">nemo</span><span class="o">.</span><span class="n">tutorials</span><span class="o">.</span><span class="n">RealFunctionDataLayer</span><span class="p">(</span>
    <span class="n">n</span><span class="o">=</span><span class="mi">10000</span><span class="p">,</span> <span class="n">batch_size</span><span class="o">=</span><span class="mi">128</span><span class="p">)</span>
<span class="n">fx</span> <span class="o">=</span> <span class="n">nemo</span><span class="o">.</span><span class="n">tutorials</span><span class="o">.</span><span class="n">TaylorNet</span><span class="p">(</span><span class="n">dim</span><span class="o">=</span><span class="mi">4</span><span class="p">)</span>
<span class="n">mse_loss</span> <span class="o">=</span> <span class="n">nemo</span><span class="o">.</span><span class="n">tutorials</span><span class="o">.</span><span class="n">MSELoss</span><span class="p">()</span>
<span class="n">l1_loss</span> <span class="o">=</span> <span class="n">nemo</span><span class="o">.</span><span class="n">tutorials</span><span class="o">.</span><span class="n">L1Loss</span><span class="p">()</span>

<span class="c1"># describe activation&#39;s flow</span>
<span class="n">x</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="n">dl</span><span class="p">()</span>
<span class="n">p</span> <span class="o">=</span> <span class="n">fx</span><span class="p">(</span><span class="n">x</span><span class="o">=</span><span class="n">x</span><span class="p">)</span>
<span class="n">mse_loss_tensor</span> <span class="o">=</span> <span class="n">mse_loss</span><span class="p">(</span><span class="n">predictions</span><span class="o">=</span><span class="n">p</span><span class="p">,</span> <span class="n">target</span><span class="o">=</span><span class="n">y</span><span class="p">)</span>
<span class="n">l1_loss_tensor</span> <span class="o">=</span> <span class="n">l1_loss</span><span class="p">(</span><span class="n">predictions</span><span class="o">=</span><span class="n">p</span><span class="p">,</span> <span class="n">target</span><span class="o">=</span><span class="n">y</span><span class="p">)</span>

<span class="c1"># SimpleLossLoggerCallback will print loss values to console.</span>
<span class="n">callback</span> <span class="o">=</span> <span class="n">nemo</span><span class="o">.</span><span class="n">core</span><span class="o">.</span><span class="n">SimpleLossLoggerCallback</span><span class="p">(</span>
    <span class="n">tensors</span><span class="o">=</span><span class="p">[</span><span class="n">l1_loss_tensor</span><span class="p">,</span> <span class="n">mse_loss_tensor</span><span class="p">],</span>
    <span class="n">print_func</span><span class="o">=</span><span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="k">print</span><span class="p">(</span>
        <span class="n">f</span><span class="s1">&#39;L1 Loss: {str(x[0].item())}&#39;</span>
        <span class="n">f</span><span class="s1">&#39;MSE Loss: {str(x[1].item())}&#39;</span><span class="p">)</span>
<span class="p">)</span>

<span class="c1">### New code starts here ###</span>
<span class="c1"># We need to create optimizers manually to enable complex training pipelines</span>
<span class="n">optimizer</span> <span class="o">=</span> <span class="n">nf</span><span class="o">.</span><span class="n">create_optimizer</span><span class="p">(</span>
    <span class="n">optimizer</span><span class="o">=</span><span class="s2">&quot;sgd&quot;</span><span class="p">,</span>
    <span class="c1"># Note we have to specify the neural modules or nmtensors that we want</span>
    <span class="c1"># to optimize for</span>
    <span class="n">things_to_optimize</span><span class="o">=</span><span class="p">[</span><span class="n">l1_loss_tensor</span><span class="p">,</span> <span class="n">mse_loss_tensor</span><span class="p">],</span>
    <span class="n">optimizer_params</span><span class="o">=</span><span class="p">{</span><span class="s2">&quot;lr&quot;</span><span class="p">:</span> <span class="mf">0.0003</span><span class="p">})</span>

<span class="c1"># Now we define our training_loop, which is a list of tuples</span>
<span class="c1"># Each tuple should have two elements</span>
<span class="c1"># The first element is the optimizer to use</span>
<span class="c1"># The second element is the loss we want to optimize</span>
<span class="n">training_loop</span> <span class="o">=</span> <span class="p">[</span>
    <span class="c1"># Optimizer MSE first and do a weight update</span>
    <span class="p">(</span><span class="n">optimizer</span><span class="p">,</span> <span class="p">[</span><span class="n">mse_loss_tensor</span><span class="p">]),</span>
    <span class="c1"># Optimizer L1 second and do a weight update</span>
    <span class="p">(</span><span class="n">optimizer</span><span class="p">,</span> <span class="p">[</span><span class="n">l1_loss_tensor</span><span class="p">]),</span>
<span class="p">]</span>

<span class="c1"># Invoke &quot;train&quot; action</span>
<span class="c1"># Note, we no longer need to pass optimizer since we have a training_loop</span>
<span class="n">nf</span><span class="o">.</span><span class="n">train</span><span class="p">(</span><span class="n">training_loop</span><span class="p">,</span> <span class="n">callbacks</span><span class="o">=</span><span class="p">[</span><span class="n">callback</span><span class="p">],</span>
         <span class="n">optimization_params</span><span class="o">=</span><span class="p">{</span><span class="s2">&quot;num_epochs&quot;</span><span class="p">:</span> <span class="mi">3</span><span class="p">})</span>
</pre></div>
</div>
</div>
<div class="section" id="multiple-optimizers-and-multiple-losses">
<h2>Multiple Optimizers and Multiple Losses<a class="headerlink" href="#multiple-optimizers-and-multiple-losses" title="Permalink to this headline">¶</a></h2>
<p>NeMo additionally supports use cases where a user would want to create more
than one optimizer. One example of such a use case would be a GAN where
we want to create an optimizer for the generator and an optimizer for the
discriminator. We also want to optimize for different losses in both cases.
Here are the highlights from examples/images/gan.py that enable such behaviour.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="o">...</span>

<span class="c1"># Creation of Neural Modules</span>
<span class="n">generator</span> <span class="o">=</span> <span class="n">nemo_simple_gan</span><span class="o">.</span><span class="n">SimpleGenerator</span><span class="p">(</span>
    <span class="n">batch_size</span><span class="o">=</span><span class="n">batch_size</span><span class="p">)</span>
<span class="n">discriminator</span> <span class="o">=</span> <span class="n">nemo_simple_gan</span><span class="o">.</span><span class="n">SimpleDiscriminator</span><span class="p">()</span>

<span class="o">...</span>

<span class="c1"># Creation of Loss NM Tensors</span>
<span class="c1"># Loss 1: Interpolated image loss</span>
<span class="n">interpolated_loss</span> <span class="o">=</span> <span class="n">disc_loss</span><span class="p">(</span><span class="n">decision</span><span class="o">=</span><span class="n">interpolated_decision</span><span class="p">)</span>
<span class="c1"># Loss 2: Real image loss</span>
<span class="n">real_loss</span> <span class="o">=</span> <span class="n">neg_disc_loss</span><span class="p">(</span><span class="n">decision</span><span class="o">=</span><span class="n">real_decision</span><span class="p">)</span>
<span class="c1"># Loss 3: WGAN Gradient Penalty</span>
<span class="n">grad_penalty</span> <span class="o">=</span> <span class="n">disc_grad_penalty</span><span class="p">(</span>
    <span class="n">interpolated_image</span><span class="o">=</span><span class="n">interpolated_image</span><span class="p">,</span>
    <span class="n">interpolated_decision</span><span class="o">=</span><span class="n">interpolated_decision</span><span class="p">)</span>

<span class="o">...</span>

<span class="c1"># Create optimizers</span>
<span class="c1"># Note that we only want one optimizer to optimize either the generator</span>
<span class="c1"># or the discriminator</span>
<span class="n">optimizer_G</span> <span class="o">=</span> <span class="n">neural_factory</span><span class="o">.</span><span class="n">create_optimizer</span><span class="p">(</span>
    <span class="n">things_to_optimize</span><span class="o">=</span><span class="p">[</span><span class="n">generator</span><span class="p">],</span>
    <span class="o">...</span><span class="p">)</span>
<span class="n">optimizer_D</span> <span class="o">=</span> <span class="n">neural_factory</span><span class="o">.</span><span class="n">create_optimizer</span><span class="p">(</span>
    <span class="n">things_to_optimize</span><span class="o">=</span><span class="p">[</span><span class="n">discriminator</span><span class="p">],</span>
    <span class="o">...</span><span class="p">)</span>

<span class="c1"># Define training_loop</span>
<span class="c1"># Note in our training loop, we want to optimize the discriminator</span>
<span class="c1"># 3x more compared to our generator</span>
<span class="n">losses_G</span> <span class="o">=</span> <span class="p">[</span><span class="n">generator_loss</span><span class="p">]</span>
<span class="n">losses_D</span> <span class="o">=</span> <span class="p">[</span><span class="n">interpolated_loss</span><span class="p">,</span> <span class="n">real_loss</span><span class="p">,</span> <span class="n">grad_penalty</span><span class="p">]</span>
<span class="n">training_loop</span> <span class="o">=</span> <span class="p">[</span>
    <span class="p">(</span><span class="n">optimizer_D</span><span class="p">,</span> <span class="n">losses_D</span><span class="p">),</span>
    <span class="p">(</span><span class="n">optimizer_D</span><span class="p">,</span> <span class="n">losses_D</span><span class="p">),</span>
    <span class="p">(</span><span class="n">optimizer_D</span><span class="p">,</span> <span class="n">losses_D</span><span class="p">),</span>
    <span class="p">(</span><span class="n">optimizer_G</span><span class="p">,</span> <span class="n">losses_G</span><span class="p">),</span>
<span class="p">]</span>

<span class="n">neural_factory</span><span class="o">.</span><span class="n">train</span><span class="p">(</span>
    <span class="n">tensors_to_optimize</span><span class="o">=</span><span class="n">training_loop</span><span class="p">,</span>
    <span class="o">...</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>


           </div>
           
          </div>
          <footer>
  
    <div class="rst-footer-buttons" role="navigation" aria-label="footer navigation">
      
        <a href="../training.html" class="btn btn-neutral float-right" title="Fast Training" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right"></span></a>
      
      
        <a href="callbacks.html" class="btn btn-neutral float-left" title="Callbacks" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left"></span> Previous</a>
      
    </div>
  

  <hr/>

  <div role="contentinfo">
    <p>
        &copy; Copyright 2018-2019, AI Applications team

    </p>
  </div>
  Built with <a href="http://sphinx-doc.org/">Sphinx</a> using a <a href="https://github.com/rtfd/sphinx_rtd_theme">theme</a> provided by <a href="https://readthedocs.org">Read the Docs</a>. 

</footer>

        </div>
      </div>

    </section>

  </div>
  


  <script type="text/javascript">
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script>

  
  
    
   

</body>
</html>