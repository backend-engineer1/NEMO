@IEEEtranBSTCTL{BSTcontrol,
  CTLuse_force_etal        = "yes",
  CTLmax_names_forced_etal = "5",
  CTLnames_show_etal       = "5"
}

@misc{lee2019biobert,
    title={BioBERT: a pre-trained biomedical language representation model for biomedical text mining},
    author={Jinhyuk Lee and Wonjin Yoon and Sungdong Kim and Donghyeon Kim and Sunkyu Kim and Chan Ho So and Jaewoo Kang},
    year={2019},
    eprint={1901.08746},
    archivePrefix={arXiv},
    primaryClass={cs.CL}
}

@misc{beltagy2019scibert,
    title={SciBERT: Pretrained Contextualized Embeddings for Scientific Text},
    author={Iz Beltagy and Arman Cohan and Kyle Lo},
    year={2019},
    eprint={1903.10676},
    archivePrefix={arXiv},
    primaryClass={cs.CL}
}

@article{baevski2018adaptive,
  title={Adaptive input representations for neural language modeling},
  author={Baevski, Alexei and Auli, Michael},
  journal={arXiv preprint arXiv:1809.10853},
  year={2018}
}

@article{merity2016pointer,
  title={Pointer sentinel mixture models},
  author={Merity, Stephen and Xiong, Caiming and Bradbury, James and Socher, Richard},
  journal={arXiv preprint arXiv:1609.07843},
  year={2016}
}

@article{devlin2018bert,
  title={Bert: Pre-training of deep bidirectional transformers for language understanding},
  author={Devlin, Jacob and Chang, Ming-Wei and Lee, Kenton and Toutanova, Kristina},
  journal={arXiv preprint arXiv:1810.04805},
  year={2018}
}