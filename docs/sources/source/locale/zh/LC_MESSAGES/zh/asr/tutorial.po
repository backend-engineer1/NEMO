# SOME DESCRIPTIVE TITLE.
# Copyright (C) 2018-2019, NVIDIA
# This file is distributed under the same license as the nemo package.
# FIRST AUTHOR <EMAIL@ADDRESS>, 2019.
#
#, fuzzy
msgid ""
msgstr ""
"Project-Id-Version: nemo 0.9.0\n"
"Report-Msgid-Bugs-To: \n"
"POT-Creation-Date: 2019-12-03 17:10+0800\n"
"PO-Revision-Date: YEAR-MO-DA HO:MI+ZONE\n"
"Last-Translator: FULL NAME <EMAIL@ADDRESS>\n"
"Language-Team: LANGUAGE <LL@li.org>\n"
"MIME-Version: 1.0\n"
"Content-Type: text/plain; charset=utf-8\n"
"Content-Transfer-Encoding: 8bit\n"
"Generated-By: Babel 2.7.0\n"

#: ../../source/zh/asr/tutorial.rst:2
msgid "教程"
msgstr ""

#: ../../source/zh/asr/tutorial.rst:4
msgid "确保你已经安装了 ``nemo`` 和 ``nemo_asr`` 参考 :ref:`installation` 部分."
msgstr ""

#: ../../source/zh/asr/tutorial.rst:8
msgid "在这个教程中你只需要用到 `nemo` 和 `nemo_asr`"
msgstr ""

#: ../../source/zh/asr/tutorial.rst:11
msgid "简介"
msgstr ""

#: ../../source/zh/asr/tutorial.rst:12
msgid ""
"这个教程中我们使用Jasper :cite:`li2019jasper` 模型。Jasper是一个基于CTC :cite:`graves2006`"
" 的端到端的语音识别模型。这个模型之所以被称之为“端到端”是因为它在不需要额外的对齐信息下就可以把输入的音频样本转到对应的抄本上。 "
"CTC可以在音频和文本中找到对齐方式。基于CTC的语音识别管道包含了下面的这些模块："
msgstr ""

#: ../../source/zh/asr/tutorial.rst:15
msgid "音频预处理(特征提取)： 信号正则化，窗口化，(log)频谱(梅尔谱或者MFCC)"
msgstr ""

#: ../../source/zh/asr/tutorial.rst:16
msgid "神经网络声学模型(在给定的每个时间步上的输入特征下，预测词表中字符c的概率分布P_t(c))"
msgstr ""

#: ../../source/zh/asr/tutorial.rst:17
msgid "CTC损失函数"
msgstr ""

#: ../../source/zh/asr/tutorial.rst:26
msgid "获取数据"
msgstr ""

#: ../../source/zh/asr/tutorial.rst:27
msgid ""
"我们会使用LibriSpeech :cite:`panayotov2015librispeech` 数据集. "
"下面这些脚本会下载并且把Librispeech转成 `nemo_asr` 需要的数据格式 :"
msgstr ""

#: ../../source/zh/asr/tutorial.rst:41
msgid ""
"你的磁盘空间至少需要26GB，如果用 ``--data_set=dev_clean,train_clean_100``; 至少用100GB， "
"如果用 ``--data_set=ALL``. 下载和处理都需要一段时间."
msgstr ""

#: ../../source/zh/asr/tutorial.rst:44
msgid "下载和转换后, 你的 `data` 文件夹应该包含两个json文件:"
msgstr ""

#: ../../source/zh/asr/tutorial.rst:46
msgid "dev_clean.json"
msgstr ""

#: ../../source/zh/asr/tutorial.rst:47
msgid "train_clean_100.json"
msgstr ""

#: ../../source/zh/asr/tutorial.rst:49
msgid ""
"在这个教程中我们会使用 `train_clean_100.json` 做训练，以及 `dev_clean.json` 做评估. "
"json文件中的每一行都指的是一个训练样本 - `audio_filepath` 包含了wav文件的路径, `duration` "
"该文件的音频时长(秒), `text` 是抄本:"
msgstr ""

#: ../../source/zh/asr/tutorial.rst:60
msgid "训练"
msgstr ""

#: ../../source/zh/asr/tutorial.rst:62
msgid ""
"我们会在Jasper家族 :cite:`li2019jasper` 中训练一个小模型。 Jasper (\"Just Another SPeech"
" Recognizer\") 是一个深度时延网络 (TDNN) 包含了一维卷积层的块(blocks)。 Jasper家族的模型的结构可以这样表示 "
"Jasper_[BxR] 其中B是块的个数, R表示的是一个块中卷积子块的个数。每个子块包含了一个一维卷积层，一层batch "
"normalization,一个ReLU激活函数, 和一个dropout层:"
msgstr ""

#: ../../source/zh/asr/tutorial.rst:69
msgid "jiaoben"
msgstr ""

#: ../../source/zh/asr/tutorial.rst:71
msgid ""
"在这个教程中我们会使用 [12x1] 的模型结构并且会用分开的卷积. 下面脚本的训练(on "
"`train_clean_100.json`)和评估(on `dev_clean.json`)都是在一块GPU上:"
msgstr ""

#: ../../source/zh/asr/tutorial.rst:75
msgid "运行Jupyter notebook，一步一步跟着这个脚本走一遍"
msgstr ""

#: ../../source/zh/asr/tutorial.rst:78
msgid "**训练脚本**"
msgstr ""

#: ../../source/zh/asr/tutorial.rst:217
msgid "这个脚本在GTX1080上完成50轮训练需要大约7小时"
msgstr ""

#: ../../source/zh/asr/tutorial.rst:224
msgid "进一步提升WER:"
msgstr ""

#: ../../source/zh/asr/tutorial.rst:221
msgid "训练的更久"
msgstr ""

#: ../../source/zh/asr/tutorial.rst:222
msgid "训更多的数据"
msgstr ""

#: ../../source/zh/asr/tutorial.rst:223
msgid "用更大的模型"
msgstr ""

#: ../../source/zh/asr/tutorial.rst:224
msgid "在多GPU上训练并且使用混精度训练(on NVIDIA Volta and Turing GPUs)"
msgstr ""

#: ../../source/zh/asr/tutorial.rst:225
msgid "从预训练好的checkpoints上开始训练"
msgstr ""

#: ../../source/zh/asr/tutorial.rst:229
msgid "混精度训练"
msgstr ""

#: ../../source/zh/asr/tutorial.rst:230
msgid ""
"NeMo中的混精度和分布式训练上基于 `NVIDIA's APEX library "
"<https://github.com/NVIDIA/apex>`_. 确保它已经安装了。"
msgstr ""

#: ../../source/zh/asr/tutorial.rst:233
msgid ""
"训混精度训练你只需要设置在 `nemo.core.NeuralModuleFactory` 中设置 `optimization_level` "
"参数为 `nemo.core.Optimization.mxprO1`. For example:"
msgstr ""

#: ../../source/zh/asr/tutorial.rst:245
msgid ""
"Because mixed precision requires Tensor Cores it only works on NVIDIA "
"Volta and Turing based GPUs 因为混精度训练需要Tensor Cores, 因此它只能在NVIDIA "
"Volta和Turing架构的GPU上运行。"
msgstr ""

#: ../../source/zh/asr/tutorial.rst:249
msgid "多GPU训练"
msgstr ""

#: ../../source/zh/asr/tutorial.rst:251
msgid "在NeMo中开启多GPU训练很容易:"
msgstr ""

#: ../../source/zh/asr/tutorial.rst:253
msgid "首先把NeuralModuleFactory中的 `placement` 设置成 `nemo.core.DeviceType.AllGpu`"
msgstr ""

#: ../../source/zh/asr/tutorial.rst:254
msgid ""
"让你的脚本能够接受 'local_rank' 参数， 你自己不要去设置这个参数， 只需要在代码中添加: "
"`parser.add_argument(\"--local_rank\", default=None, type=int)`"
msgstr ""

#: ../../source/zh/asr/tutorial.rst:255
msgid "用 `torch.distributed.launch` 包来运行你的脚本 (把<num_gpus>改成GPU的数量):"
msgstr ""

#: ../../source/zh/asr/tutorial.rst:263
msgid "大量训练样本例子"
msgstr ""

#: ../../source/zh/asr/tutorial.rst:265
msgid ""
"请参考 `<nemo_git_repo_root>/examples/asr/jasper.py` "
"为例，做一个更全面的理解。它构建了一个训练的有向无环图，在不同的验证集上构建了多达三个有向无环图。"
msgstr ""

#: ../../source/zh/asr/tutorial.rst:267
msgid "假设你用的上基于Volta的DGX, 你可以这么运行:"
msgstr ""

#: ../../source/zh/asr/tutorial.rst:274
msgid ""
"你可以用逗号分割不同的数据集: `--train_manifest=/manifests/librivox-train-"
"all.json,/manifests/librivox-train-all-"
"sp10pcnt.json,/manifests/cv/validated.json`. Here it combines 3 data "
"sets: LibriSpeech, Mozilla Common Voice and LibriSpeech speed perturbed."
msgstr ""

#: ../../source/zh/asr/tutorial.rst:278
msgid "微调"
msgstr ""

#: ../../source/zh/asr/tutorial.rst:279
msgid "如果我们从一个好的预训练模型开始训练，训练时间会大大的减小:"
msgstr ""

#: ../../source/zh/asr/tutorial.rst:281
msgid ""
"获取预训练模型 (jasper_encoder, jasper_decoder and configuration files) `from "
"here <https://ngc.nvidia.com/catalog/models/nvidia:quartznet15x5>`_."
msgstr ""

#: ../../source/zh/asr/tutorial.rst:282
msgid "在你初始化好jasper_encoder和jasper_decoder后，可以这样加载权重:"
msgstr ""

#: ../../source/zh/asr/tutorial.rst:292
msgid "微调的时候，用小一点的学习率"
msgstr ""

#: ../../source/zh/asr/tutorial.rst:296
msgid "推理"
msgstr ""

#: ../../source/zh/asr/tutorial.rst:298
msgid ""
"首先下载预训练模型(jasper_encoder, jasper_decoder and configuration files) `戳这里 "
"<https://ngc.nvidia.com/catalog/models/nvidia:quartznet15x5>`_ 放到 "
"`<path_to_checkpoints>`. 我们会用这个预训练模型在LibriSpeech dev-clean数据集上测试WER."
msgstr ""

#: ../../source/zh/asr/tutorial.rst:306
msgid "用语言模型推理"
msgstr ""

#: ../../source/zh/asr/tutorial.rst:309
msgid "用KenLM构建的语言模型"
msgstr ""

#: ../../source/zh/asr/tutorial.rst:310
msgid ""
"我们会使用 `Baidu's CTC decoder with LM implementation. "
"<https://github.com/PaddlePaddle/DeepSpeech>`_."
msgstr ""

#: ../../source/zh/asr/tutorial.rst:312
msgid "请按照下面的步骤:"
msgstr ""

#: ../../source/zh/asr/tutorial.rst:314
msgid "到scripts目录下 ``cd <nemo_git_repo_root>/scripts``"
msgstr ""

#: ../../source/zh/asr/tutorial.rst:318
msgid "安装百度CTC解码器 (如果在docker容器中不需要用sudo):"
msgstr ""

#: ../../source/zh/asr/tutorial.rst:316
msgid "``sudo apt-get update && sudo apt-get install swig``"
msgstr ""

#: ../../source/zh/asr/tutorial.rst:317
msgid ""
"``sudo apt-get install pkg-config libflac-dev libogg-dev libvorbis-dev "
"libboost-dev``"
msgstr ""

#: ../../source/zh/asr/tutorial.rst:318
msgid ""
"``sudo apt-get install libsndfile1-dev python-setuptools libboost-all-dev"
" python-dev``"
msgstr ""

#: ../../source/zh/asr/tutorial.rst:319
msgid "``./install_decoders.sh``"
msgstr ""

#: ../../source/zh/asr/tutorial.rst:320
msgid "在Librispeech上构建一个6-gram KenLM的语言模型  ``./build_6-gram_OpenSLR_lm.sh``"
msgstr ""

#: ../../source/zh/asr/tutorial.rst:321
msgid "运行 jasper_infer.py 带上 --lm_path来指定语言模型的路径"
msgstr ""

#: ../../source/zh/asr/tutorial.rst:329
msgid "参考"
msgstr ""

