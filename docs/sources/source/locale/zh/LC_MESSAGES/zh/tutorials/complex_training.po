# SOME DESCRIPTIVE TITLE.
# Copyright (C) 2018-2019, NVIDIA
# This file is distributed under the same license as the nemo package.
# FIRST AUTHOR <EMAIL@ADDRESS>, 2019.
#
#, fuzzy
msgid ""
msgstr ""
"Project-Id-Version: nemo 0.9.0\n"
"Report-Msgid-Bugs-To: \n"
"POT-Creation-Date: 2019-12-03 17:10+0800\n"
"PO-Revision-Date: YEAR-MO-DA HO:MI+ZONE\n"
"Last-Translator: FULL NAME <EMAIL@ADDRESS>\n"
"Language-Team: LANGUAGE <LL@li.org>\n"
"MIME-Version: 1.0\n"
"Content-Type: text/plain; charset=utf-8\n"
"Content-Transfer-Encoding: 8bit\n"
"Generated-By: Babel 2.7.0\n"

#: ../../source/zh/tutorials/complex_training.rst:2
msgid "复杂训练流程 (GAN例子)"
msgstr ""

#: ../../source/zh/tutorials/complex_training.rst:4
msgid "目前为止，训练样本在所有可训练的神经模块中用了一个优化器来优化一个损失函数。 NeMo进一步扩充了用例，这些用例会用到多个损失函数和多个优化器。"
msgstr ""

#: ../../source/zh/tutorials/complex_training.rst:8
msgid "我们所有的流程都只支持一个数据层。"
msgstr ""

#: ../../source/zh/tutorials/complex_training.rst:11
msgid "多个损失函数"
msgstr ""

#: ../../source/zh/tutorials/complex_training.rst:12
msgid ""
"以我们之前的Hello World为例子。假设我们现在想要优化一个平方误差损失函数和l1损失函数。 我们可以把这代表着两个损失函数的张量传给 "
":meth:`NeuralFactory.train()<nemo.core.neural_factory.NeuralModuleFactory.train>`。"
" 下面是一个例子："
msgstr ""

#: ../../source/zh/tutorials/complex_training.rst:54
msgid ""
"我们可以进一步拓展这个优化器使得每次优化一个损失函数。比如说，我们不想 根据mse_loss + "
"l1_loss计算梯度，我们想先根据mse_loss计算梯度，做一个 "
"权重更新，然后根据l1_loss求导，再做另一个权重更新。那么我们必须要定义我们 的训练循环："
msgstr ""

#: ../../source/zh/tutorials/complex_training.rst:113
msgid "多个优化器和多个损失函数"
msgstr ""

#: ../../source/zh/tutorials/complex_training.rst:114
msgid ""
"NeMo也支持用户想要定义多个优化器的用例。一个这样的例子是GAN，我们想要给生成器 "
"一个优化器，给判别器一个优化器。我们也想要优化不同的损失函数。 这个是来自examples/images/gan.py下面的支持这种操作的代码："
msgstr ""

