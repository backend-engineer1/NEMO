# SOME DESCRIPTIVE TITLE.
# Copyright (C) 2018-2019, NVIDIA
# This file is distributed under the same license as the nemo package.
# FIRST AUTHOR <EMAIL@ADDRESS>, 2019.
#
#, fuzzy
msgid ""
msgstr ""
"Project-Id-Version: nemo 0.9.0\n"
"Report-Msgid-Bugs-To: \n"
"POT-Creation-Date: 2019-12-03 17:10+0800\n"
"PO-Revision-Date: YEAR-MO-DA HO:MI+ZONE\n"
"Last-Translator: FULL NAME <EMAIL@ADDRESS>\n"
"Language-Team: LANGUAGE <LL@li.org>\n"
"MIME-Version: 1.0\n"
"Content-Type: text/plain; charset=utf-8\n"
"Content-Transfer-Encoding: 8bit\n"
"Generated-By: Babel 2.7.0\n"

#: ../../source/zh/nlp/ner.rst:2
msgid "教程"
msgstr ""

#: ../../source/zh/nlp/ner.rst:4
msgid ""
"在教程前，请确认你已经安装了 ``nemo`` 和 ``nemo_nlp`` 。你可以通过这个部分获得更多的信息： "
":ref:`installation`"
msgstr ""

#: ../../source/zh/nlp/ner.rst:8
msgid "简介"
msgstr ""

#: ../../source/zh/nlp/ner.rst:10
msgid ""
"这个教程将介绍如何在NeMo中，实现命名实体识别(Named Entity Recognition, "
"NER)。我们将通过一个预训练好的BERT模型来进行展示，或者你也可以使用一个训练好的模型！ 你可以通过BERT预训练教程获得更多的信息。"
msgstr ""

#: ../../source/zh/nlp/ner.rst:14
msgid "下载数据集"
msgstr ""

#: ../../source/zh/nlp/ner.rst:16
msgid "`CoNLL-2003`_ 是一个NER上标准的验证集合，当然任何一个NER数据集合都可以。数据集合需要满足的条件是符合以下的格式:"
msgstr ""

#: ../../source/zh/nlp/ner.rst:34
msgid ""
"这里，词和标注使用空格进行分隔，但是在你的数据集里，它们需要用用tab分隔。每一行需要符合这样的格式: [WORD] [TAB] [LABEL] "
"（中间没有空格)。一如 `CoNLL-2003 website`_ 网页上，词性标签之间可能会有列。每个句子中间需要用空行进行分隔。"
msgstr ""

#: ../../source/zh/nlp/ner.rst:41
msgid "训练"
msgstr ""

#: ../../source/zh/nlp/ner.rst:45
msgid "我们建议试试使用Jupyter来运行这部分代码，这会使得调试更加容易!"
msgstr ""

#: ../../source/zh/nlp/ner.rst:47
msgid ""
"首先，我们需要使用所支持的后端，来创建我们的neural "
"factory。你需要确认使用多GPU或者混合精度训练。这个教程中我们使用单GPU训练，不使用混合精度。如果你想使用混合精度训练，需要设置 "
"``amp_opt_level`` 这个参数为 ``O1`` 或者 ``O2`` 。"
msgstr ""

#: ../../source/zh/nlp/ner.rst:58
msgid ""
"接着，我们需要定义我们的tokenizer和BERT模型。你可以有多种方式来实现。注意，NER是大小写敏感的(\"New York "
"Ciyt\"比\"new york city\"更容易被识别出来)，所以我们建议使用区分大小写的模型。"
msgstr ""

#: ../../source/zh/nlp/ner.rst:60
msgid ""
"如果你正在使用一个标准的BERT模型，我们建议你使用下面这条命令。想获取完整的BERT列表，可以参考 "
"``nemo_nlp.huggingface.BERT.list_pretrained_models()`` 。"
msgstr ""

#: ../../source/zh/nlp/ner.rst:69
msgid ""
"如果你在使用一个自己预训练好的BERT模型，你可以使用下面这条命令。这里，你需要把 ``args.bert_checkpoint`` "
"这个参数改成你的checkpoint文件所在的位置。"
msgstr ""

#: ../../source/zh/nlp/ner.rst:80
msgid "现在，创建训练和验证数据集合:"
msgstr ""

#: ../../source/zh/nlp/ner.rst:98
msgid "接着，我们需要在预先训练好的模型上，创建分类器并定义损失函数:"
msgstr ""

#: ../../source/zh/nlp/ner.rst:109
msgid "并创建管道用来进行训练和验证:"
msgstr ""

#: ../../source/zh/nlp/ner.rst:127
msgid "现在，我们需要设置callbacks，一共有3个callbacks:"
msgstr ""

#: ../../source/zh/nlp/ner.rst:129
msgid "`SimpleLossLoggerCallback` 打印出训练过程中的损失函数值"
msgstr ""

#: ../../source/zh/nlp/ner.rst:130
msgid ""
"`EvaluatorCallback` 来验证我们dev集合上F1的值。在这个例子中， `EvaluatorCallback` 也会打印出 "
"`output.txt` 上的预测值，这有利于找出模型哪个部分出了问题。"
msgstr ""

#: ../../source/zh/nlp/ner.rst:131
msgid "`CheckpointCallback` 用于保存和读取checkpoints."
msgstr ""

#: ../../source/zh/nlp/ner.rst:135
msgid ""
"Tensorboard_ 是一个非常好用的调试工具。它在本教程中不是一个必须安装的工具，如果你想使用的话，需要先安装 tensorboardX_ "
"接着在微调过程中使用如下的命令："
msgstr ""

#: ../../source/zh/nlp/ner.rst:166
msgid "最后，我们需要定义学习率规则和优化器，并且开始训练："
msgstr ""

#: ../../source/zh/nlp/ner.rst:183
msgid "使用其它的BERT模型"
msgstr ""

#: ../../source/zh/nlp/ner.rst:185
msgid ""
"除了可以使用谷歌提供的预训练BERT模型和你自己训练的BERT模型外，在NeMo中，也可以使用来自第三方的BERT模型，只要这个模型的参数可以加载到Pytorch中即可。例如，如果你想使用"
" SciBERT_ 来微调："
msgstr ""

#: ../../source/zh/nlp/ner.rst:198
msgid "接着，当你加载你的BERT模型，你需要指定模型所在的目录名："
msgstr ""

#: ../../source/zh/nlp/ner.rst:207
msgid ""
"如果你想使用TensorFlow训练好的模型，例如BioBERT，你需要首先使用Hugging Face提供的 `model conversion"
" script`_ 进行模型转换，再在NeMo中使用这个模型。"
msgstr ""

