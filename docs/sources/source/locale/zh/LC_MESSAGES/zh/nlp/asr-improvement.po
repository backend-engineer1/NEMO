# SOME DESCRIPTIVE TITLE.
# Copyright (C) 2018-2019, NVIDIA
# This file is distributed under the same license as the nemo package.
# FIRST AUTHOR <EMAIL@ADDRESS>, 2019.
#
#, fuzzy
msgid ""
msgstr ""
"Project-Id-Version: nemo 0.9.0\n"
"Report-Msgid-Bugs-To: \n"
"POT-Creation-Date: 2019-12-03 17:10+0800\n"
"PO-Revision-Date: YEAR-MO-DA HO:MI+ZONE\n"
"Last-Translator: FULL NAME <EMAIL@ADDRESS>\n"
"Language-Team: LANGUAGE <LL@li.org>\n"
"MIME-Version: 1.0\n"
"Content-Type: text/plain; charset=utf-8\n"
"Content-Transfer-Encoding: 8bit\n"
"Generated-By: Babel 2.7.0\n"

#: ../../source/zh/nlp/asr-improvement.rst:2
msgid "教程"
msgstr ""

#: ../../source/zh/nlp/asr-improvement.rst:4
msgid ""
"在这个教程中，我们会训练一个语音识别的后处理模型来纠正端到端语音识别模型的输出错误。这个模型和翻译模型很相似，但和传统语音识别模型的再打分很不一样。"
" "
"这个模型的架构是基于注意力机制的编码器解码器架构，其中编码器和解码器都是用BERT的预训练语言模型初始化的。为了训练这个模型，我们用预训练的Jasper语音识别模型"
" :cite:`li2019jasper` 产生的错误来收集数据集。"
msgstr ""

#: ../../source/zh/nlp/asr-improvement.rst:9
msgid "数据"
msgstr ""

#: ../../source/zh/nlp/asr-improvement.rst:10
msgid ""
"**数据收集** 我们用Jasper :cite:`li2019jasper`  在Librispeech数据集 "
":cite:`panayotov2015librispeech` 上训练的模型为这个任务收集数据集。 下载Librispeech数据集, 参考 "
":ref:`LibriSpeech_dataset`. 获得Jasper预训练模型, 参考 :ref:`Jasper_model`. "
"Librispeech训练数据集包含三个部分: train-clean-100, train-clean-360, 和train-"
"clean-500 总共281000个训练样本 . 我们用两个方法来扩增数据集:"
msgstr ""

#: ../../source/zh/nlp/asr-improvement.rst:16
msgid "我们把所有的训练集分成10份，然后用交叉验证的方法训练10个Jasper模型: 一个模型在9份数据集上训练，然后再剩下的那份数据集上做语音识别."
msgstr ""

#: ../../source/zh/nlp/asr-improvement.rst:17
msgid "我们用预训练的Jasper模型，在训练集上做推理的时候，打开dropout。这个过程用不同的随机种子重复多次。"
msgstr ""

#: ../../source/zh/nlp/asr-improvement.rst:19
msgid ""
"**数据后处理** 收集到的数据集需要去除重复以及错词率大于0.5的样本。 得到的数据集包含1,700,000对 \"坏\" 英文-\"好\" "
"英文样本对。"
msgstr ""

#: ../../source/zh/nlp/asr-improvement.rst:22
msgid ""
"**开发和测试集准备**. Librispeech包含两个开发集 (dev-clean 和 dev-other) 以及2个测试 (test-"
"clean 和 test-other). "
"在我们的任务中，我们也这么分。我们把这些数据集放到预训练好的Jasper模型中，用greedy解码得到语音识别的输出结果。这些结果 "
"在我们的教程中用来做评测。"
msgstr ""

#: ../../source/zh/nlp/asr-improvement.rst:28
msgid "从预训练BERT模型中加载参数"
msgstr ""

#: ../../source/zh/nlp/asr-improvement.rst:29
msgid ""
"编码器和解码器都用的是预训练的BERT模型参数。 "
"因为BERT的语言模型和Transformer的编码器结构相同，因此没有其他什么需要做的。从预训练的BERT模型中为解码器准备参数，我们写了一个脚本"
" ``get_decoder_params_from_bert.py`` 会从 ``pytorch-transformers`` "
":cite:`huggingface2019transformers` 下载参数，并把他们映射到解码器的参数上. "
"编码器和解码器的注意力是用self-attention参数做初始化的。 这个脚本位于 ``scripts`` 文件目录下，接受两个参数:"
msgstr ""

#: ../../source/zh/nlp/asr-improvement.rst:33
msgid "``--model_name``: e.g. ``bert-base-cased``, ``bert-base-uncased``, etc."
msgstr ""

#: ../../source/zh/nlp/asr-improvement.rst:34
msgid "``--save_to``: 参数将要存储的文件目录"
msgstr ""

#: ../../source/zh/nlp/asr-improvement.rst:42
msgid "神经模块概览"
msgstr ""

#: ../../source/zh/nlp/asr-improvement.rst:43
msgid ""
"首先，因为所有的模块都是由NeMo构建的, 我们需要初始化Neural Module Factory，我们需要定义 1) backend "
"(PyTorch 还是 TensorFlow), 2) 混精度优化等级, 3) GPU的loca rank, 以及 4) "
"一个实验管理器，创建一个时间戳的文件夹来存储checkpoints和相关的输出，日志文件以及TensorBoard的图."
msgstr ""

#: ../../source/zh/nlp/asr-improvement.rst:56
msgid ""
"接着我们定义分词器把所有的词转到它们对应的序号上. 我们会使用 ``bert-base-uncased`` 模型的词表， "
"因为我们的数据集只包含不区分大小写的文本:"
msgstr ""

#: ../../source/zh/nlp/asr-improvement.rst:63
msgid "编码器模块对应于BERT的语言模型，它来自于 ``nemo_nlp.huggingface`` collection:"
msgstr ""

#: ../../source/zh/nlp/asr-improvement.rst:74
msgid "让词嵌入的大小（包括其他的张量维度）能够整除8可以得到最好的GPU利用率和混精度训练加速。"
msgstr ""

#: ../../source/zh/nlp/asr-improvement.rst:88
msgid ""
"接着, 我们构建transformer解码器神经模块. 因为我们会用BERT预训练的参数来初始化我们的解码器, 我们设置隐藏层激活函数为 "
"``\"hidden_act\": \"gelu\"`` 以及设置学习位置编码 ``\"learn_positional_encodings\":"
" True``:"
msgstr ""

#: ../../source/zh/nlp/asr-improvement.rst:105
msgid "为了把预训练参数加载到解码器参数中, 我们用解码器神经模块的属性函数 ``restore_from`` 来加载:"
msgstr ""

#: ../../source/zh/nlp/asr-improvement.rst:113
msgid "模型训练"
msgstr ""

#: ../../source/zh/nlp/asr-improvement.rst:115
msgid ""
"训练模型，运行 ``asr_postprocessor.py.py``， 它位于 ``examples/nlp`` 目录中. "
"我们用novograd优化器来训练  :cite:`ginsburg2019stochastic`, 设置学习率 ``lr=0.001`` "
"，多项式学习率衰减策略, ``1000`` 步预热, 每个GPU的批量为 ``4096*8`` 符号, 以及 ``0.25`` "
"dropout概率. 我们再8张GPU上做训练。可以用下面的方法开启多GPU训练模式:"
msgstr ""

#: ../../source/zh/nlp/asr-improvement.rst:124
msgid "参考"
msgstr ""

