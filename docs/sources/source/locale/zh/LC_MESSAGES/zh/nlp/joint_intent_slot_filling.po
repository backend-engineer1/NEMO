# SOME DESCRIPTIVE TITLE.
# Copyright (C) 2018-2019, NVIDIA
# This file is distributed under the same license as the nemo package.
# FIRST AUTHOR <EMAIL@ADDRESS>, 2019.
#
#, fuzzy
msgid ""
msgstr ""
"Project-Id-Version: nemo 0.9.0\n"
"Report-Msgid-Bugs-To: \n"
"POT-Creation-Date: 2019-12-03 17:10+0800\n"
"PO-Revision-Date: YEAR-MO-DA HO:MI+ZONE\n"
"Last-Translator: FULL NAME <EMAIL@ADDRESS>\n"
"Language-Team: LANGUAGE <LL@li.org>\n"
"MIME-Version: 1.0\n"
"Content-Type: text/plain; charset=utf-8\n"
"Content-Transfer-Encoding: 8bit\n"
"Generated-By: Babel 2.7.0\n"

#: ../../source/zh/nlp/joint_intent_slot_filling.rst:2
msgid "教程"
msgstr ""

#: ../../source/zh/nlp/joint_intent_slot_filling.rst:4
msgid ""
"在这个教程中，我们将使用BERT模型，来实现一个意图识别（intent）和槽填充（slot filling）混合系统，参考自 `BERT for "
"Joint Intent Classification and Slot Filling "
"<https://arxiv.org/abs/1902.10909>`_ :cite:`chen2019bert`。本教程中所有的代码全部来自 "
"``examples/nlp/joint_intent_slot_with_bert.py``."
msgstr ""

#: ../../source/zh/nlp/joint_intent_slot_filling.rst:6
msgid ""
"我们可以使用 `--pretrained_bert_model` "
"这个参数，来选择四个预训练好的BERT模型。当前，我们使用的加载预训练模型的脚本均来自 `pytorch_transformers` "
"。更多预训练好的模型在 `这里 <https://huggingface.co/pytorch-"
"transformers/pretrained_models.html>`__ 。"
msgstr ""

#: ../../source/zh/nlp/joint_intent_slot_filling.rst:10
msgid "写在开头"
msgstr ""

#: ../../source/zh/nlp/joint_intent_slot_filling.rst:12
msgid "**模型细节** 这个模型会一起训练一个句子层面的分类器，和一个符号串层面的槽分类器，通过最小化如下的混合损失:"
msgstr ""

#: ../../source/zh/nlp/joint_intent_slot_filling.rst:15
msgid "intent_loss * intent_loss_weight + slot_loss * (1 - intent_loss_weight)"
msgstr ""

#: ../../source/zh/nlp/joint_intent_slot_filling.rst:17
msgid "当 `intent_loss_weight = 0.5` 时, 它等价于最大化:"
msgstr ""

#: ../../source/zh/nlp/joint_intent_slot_filling.rst:19
msgid "p(y | x)P(s1, s2, ..., sn | x)"
msgstr ""

#: ../../source/zh/nlp/joint_intent_slot_filling.rst:21
msgid ""
"这里x是一个有n个符号串的序列(x1, x2, ..., xn)，y是x预测出的意图，s1, s2, ..., sn 是对应于x1, x2, "
"..., xn预测出的槽。"
msgstr ""

#: ../../source/zh/nlp/joint_intent_slot_filling.rst:23
msgid "**数据集**"
msgstr ""

#: ../../source/zh/nlp/joint_intent_slot_filling.rst:28
msgid "这个模型可以应用到任意一个符合如下格式的数据集:"
msgstr ""

#: ../../source/zh/nlp/joint_intent_slot_filling.rst:26
msgid "输入文件: 一个 `tsv` 文件，第一行为 [sentence][tab][label]"
msgstr ""

#: ../../source/zh/nlp/joint_intent_slot_filling.rst:28
msgid "槽文件: 句子中所有符号串的槽标注，使用空格分隔。槽标注的数量需要与句子中所有符号串的数量保持一致。"
msgstr ""

#: ../../source/zh/nlp/joint_intent_slot_filling.rst:30
msgid ""
"当前，我们提供多个数据集合的预处理脚本，包括: ATIS可以通过 `Kaggle "
"<https://www.kaggle.com/siddhadev/atis-dataset-from-ms-cntk>`_ "
"进行下载；SNIP对话语言理解数据集，可以通过 `这里 <https://github.com/snipsco/spoken-language-"
"understanding-research-datasets>`__ 获取。预处理脚本在 "
"``collections/nemo_nlp/nemo_nlp/text_data_utils.py``。"
msgstr ""

#: ../../source/zh/nlp/joint_intent_slot_filling.rst:34
msgid "代码结构"
msgstr ""

#: ../../source/zh/nlp/joint_intent_slot_filling.rst:36
msgid ""
"首先，我们初始化Neural Module "
"Factory，需要定义1、后端（Pytorch或是TensorFlow)；2、混合精度优化的级别；3、本地GPU的序列号；4、一个实验的管理器，用于创建文件夹来保存相应的checkpoint、输出、日志文件和TensorBoard的图。"
msgstr ""

#: ../../source/zh/nlp/joint_intent_slot_filling.rst:48
msgid ""
"我们定义分词器，它可以将文本转换成符号串，这里使用来自 "
"`pytorch_transformers`的内置分词器。其将使用BERT模型的映射，把文本转成相应的符号串。"
msgstr ""

#: ../../source/zh/nlp/joint_intent_slot_filling.rst:55
msgid "接着，我们定义所有的神经网络模块，加入到意图识别和槽填充混合系统的流程中。"
msgstr ""

#: ../../source/zh/nlp/joint_intent_slot_filling.rst:57
msgid ""
"处理数据: `nemo_nlp/nemo_nlp/text_data_utils.py` 中的 `JointIntentSlotDataDesc`"
" 类，用于将源数据处理成 `BertJointIntentSlotDataset` "
"支持的类型。当前，它支持SNIPS和ATIS两种格式的数据，当你也可以实现预处理脚本，来支持任意格式的数据。"
msgstr ""

#: ../../source/zh/nlp/joint_intent_slot_filling.rst:59
msgid ""
"JointIntentSlotDataDesc 对象包含例如 `self.train_file`, `self.train_slot_file`,"
" `self.eval_file`, `self.eval_slot_file`,  `self.intent_dict_file` 和 "
"`self.slot_dict_file`等信息。"
msgstr ""

#: ../../source/zh/nlp/joint_intent_slot_filling.rst:67
msgid "数据集: 将数据转换成DataLayerNM可以接收的格式."
msgstr ""

#: ../../source/zh/nlp/joint_intent_slot_filling.rst:89
msgid ""
"DataLayer: 一个单独的层，可以用于在你的数据集中进行语义检查，并将它转换到DataLayerNM中。你需要定义 "
"`input_ports` 和 `output_ports` 。"
msgstr ""

#: ../../source/zh/nlp/joint_intent_slot_filling.rst:101
msgid "加载预训练好的模型，并得到相应输入的隐层状态。"
msgstr ""

#: ../../source/zh/nlp/joint_intent_slot_filling.rst:110
msgid "为我们的任务创建一个分类器。"
msgstr ""

#: ../../source/zh/nlp/joint_intent_slot_filling.rst:123
msgid "创建损失函数。"
msgstr ""

#: ../../source/zh/nlp/joint_intent_slot_filling.rst:136
msgid "创建相应的callbacks，来保存checkpoints，打印训练过程和测试结果。"
msgstr ""

#: ../../source/zh/nlp/joint_intent_slot_filling.rst:161
msgid "最后，我们定义优化器的参数，并开始训练流程。"
msgstr ""

#: ../../source/zh/nlp/joint_intent_slot_filling.rst:177
msgid "模型训练"
msgstr ""

#: ../../source/zh/nlp/joint_intent_slot_filling.rst:179
msgid ""
"为了训练一个意图识别和槽填充的混合任务，运行 ``nemo/examples/nlp`` 下的脚本 "
"``joint_intent_slot_with_bert.py`` ："
msgstr ""

#: ../../source/zh/nlp/joint_intent_slot_filling.rst:190
msgid "测试的话，需要运行："
msgstr ""

#: ../../source/zh/nlp/joint_intent_slot_filling.rst:198
msgid "对一个检索进行测试，需要运行："
msgstr ""

#: ../../source/zh/nlp/joint_intent_slot_filling.rst:208
msgid "参考文献"
msgstr ""

