# SOME DESCRIPTIVE TITLE.
# Copyright (C) 2018-2019, NVIDIA
# This file is distributed under the same license as the nemo package.
# FIRST AUTHOR <EMAIL@ADDRESS>, 2019.
#
#, fuzzy
msgid ""
msgstr ""
"Project-Id-Version: nemo 0.9.0\n"
"Report-Msgid-Bugs-To: \n"
"POT-Creation-Date: 2019-12-03 17:10+0800\n"
"PO-Revision-Date: YEAR-MO-DA HO:MI+ZONE\n"
"Last-Translator: FULL NAME <EMAIL@ADDRESS>\n"
"Language-Team: LANGUAGE <LL@li.org>\n"
"MIME-Version: 1.0\n"
"Content-Type: text/plain; charset=utf-8\n"
"Content-Transfer-Encoding: 8bit\n"
"Generated-By: Babel 2.7.0\n"

#: ../../source/tts/tutorial.rst:2
msgid "Tutorial"
msgstr ""

#: ../../source/tts/tutorial.rst:4
msgid ""
"Make sure you have installed ``nemo``, ``nemo_asr``, and ``nemo_tts`` "
"collection. See the :ref:`installation` section."
msgstr ""

#: ../../source/tts/tutorial.rst:8
msgid ""
"You only need `nemo`, `nemo_asr`, and `nemo_tts` collection for this "
"tutorial."
msgstr ""

#: ../../source/tts/tutorial.rst:12
msgid "Introduction"
msgstr ""

#: ../../source/tts/tutorial.rst:13
msgid ""
"Speech synthesis, also called Text to Speech (TTS), generally creates "
"human audible speech from text. TTS using neural networks typically "
"consists of two neural networks. The first neural network converts text "
"to an intermediate audio representation, usually derived from a "
"spectrogram. The second neural network, called the neural vocoder, "
"converts the audio representation to audio files, for us in the form of "
".wav files. While there has been recent research shown to be able to "
"combine these two models into one, we focus on the two model approach."
msgstr ""

#: ../../source/tts/tutorial.rst:22
msgid "NeMo supports the following models:"
msgstr ""

#: ../../source/tts/tutorial.rst:24
msgid ""
"`Tacotron 2 <https://arxiv.org/abs/1712.05884>`_ :a model that converts "
"text to mel spectrograms"
msgstr ""

#: ../../source/tts/tutorial.rst:26
msgid ""
"`Waveglow <https://arxiv.org/abs/1811.00002>`_: a model that converts mel"
" spectrograms to audio"
msgstr ""

#: ../../source/tts/tutorial.rst:29
msgid ""
"To train your own models, you can go through the following sections. If "
"you want to run inference with our pre-trained models, skip to the "
":ref:`Inference <TTS Inference>` section."
msgstr ""

#: ../../source/tts/tutorial.rst:34
msgid "Get data"
msgstr ""

#: ../../source/tts/tutorial.rst:35
msgid ""
"Both Tacotron 2 and Waveglow are trained using the `LJSpeech "
"<https://keithito.com/LJ-Speech-Dataset/>`__ dataset. You can use a "
"helper script to get and process the dataset for use with NeMo. The "
"script is located at NeMo/scripts and can be run like so:"
msgstr ""

#: ../../source/tts/tutorial.rst:44
msgid ""
"For more details on the LJSpeech dataset, see :ref:`our docs here "
"<ljspeech>`."
msgstr ""

#: ../../source/tts/tutorial.rst:46
msgid ""
"For Mandarin, we use `Chinese Standard Mandarin Speech Copus <https://www"
".data-baker.com/open_source.html>`__. You can use a helper script to get "
"and process the dataset for use with NeMo. The dataset download link in "
"the script is provided by Databaker (Beijing) Technology Co.,Ltd. The "
"script is located at NeMo/scripts and can be run like so:"
msgstr ""

#: ../../source/tts/tutorial.rst:55
msgid ""
"For more details on the Chinese Standard Mandarin Speech Copus, see "
":ref:`our docs here <csmsc>`."
msgstr ""

#: ../../source/tts/tutorial.rst:58
msgid "Training"
msgstr ""

#: ../../source/tts/tutorial.rst:59
msgid ""
"NeMo supports training both Tacotron 2 and Waveglow. For the purposes of "
"this tutorial, we will be focusing on training Tacotron 2 as that "
"determines the majority of the characteristics of the trained audio such "
"as the gender, and prosody. Furthermore, in our experiments, Waveglow has"
" been shown to work as an unverisal vocoder. Our pretrained Waveglow, "
"though trained on read female English speech, can be used as vocoder for "
"male voices as well as other languages such as Mandarin."
msgstr ""

#: ../../source/tts/tutorial.rst:67
msgid ""
"Training Tacotron 2 can be done by running the `tacotron2.py` file inside"
" NeMo/examples/tts. Assuming you are inside the NeMo/examples/tts "
"directory, you can run the following to start training:"
msgstr ""

#: ../../source/tts/tutorial.rst:75
msgid ""
"Training Tacotron 2 on Mandarin also can be done by running the "
"`tacotron2.py` file. You can run the following to start training:"
msgstr ""

#: ../../source/tts/tutorial.rst:83
msgid ""
"Tacotron 2 normally takes around 20,000 steps for attention to be "
"learned. Once attention is learned, this is when you can use the model to"
" generate audible speech."
msgstr ""

#: ../../source/tts/tutorial.rst:88
msgid "Mixed Precision training"
msgstr ""

#: ../../source/tts/tutorial.rst:89
msgid ""
"Enabling or disabling mixed precision training can be changed through a "
"command line argument --amp_opt_level. Recommended and default values for"
" Tacotron 2 and Waveglow are O1. It can be:"
msgstr ""

#: ../../source/tts/tutorial.rst:93
msgid "O0: float32 training"
msgstr ""

#: ../../source/tts/tutorial.rst:94
msgid "O1: mixed precision training"
msgstr ""

#: ../../source/tts/tutorial.rst:95
msgid "O2: mixed precision training"
msgstr ""

#: ../../source/tts/tutorial.rst:96
msgid "O3: float16 training"
msgstr ""

#: ../../source/tts/tutorial.rst:99
msgid ""
"Because mixed precision requires Tensor Cores it only works on NVIDIA "
"Volta and Turing based GPUs"
msgstr ""

#: ../../source/tts/tutorial.rst:103
msgid "Multi-GPU training"
msgstr ""

#: ../../source/tts/tutorial.rst:104
msgid ""
"Running on multiple GPUs can be enabled but calling running the "
"torch.distributed.launch module and sepcifying the num_gpus as the "
"--nproc_per_node argument:"
msgstr ""

#: ../../source/tts/tutorial.rst:116
msgid "Inference"
msgstr ""

#: ../../source/tts/tutorial.rst:117
msgid ""
"You can now to inference with either your own trained Tacotron 2, or you "
"can use our pre-trained Tacotron 2 model (LINK TO BE ADDED). Please "
"download our pretrained model here (LINK TO BE ADDED). Next create the "
"texts that you want to generate and add them to a json like the training "
"dataset. They should have lines like so:"
msgstr ""

#: ../../source/tts/tutorial.rst:128
msgid "For Mandarin, they should have lines like so:"
msgstr ""

#: ../../source/tts/tutorial.rst:135
msgid ""
"The \"text\" should contain the **pinyin** sequence in Mandarin. The "
"digit behind each Chinese character's pinyin is the **tone**. 0 stands "
"for soft tone."
msgstr ""

#: ../../source/tts/tutorial.rst:137
msgid ""
"Inference can be done with the tts_infer.py file under the "
"NeMo/examples/tts folder like so:"
msgstr ""

#: ../../source/tts/tutorial.rst:144
msgid ""
"For Mandarin, remember to replace the config file of Tacotron 2 with "
"tacotron2_mandarin.yaml."
msgstr ""

