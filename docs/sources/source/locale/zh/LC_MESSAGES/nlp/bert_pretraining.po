# SOME DESCRIPTIVE TITLE.
# Copyright (C) 2018-2019, NVIDIA
# This file is distributed under the same license as the nemo package.
# FIRST AUTHOR <EMAIL@ADDRESS>, 2019.
#
#, fuzzy
msgid ""
msgstr ""
"Project-Id-Version: nemo 0.9.0\n"
"Report-Msgid-Bugs-To: \n"
"POT-Creation-Date: 2019-12-03 17:10+0800\n"
"PO-Revision-Date: YEAR-MO-DA HO:MI+ZONE\n"
"Last-Translator: FULL NAME <EMAIL@ADDRESS>\n"
"Language-Team: LANGUAGE <LL@li.org>\n"
"MIME-Version: 1.0\n"
"Content-Type: text/plain; charset=utf-8\n"
"Content-Transfer-Encoding: 8bit\n"
"Generated-By: Babel 2.7.0\n"

#: ../../source/nlp/bert_pretraining.rst:2
msgid "Pretraining BERT"
msgstr ""

#: ../../source/nlp/bert_pretraining.rst:4
msgid ""
"In this tutorial, we will build and train a masked language model, either"
" from scratch or from a pretrained BERT model, using the BERT "
"architecture :cite:`devlin2018bert`. Make sure you have ``nemo`` and "
"``nemo_nlp`` installed before starting this tutorial. See the "
":ref:`installation` section for more details."
msgstr ""

#: ../../source/nlp/bert_pretraining.rst:6
msgid ""
"The code used in this tutorial can be found at "
"``examples/nlp/bert_pretraining.py``."
msgstr ""

#: ../../source/nlp/bert_pretraining.rst:9
msgid "Introduction"
msgstr ""

#: ../../source/nlp/bert_pretraining.rst:11
msgid ""
"Creating domain-specific BERT models can be advantageous for a wide range"
" of applications. One notable is domain-specific BERT in a biomedical "
"setting, similar to BioBERT :cite:`lee2019biobert` and SciBERT "
":cite:`beltagy2019scibert`."
msgstr ""

#: ../../source/nlp/bert_pretraining.rst:15
msgid "Download Corpus"
msgstr ""

#: ../../source/nlp/bert_pretraining.rst:17
msgid ""
"For demonstration purposes, we will be using the very small WikiText-2 "
"dataset :cite:`merity2016pointer`."
msgstr ""

#: ../../source/nlp/bert_pretraining.rst:19
msgid ""
"To download the dataset, run the script "
"``examples/nlp/scripts/get_wt2.sh``. After downloading and unzipping, the"
" folder should include 3 files that look like this:"
msgstr ""

#: ../../source/nlp/bert_pretraining.rst:27
msgid ""
"To train BERT on a Chinese dataset, you may download the Chinese "
"Wikipedia corpus wiki2019zh_. After downloading, you may unzip and use "
"the script ``examples/nlp/scripts/process_wiki_zh.py`` for preprocessing "
"the raw text."
msgstr ""

#: ../../source/nlp/bert_pretraining.rst:36
msgid "Create the tokenizer model"
msgstr ""

#: ../../source/nlp/bert_pretraining.rst:37
msgid ""
"`BERTPretrainingDataDesc` converts your dataset into the format "
"compatible with `BertPretrainingDataset`. The most computationally "
"intensive step is to tokenize the dataset to create a vocab file and a "
"tokenizer model."
msgstr ""

#: ../../source/nlp/bert_pretraining.rst:39
msgid ""
"You can also use an available vocab or tokenizer model to skip this step."
" If you already have a pretrained tokenizer model, copy it to the "
"``[data_dir]/bert`` folder under the name ``tokenizer.model`` and the "
"script will skip this step."
msgstr ""

#: ../../source/nlp/bert_pretraining.rst:41
msgid ""
"If have an available vocab, say the ``vocab.txt`` file from any "
"`pretrained BERT model`_, copy it to the ``[data_dir]/bert`` folder under"
" the name ``vocab.txt``."
msgstr ""

#: ../../source/nlp/bert_pretraining.rst:54
msgid ""
"We need to define our tokenizer. If you'd like to use a custom vocabulary"
" file, we strongly recommend you use our `SentencePieceTokenizer`. "
"Otherwise, if you'll be using a vocabulary file from another pre-trained "
"BERT model, you should use `NemoBertTokenizer`."
msgstr ""

#: ../../source/nlp/bert_pretraining.rst:56
msgid "To train on a Chinese dataset, you should use `NemoBertTokenizer`."
msgstr ""

#: ../../source/nlp/bert_pretraining.rst:68
msgid "Create the model"
msgstr ""

#: ../../source/nlp/bert_pretraining.rst:72
msgid ""
"We recommend you try this out in a Jupyter notebook. It'll make debugging"
" much easier!"
msgstr ""

#: ../../source/nlp/bert_pretraining.rst:74
msgid ""
"First, we need to create our neural factory with the supported backend. "
"How you should define it depends on whether you'd like to multi-GPU or "
"mixed-precision training. This tutorial assumes that you're training on "
"one GPU, without mixed precision. If you want to use mixed precision, set"
" ``amp_opt_level`` to ``O1`` or ``O2``."
msgstr ""

#: ../../source/nlp/bert_pretraining.rst:85
msgid ""
"We also need to define the BERT model that we will be pre-training. Here,"
" you can configure your model size as needed. If you want to train from "
"scratch, use this:"
msgstr ""

#: ../../source/nlp/bert_pretraining.rst:98
msgid ""
"If you want to start pre-training from existing BERT checkpoints, use the"
" following code. For the full list of BERT model names, check out "
"`nemo_nlp.huggingface.BERT.list_pretrained_models()`"
msgstr ""

#: ../../source/nlp/bert_pretraining.rst:104
msgid ""
"Next, we will define our classifier and loss functions. We will "
"demonstrate how to pre-train with both MLM (masked language model) and "
"NSP (next sentence prediction) losses, but you may observe higher "
"downstream accuracy by only pre-training with MLM loss."
msgstr ""

#: ../../source/nlp/bert_pretraining.rst:122
msgid ""
"Then, we create the pipeline gtom input to output that can be used for "
"both training and evaluation:"
msgstr ""

#: ../../source/nlp/bert_pretraining.rst:161
msgid "Next, we define necessary callbacks:"
msgstr ""

#: ../../source/nlp/bert_pretraining.rst:163
msgid "`SimpleLossLoggerCallback`: tracking loss during training"
msgstr ""

#: ../../source/nlp/bert_pretraining.rst:164
msgid "`EvaluatorCallback`: tracking metrics during evaluation at set intervals"
msgstr ""

#: ../../source/nlp/bert_pretraining.rst:165
msgid "`CheckpointCallback`: saving model checkpoints at set intervals"
msgstr ""

#: ../../source/nlp/bert_pretraining.rst:175
msgid ""
"Tensorboard_ is a great debugging tool. It's not a requirement for this "
"tutorial, but if you'd like to use it, you should install tensorboardX_ "
"and run the following command during pre-training:"
msgstr ""

#: ../../source/nlp/bert_pretraining.rst:185
msgid ""
"We also recommend you export your model's parameters to a config file. "
"This makes it easier to load your BERT model into NeMo later, as "
"explained in our NER tutorial."
msgstr ""

#: ../../source/nlp/bert_pretraining.rst:194
msgid "Finally, you should define your optimizer, and start training!"
msgstr ""

#: ../../source/nlp/bert_pretraining.rst:212
msgid "References"
msgstr ""

