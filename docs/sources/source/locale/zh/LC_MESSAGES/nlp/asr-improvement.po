# SOME DESCRIPTIVE TITLE.
# Copyright (C) 2018-2019, NVIDIA
# This file is distributed under the same license as the nemo package.
# FIRST AUTHOR <EMAIL@ADDRESS>, 2019.
#
#, fuzzy
msgid ""
msgstr ""
"Project-Id-Version: nemo 0.9.0\n"
"Report-Msgid-Bugs-To: \n"
"POT-Creation-Date: 2019-12-03 17:10+0800\n"
"PO-Revision-Date: YEAR-MO-DA HO:MI+ZONE\n"
"Last-Translator: FULL NAME <EMAIL@ADDRESS>\n"
"Language-Team: LANGUAGE <LL@li.org>\n"
"MIME-Version: 1.0\n"
"Content-Type: text/plain; charset=utf-8\n"
"Content-Transfer-Encoding: 8bit\n"
"Generated-By: Babel 2.7.0\n"

#: ../../source/nlp/asr-improvement.rst:2
msgid "Tutorial"
msgstr ""

#: ../../source/nlp/asr-improvement.rst:4
msgid ""
"In this tutorial we will train an ASR postprocessing model to correct "
"mistakes in output of end-to-end speech recognition model. This model "
"method works similar to translation model in contrast to traditional ASR "
"language model rescoring. The model architecture is attention based "
"encoder-decoder where both encoder and decoder are initialized with "
"pretrained BERT language model. To train this model we collected dataset "
"with typical ASR errors by using pretrained Jasper ASR model "
":cite:`li2019jasper`."
msgstr ""

#: ../../source/nlp/asr-improvement.rst:8
msgid "Data"
msgstr ""

#: ../../source/nlp/asr-improvement.rst:9
msgid ""
"**Data collection.** We collected dataset for this tutorial with Jasper "
"ASR model :cite:`li2019jasper` trained on Librispeech dataset "
":cite:`panayotov2015librispeech`. To download the Librispeech dataset, "
"see :ref:`LibriSpeech_dataset`. To obtain the pretrained Jasper model, "
"see :ref:`Jasper_model`. Librispeech training dataset consists of three "
"parts: train-clean-100, train-clean-360, and train-clean-500 which give "
"281k training examples in total. To augment this data we used two "
"techniques:"
msgstr ""

#: ../../source/nlp/asr-improvement.rst:16
msgid ""
"We split all training data into 10 folds and trained 10 Jasper models in "
"cross-validation manner: a model was trained on 9 folds and used to make "
"ASR predictions for the remaining fold."
msgstr ""

#: ../../source/nlp/asr-improvement.rst:17
msgid ""
"We took pretrained Jasper model and enabled dropout during inference on "
"training data. This procedure was repeated multiple times with different "
"random seeds."
msgstr ""

#: ../../source/nlp/asr-improvement.rst:19
msgid ""
"**Data postprocessing.** The collected dataset was postprocessed by "
"removing duplicates and examples with word error rate higher than 0.5. "
"The resulting training dataset consists of 1.7M pairs of \"bad\" "
"English-\"good\" English examples."
msgstr ""

#: ../../source/nlp/asr-improvement.rst:23
msgid ""
"**Dev and test datasets preparation**. Librispeech contains 2 dev "
"datasets (dev-clean and dev-other) and 2 test datasets (test-clean and "
"test-other). For our task we kept the same splits. We fed these datasets "
"to a pretrained Jasper model with the greedy decoding to get the ASR "
"predictions that are used for evaluation in our tutorial."
msgstr ""

#: ../../source/nlp/asr-improvement.rst:30
msgid "Importing parameters from pretrained BERT"
msgstr ""

#: ../../source/nlp/asr-improvement.rst:31
msgid ""
"Both encoder and decoder are initialized with pretrained BERT parameters."
" Since BERT language model has the same architecture as transformer "
"encoder, there is no need to do anything additional. To prepare decoder "
"parameters from pretrained BERT we wrote a script "
"``get_decoder_params_from_bert.py`` that downloads BERT parameters from "
"the ``pytorch-transformers`` repository "
":cite:`huggingface2019transformers` and maps them into a transformer "
"decoder. Encoder-decoder attention is initialized with self-attention "
"parameters. The script is located under ``scripts`` directory and accepts"
" 2 arguments:"
msgstr ""

#: ../../source/nlp/asr-improvement.rst:35
msgid "``--model_name``: e.g. ``bert-base-cased``, ``bert-base-uncased``, etc."
msgstr ""

#: ../../source/nlp/asr-improvement.rst:36
msgid "``--save_to``: a directory where the parameters will be saved"
msgstr ""

#: ../../source/nlp/asr-improvement.rst:44
msgid "Neural modules overview"
msgstr ""

#: ../../source/nlp/asr-improvement.rst:45
msgid ""
"First, as with all models built in NeMo, we instantiate Neural Module "
"Factory which defines 1) backend (PyTorch or TensorFlow), 2) mixed "
"precision optimization level, 3) local rank of the GPU, and 4) an "
"experiment manager that creates a timestamped folder to store "
"checkpoints, relevant outputs, log files, and TensorBoard graphs."
msgstr ""

#: ../../source/nlp/asr-improvement.rst:58
msgid ""
"Then we define tokenizer to convert tokens into indices. We will use "
"``bert-base-uncased`` vocabulary, since our dataset only contains uncased"
" text:"
msgstr ""

#: ../../source/nlp/asr-improvement.rst:65
msgid ""
"The encoder block is a neural module corresponding to BERT language model"
" from ``nemo_nlp.huggingface`` collection:"
msgstr ""

#: ../../source/nlp/asr-improvement.rst:76
msgid ""
"Making embedding size (as well as all other tensor dimensions) divisible "
"by 8 will help to get the best GPU utilization and speed-up with mixed "
"precision training."
msgstr ""

#: ../../source/nlp/asr-improvement.rst:91
msgid ""
"Next, we construct transformer decoder neural module. Since we will be "
"initializing decoder with pretrained BERT parameters, we set hidden "
"activation to ``\"hidden_act\": \"gelu\"`` and learn positional encodings"
" ``\"learn_positional_encodings\": True``:"
msgstr ""

#: ../../source/nlp/asr-improvement.rst:108
msgid ""
"To load the pretrained parameters into decoder, we use ``restore_from`` "
"attribute function of the decoder neural module:"
msgstr ""

#: ../../source/nlp/asr-improvement.rst:116
msgid "Model training"
msgstr ""

#: ../../source/nlp/asr-improvement.rst:118
msgid ""
"To train the model run ``asr_postprocessor.py.py`` located in "
"``examples/nlp`` directory. We train with novograd optimizer "
":cite:`ginsburg2019stochastic`, learning rate ``lr=0.001``, polynomial "
"learning rate decay policy, ``1000`` warmup steps, per-gpu batch size of "
"``4096*8`` tokens, and ``0.25`` dropout probability. We trained on 8 "
"GPUS. To launch the training in multi-gpu mode run the following command:"
msgstr ""

#: ../../source/nlp/asr-improvement.rst:127
msgid "References"
msgstr ""

