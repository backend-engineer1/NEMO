# SOME DESCRIPTIVE TITLE.
# Copyright (C) 2018-2019, NVIDIA
# This file is distributed under the same license as the nemo package.
# FIRST AUTHOR <EMAIL@ADDRESS>, 2019.
#
#, fuzzy
msgid ""
msgstr ""
"Project-Id-Version: nemo 0.9.0\n"
"Report-Msgid-Bugs-To: \n"
"POT-Creation-Date: 2019-12-03 17:10+0800\n"
"PO-Revision-Date: YEAR-MO-DA HO:MI+ZONE\n"
"Last-Translator: FULL NAME <EMAIL@ADDRESS>\n"
"Language-Team: LANGUAGE <LL@li.org>\n"
"MIME-Version: 1.0\n"
"Content-Type: text/plain; charset=utf-8\n"
"Content-Transfer-Encoding: 8bit\n"
"Generated-By: Babel 2.7.0\n"

#: ../../source/nlp/neural_machine_translation.rst:2
msgid "Tutorial"
msgstr ""

#: ../../source/nlp/neural_machine_translation.rst:4
msgid ""
"In this tutorial we are going to implement Neural Machine Translation "
"(NMT) system based on `Transformer encoder-decoder architecture "
"<https://arxiv.org/abs/1706.03762>`_ :cite:`vaswani2017attention`. All "
"code used in this tutorial is based on ``examples/nlp/nmt_tutorial.py``."
msgstr ""

#: ../../source/nlp/neural_machine_translation.rst:7
msgid "Preliminaries"
msgstr ""

#: ../../source/nlp/neural_machine_translation.rst:9
msgid ""
"**Dataset.** We use WMT16 English-German dataset which consists of "
"approximately 4.5 million sentence pairs before preprocessing. To clean "
"the dataset we remove all sentence pairs such that:"
msgstr ""

#: ../../source/nlp/neural_machine_translation.rst:11
msgid ""
"The length of either source or target is greater than 128 or smaller than"
" 3 tokens."
msgstr ""

#: ../../source/nlp/neural_machine_translation.rst:12
msgid "Absolute difference between source and target is greater than 25 tokens."
msgstr ""

#: ../../source/nlp/neural_machine_translation.rst:13
msgid "One sentence is more than 2.5 times longer than the other."
msgstr ""

#: ../../source/nlp/neural_machine_translation.rst:14
msgid ""
"Target sentence is the exact copy of the source sentence "
":cite:`ott2018analyzing`."
msgstr ""

#: ../../source/nlp/neural_machine_translation.rst:16
msgid ""
"We use newstest2013 for development and newstest2014 for testing. All "
"datasets, as well as the tokenizer model can be downloaded from `here "
"<https://drive.google.com/open?id=1AErD1hEg16Yt28a-IGflZnwGTg9O27DT>`__. "
"In the following steps, we assume that all data is located at "
"**<path_to_data>**."
msgstr ""

#: ../../source/nlp/neural_machine_translation.rst:18
msgid ""
"**Resources.** Training script ``examples/nlp/nmt_tutorial.py`` used in "
"this tutorial allows to train Transformer-big architecture to **29.2** "
"BLEU / **28.5** SacreBLEU on newstest2014 in approximately 15 hours on "
"NVIDIA's DGX-1 with 16GB Volta GPUs. This setup can also be replicated "
"with fewer resources by using more steps of gradient accumulation "
":cite:`ott2018scaling`."
msgstr ""

#: ../../source/nlp/neural_machine_translation.rst:21
msgid ""
"Launching training script without any arguments will run training on much"
" smaller dataset (newstest2013) of 3000 sentence pairs and validate on "
"the subset of this dataset consisting of 100 sentence pairs. This is "
"useful for debugging purposes: if everything is set up correctly, "
"validation BLEU will reach >99 and training / validation losses will go "
"to <1.5 pretty fast."
msgstr ""

#: ../../source/nlp/neural_machine_translation.rst:24
msgid "Code overview"
msgstr ""

#: ../../source/nlp/neural_machine_translation.rst:26
msgid ""
"First of all, we instantiate Neural Module Factory which defines 1) "
"backend, 2) mixed precision optimization level, and 3) local rank of the "
"GPU."
msgstr ""

#: ../../source/nlp/neural_machine_translation.rst:37
msgid ""
"We define tokenizer which allows to transform input text into tokens. In "
"this tutorial, we use joint `Byte Pair Encodings (BPE) "
"<https://arxiv.org/abs/1508.07909>`_ :cite:`sennrich2015neural` trained "
"on WMT16 En-De corpus with `YouTokenToMe library "
"<https://github.com/VKCOM/YouTokenToMe>`_. In contrast to the models "
"presented in the literature (which usually have vocabularies of size "
"30000+), we work with 4x smaller vocabulary of 8192 BPEs. It achieves the"
" same level of performance but allows to increase the batch size by 20% "
"which in turn leads to faster convergence."
msgstr ""

#: ../../source/nlp/neural_machine_translation.rst:47
msgid ""
"To leverage the best GPU utilization and mixed precision speedup, make "
"sure that the vocabulary size (as well as all sizes in the model) is "
"divisible by 8."
msgstr ""

#: ../../source/nlp/neural_machine_translation.rst:49
msgid ""
"If the source language differs from the target language a lot, then we "
"should use different tokenizers for them. For example, if the source "
"language is English and the target language is Chinese, we can use "
"YouTokenToMeTokenizer for source and CharTokenizer for target. This means"
" the input of the model are English BPEs and the output of the model are "
"Chinese characters."
msgstr ""

#: ../../source/nlp/neural_machine_translation.rst:60
msgid ""
"You should pass the path of the vocabulary file to the CharTokenizer. The"
" vocabulary file should contain the characters of the corresponding "
"language."
msgstr ""

#: ../../source/nlp/neural_machine_translation.rst:62
msgid "Next, we define all Neural Modules necessary for our model:"
msgstr ""

#: ../../source/nlp/neural_machine_translation.rst:64
msgid "Transformer Encoder and Decoder."
msgstr ""

#: ../../source/nlp/neural_machine_translation.rst:65
msgid ""
"`TokenClassifier` for mapping output of the decoder into probability "
"distribution over vocabulary."
msgstr ""

#: ../../source/nlp/neural_machine_translation.rst:66
msgid "Beam Search module for generating translations."
msgstr ""

#: ../../source/nlp/neural_machine_translation.rst:67
msgid "Loss function (cross entropy with label smoothing regularization)."
msgstr ""

#: ../../source/nlp/neural_machine_translation.rst:77
msgid ""
"Following `Press and Wolf, 2016 <https://arxiv.org/abs/1608.05859>`_ "
":cite:`press2016using`, we also tie the parameters of embedding and "
"softmax layers:"
msgstr ""

#: ../../source/nlp/neural_machine_translation.rst:85
msgid ""
"You should not tie the parameters if you use different tokenizers for "
"source and target."
msgstr ""

#: ../../source/nlp/neural_machine_translation.rst:87
msgid ""
"Then, we create the pipeline gtom input to output that can be used for "
"both training and evaluation. An important element of this pipeline is "
"the datalayer that packs input sentences into batches of similar length "
"to minimize the use of padding symbol. Note, that the maximum allowed "
"number of tokens in a batch is given in **source and target** tokens."
msgstr ""

#: ../../source/nlp/neural_machine_translation.rst:122
msgid "Next, we define necessary callbacks:"
msgstr ""

#: ../../source/nlp/neural_machine_translation.rst:124
msgid "`SimpleLossLoggerCallback`: tracking loss during training"
msgstr ""

#: ../../source/nlp/neural_machine_translation.rst:125
msgid ""
"`EvaluatorCallback`: tracking BLEU score on evaluation dataset at set "
"intervals"
msgstr ""

#: ../../source/nlp/neural_machine_translation.rst:126
msgid "`CheckpointCallback`: saving model checkpoints"
msgstr ""

#: ../../source/nlp/neural_machine_translation.rst:138
msgid ""
"The BLEU score is calculated between detokenized translation (generated "
"with beam search) and genuine evaluation dataset. For the sake of "
"completeness, we report both  `SacreBLEU "
"<https://github.com/mjpost/sacreBLEU>`_ :cite:`post2018call` and "
"`tokenized BLEU score <https://github.com/moses-"
"smt/mosesdecoder/blob/master/scripts/generic/multi-bleu.perl>`_ commonly "
"used in the literature."
msgstr ""

#: ../../source/nlp/neural_machine_translation.rst:140
msgid "Finally, we define the optimization parameters and run the whole pipeline."
msgstr ""

#: ../../source/nlp/neural_machine_translation.rst:160
msgid "Model training"
msgstr ""

#: ../../source/nlp/neural_machine_translation.rst:162
msgid ""
"To train the Transformer-big model, run ``nmt_tutorial.py`` located at "
"``nemo/examples/nlp``:"
msgstr ""

#: ../../source/nlp/neural_machine_translation.rst:176
msgid ""
"This command runs training on 8 GPUs with at least 16 GB of memory. If "
"your GPUs have less memory, decrease the **batch_size** parameter. To "
"train with bigger batches which do not fit into the memory, increase the "
"**iter_per_step** parameter."
msgstr ""

#: ../../source/nlp/neural_machine_translation.rst:178
msgid ""
"If you want to train a English-Chinese translation model. You should also"
" set **--src_lang** to **en**, **--tgt_lang** to **zh**, and "
"**--tgt_tokenizer_model** to the path of the Chinese vocabulary file. You"
" can refer to the Chinese data sample located at "
"``/tests/data/nmt_en_zh_sample_data/``."
msgstr ""

#: ../../source/nlp/neural_machine_translation.rst:181
msgid "Translation with pretrained model"
msgstr ""

#: ../../source/nlp/neural_machine_translation.rst:183
msgid ""
"Put your saved checkpoint (or download good checkpoint which obtains 28.5"
" SacreBLEU on newstest2014 from `here "
"<https://ngc.nvidia.com/catalog/models/nvidia:transformer_big_en_de_8k>`__)"
" into **<path_to_ckpt>**."
msgstr ""

#: ../../source/nlp/neural_machine_translation.rst:184
msgid "Run ``nmt_tutorial.py`` in an interactive mode::"
msgstr ""

#: ../../source/nlp/neural_machine_translation.rst:196
msgid "References"
msgstr ""

