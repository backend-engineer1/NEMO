# SOME DESCRIPTIVE TITLE.
# Copyright (C) 2018-2019, NVIDIA
# This file is distributed under the same license as the nemo package.
# FIRST AUTHOR <EMAIL@ADDRESS>, 2019.
#
#, fuzzy
msgid ""
msgstr ""
"Project-Id-Version: nemo 0.9.0\n"
"Report-Msgid-Bugs-To: \n"
"POT-Creation-Date: 2019-12-03 17:10+0800\n"
"PO-Revision-Date: YEAR-MO-DA HO:MI+ZONE\n"
"Last-Translator: FULL NAME <EMAIL@ADDRESS>\n"
"Language-Team: LANGUAGE <LL@li.org>\n"
"MIME-Version: 1.0\n"
"Content-Type: text/plain; charset=utf-8\n"
"Content-Transfer-Encoding: 8bit\n"
"Generated-By: Babel 2.7.0\n"

#: ../../source/nlp/ner.rst:2
msgid "Tutorial"
msgstr ""

#: ../../source/nlp/ner.rst:4
msgid ""
"Make sure you have ``nemo`` and ``nemo_nlp`` installed before starting "
"this tutorial. See the :ref:`installation` section for more details."
msgstr ""

#: ../../source/nlp/ner.rst:8
msgid "Introduction"
msgstr ""

#: ../../source/nlp/ner.rst:10
msgid ""
"This tutorial explains how to implement named entity recognition (NER) in"
" NeMo. We'll show how to do this with a pre-trained BERT model, or with "
"one that you trained yourself! For more details, check out our BERT "
"pretraining tutorial."
msgstr ""

#: ../../source/nlp/ner.rst:13
msgid "Download Dataset"
msgstr ""

#: ../../source/nlp/ner.rst:15
msgid ""
"`CoNLL-2003`_ is a standard evaluation dataset for NER, but any NER "
"dataset will work. CoNLL-2003 dataset could also be found `here`_. The "
"only requirement is that the data is splitted into 2 files: text.txt and "
"labels.txt. The text.txt files should be formatted like this:"
msgstr ""

#: ../../source/nlp/ner.rst:26
msgid "The labels.txt files should be formatted like this:"
msgstr ""

#: ../../source/nlp/ner.rst:34
msgid ""
"Each line of the text.txt file contains text sequences, where words are "
"separated with spaces. The labels.txt file contains corresponding labels "
"for each word in text.txt, the labels are separated with spaces. Each "
"line of the files should follow the format: [WORD] [SPACE] [WORD] [SPACE]"
" [WORD] (for text.txt) and [LABEL] [SPACE] [LABEL] [SPACE] [LABEL] (for "
"labels.txt). There can be columns in between for part-of-speech tags, as "
"shown on the `CoNLL-2003 website`_."
msgstr ""

#: ../../source/nlp/ner.rst:41
msgid "Training"
msgstr ""

#: ../../source/nlp/ner.rst:45
msgid ""
"We recommend you try this out in a Jupyter notebook. It'll make debugging"
" much easier! See examples/nlp/NERWithBERT.ipynb"
msgstr ""

#: ../../source/nlp/ner.rst:48
msgid ""
"First, we need to create our neural factory with the supported backend. "
"How you should define it depends on whether you'd like to multi-GPU or "
"mixed-precision training. This tutorial assumes that you're training on "
"one GPU, without mixed precision (``optimization_level=\"O0\"``). If you "
"want to use mixed precision, set ``optimization_level`` to ``O1`` or "
"``O2``."
msgstr ""

#: ../../source/nlp/ner.rst:57
msgid ""
"Next, we'll need to define our tokenizer and our BERT model. There are a "
"couple of different ways you can do this. Keep in mind that NER benefits "
"from casing (\"New York City\" is easier to identify than \"new york "
"city\"), so we recommend you use cased models."
msgstr ""

#: ../../source/nlp/ner.rst:59
msgid ""
"If you're using a standard BERT model, you should do it as follows. To "
"see the full list of BERT model names, check out "
"``nemo_nlp.huggingface.BERT.list_pretrained_models()``"
msgstr ""

#: ../../source/nlp/ner.rst:67
msgid ""
"If you're using a BERT model that you pre-trained yourself, you should do"
" it like this. You should replace ``args.bert_checkpoint`` with the path "
"to your checkpoint file."
msgstr ""

#: ../../source/nlp/ner.rst:78
msgid ""
"We need to create the classifier to sit on top of the pretrained model "
"and define the loss function:"
msgstr ""

#: ../../source/nlp/ner.rst:89
msgid "And create the pipeline that can be used for both training and evaluation."
msgstr ""

#: ../../source/nlp/ner.rst:123
msgid "Now, create the train and evaluation datasets:"
msgstr ""

#: ../../source/nlp/ner.rst:129
msgid "Now, we will set up our callbacks. We will use 3 callbacks:"
msgstr ""

#: ../../source/nlp/ner.rst:131
msgid "`SimpleLossLoggerCallback` to print loss values during training"
msgstr ""

#: ../../source/nlp/ner.rst:132
msgid ""
"`EvaluatorCallback` to evaluate our F1 score on the dev dataset. In this "
"example, `EvaluatorCallback` will also output predictions to "
"`output.txt`, which can be helpful with debugging what our model gets "
"wrong."
msgstr ""

#: ../../source/nlp/ner.rst:133
msgid "`CheckpointCallback` to save and restore checkpoints."
msgstr ""

#: ../../source/nlp/ner.rst:137
msgid ""
"Tensorboard_ is a great debugging tool. It's not a requirement for this "
"tutorial, but if you'd like to use it, you should install tensorboardX_ "
"and run the following command during fine-tuning:"
msgstr ""

#: ../../source/nlp/ner.rst:162
msgid ""
"Finally, we will define our learning rate policy and our optimizer, and "
"start training."
msgstr ""

#: ../../source/nlp/ner.rst:178
msgid "To train NEW with BERT using the provided scripts"
msgstr ""

#: ../../source/nlp/ner.rst:180
msgid "To run the provided training script:"
msgstr ""

#: ../../source/nlp/ner.rst:186
msgid "To run inference:"
msgstr ""

#: ../../source/nlp/ner.rst:193
msgid ""
"Note, label_ids.csv file will be generated during training and stored in "
"the data_dir folder."
msgstr ""

#: ../../source/nlp/ner.rst:196
msgid "Using Other BERT Models"
msgstr ""

#: ../../source/nlp/ner.rst:198
msgid ""
"In addition to using pre-trained BERT models from Google and BERT models "
"that you've trained yourself, in NeMo it's possible to use other third-"
"party BERT models as well, as long as the weights were exported with "
"PyTorch. For example, if you want to fine-tune an NER task with "
"SciBERT_..."
msgstr ""

#: ../../source/nlp/ner.rst:211
msgid ""
"And then, when you load your BERT model, you should specify the name of "
"the directory for the model name."
msgstr ""

#: ../../source/nlp/ner.rst:220
msgid ""
"If you want to use a TensorFlow-based model, such as BioBERT, you should "
"be able to use it in NeMo by first using this `model conversion script`_ "
"provided by Hugging Face."
msgstr ""

