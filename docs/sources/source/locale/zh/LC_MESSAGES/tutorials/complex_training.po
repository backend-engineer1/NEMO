# SOME DESCRIPTIVE TITLE.
# Copyright (C) 2018-2019, NVIDIA
# This file is distributed under the same license as the nemo package.
# FIRST AUTHOR <EMAIL@ADDRESS>, 2019.
#
#, fuzzy
msgid ""
msgstr ""
"Project-Id-Version: nemo 0.9.0\n"
"Report-Msgid-Bugs-To: \n"
"POT-Creation-Date: 2019-12-03 17:10+0800\n"
"PO-Revision-Date: YEAR-MO-DA HO:MI+ZONE\n"
"Last-Translator: FULL NAME <EMAIL@ADDRESS>\n"
"Language-Team: LANGUAGE <LL@li.org>\n"
"MIME-Version: 1.0\n"
"Content-Type: text/plain; charset=utf-8\n"
"Content-Transfer-Encoding: 8bit\n"
"Generated-By: Babel 2.7.0\n"

#: ../../source/tutorials/complex_training.rst:2
msgid "Complex Training Pipelines (GAN Example)"
msgstr ""

#: ../../source/tutorials/complex_training.rst:4
msgid ""
"So far, training examples have utilized one optimizer to optimize one "
"loss across all Trainable Neural Modules. NeMo further extends to uses "
"cases that require multiple losses and multiple optimizers."
msgstr ""

#: ../../source/tutorials/complex_training.rst:9
msgid "All of our pipelines only support one datalayer."
msgstr ""

#: ../../source/tutorials/complex_training.rst:12
msgid "Multiple Losses"
msgstr ""

#: ../../source/tutorials/complex_training.rst:13
msgid ""
"Taking our Hello World example from earlier. Let's say that we now want "
"to optimize for both a square error loss and a l1 loss. We can pass both "
"the square error loss tensor and the l1 loss tensor to "
":meth:`NeuralFactory.train()<nemo.core.neural_factory.NeuralModuleFactory.train>`."
" An example is shown below."
msgstr ""

#: ../../source/tutorials/complex_training.rst:56
msgid ""
"We can further extend this to optimize one loss at a time. Let's say that"
" instead of computing derivatives and gradients with respect to mse_loss "
"+ l1_loss, we want to first compute gradients with respect to mse_loss, "
"do a weight update, and then compute gradients with respect to l1_loss, "
"and do another weight update. Here we have to define our own training "
"loop."
msgstr ""

#: ../../source/tutorials/complex_training.rst:118
msgid "Multiple Optimizers and Multiple Losses"
msgstr ""

#: ../../source/tutorials/complex_training.rst:119
msgid ""
"NeMo additionally supports use cases where a user would want to create "
"more than one optimizer. One example of such a use case would be a GAN "
"where we want to create an optimizer for the generator and an optimizer "
"for the discriminator. We also want to optimize for different losses in "
"both cases. Here are the highlights from examples/images/gan.py that "
"enable such behaviour."
msgstr ""

