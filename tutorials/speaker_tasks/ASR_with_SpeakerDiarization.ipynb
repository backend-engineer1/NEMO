{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Automatic Speech Recognition with Speaker Diarization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "You can run either this notebook locally (if you have all the dependencies and a GPU) or on Google Colab.\n",
    "\n",
    "Instructions for setting up Colab are as follows:\n",
    "1. Open a new Python 3 notebook.\n",
    "2. Import this notebook from GitHub (File -> Upload Notebook -> \"GITHUB\" tab -> copy/paste GitHub URL)\n",
    "3. Connect to an instance with a GPU (Runtime -> Change runtime type -> select \"GPU\" for hardware accelerator)\n",
    "4. Run this cell to set up dependencies.\n",
    "\"\"\"\n",
    "# If you're using Google Colab and not running locally, run this cell.\n",
    "\n",
    "## Install dependencies\n",
    "!pip install wget\n",
    "!apt-get install sox libsndfile1 ffmpeg\n",
    "!pip install unidecode\n",
    "\n",
    "# ## Install NeMo\n",
    "BRANCH = 'main'\n",
    "!python -m pip install git+https://github.com/NVIDIA/NeMo.git@$BRANCH#egg=nemo_toolkit[asr]\n",
    "\n",
    "## Install TorchAudio\n",
    "!pip install torchaudio -f https://download.pytorch.org/whl/torch_stable.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Speaker diarization lets us figure out \"who spoke when\" in the transcription. Without speaker diarization, we cannot distinguish the speakers in the transcript generated from automatic speech recognition (ASR). Nowadays, ASR combined with speaker diarization has shown immense use in many tasks, ranging from analyzing meeting transcription to media indexing. \n",
    "\n",
    "In this tutorial, we demonstrate how we can get ASR transcriptions combined with speaker labels. Since we don't include detailed process of getting ASR result or diarization result, please refer to the following links for more in-depth description.\n",
    "\n",
    "If you need detailed understanding of transcribing words with ASR, refer to this [ASR Tutorial](https://github.com/NVIDIA/NeMo/blob/stable/tutorials/asr/ASR_with_NeMo.ipynb) tutorial.\n",
    "\n",
    "\n",
    "For detailed parameter setting and execution of speaker diarization, refer to this [Diarization Inference](https://github.com/NVIDIA/NeMo/blob/stable/tutorials/speaker_tasks/Speaker_Diarization_Inference.ipynb) tutorial.\n",
    "\n",
    "\n",
    "An example script that runs ASR and speaker diarization together can be found at [ASR with Diarization](https://github.com/NVIDIA/NeMo/blob/main/examples/speaker_tasks/diarization/asr_with_diarization.py).\n",
    "\n",
    "### Speaker diarization in ASR pipeline\n",
    "\n",
    "Speaker diarization result in ASR pipeline should align well with ASR output. Thus, we use ASR output to create Voice Activity Detection (VAD) timestamps to obtain segments we want to diarize. The segments we obtain from the VAD timestamps are further segmented into sub-segments in the speaker diarization step. Finally, after obtaining the speaker labels from speaker diarization, we match the decoded words with speaker labels to generate a transcript with speaker labels.\n",
    "\n",
    "    ASR → VAD timestamps and decoded words → speaker diarization → speaker label matching"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import libraries\n",
    "\n",
    "Let's first import nemo asr and other libraries for visualization purposes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nemo.collections.asr as nemo_asr\n",
    "import numpy as np\n",
    "from IPython.display import Audio, display\n",
    "import librosa\n",
    "import os\n",
    "import wget\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import nemo\n",
    "from nemo.collections.asr.parts.utils.diarization_utils import ASR_DIAR_OFFLINE\n",
    "\n",
    "import glob\n",
    "\n",
    "import pprint\n",
    "pp = pprint.PrettyPrinter(indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_file(path_to_file):\n",
    "    with open(path_to_file) as f:\n",
    "        contents = f.read().splitlines()\n",
    "    return contents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We demonstrate this tutorial using a merged an4 audio, that has two speakers (male and female) speaking dates in different formats. If the audio does not exist, we download it, and listen to it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ROOT = os.getcwd()\n",
    "data_dir = os.path.join(ROOT,'data')\n",
    "os.makedirs(data_dir, exist_ok=True)\n",
    "an4_audio_url = \"https://nemo-public.s3.us-east-2.amazonaws.com/an4_diarize_test.wav\"\n",
    "if not os.path.exists(os.path.join(data_dir,'an4_diarize_test.wav')):\n",
    "    AUDIO_FILENAME = wget.download(an4_audio_url, data_dir)\n",
    "else:\n",
    "    AUDIO_FILENAME = os.path.join(data_dir,'an4_diarize_test.wav')\n",
    "\n",
    "audio_file_list = glob.glob(f\"{data_dir}/*.wav\")\n",
    "print(\"Input audio file list: \\n\", audio_file_list)\n",
    "\n",
    "signal, sample_rate = librosa.load(AUDIO_FILENAME, sr=None)\n",
    "display(Audio(signal,rate=sample_rate))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`display_waveform()` and `get_color()` functions are defined for displaying the waveform with diarization results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_waveform(signal,text='Audio',overlay_color=[]):\n",
    "    fig,ax = plt.subplots(1,1)\n",
    "    fig.set_figwidth(20)\n",
    "    fig.set_figheight(2)\n",
    "    plt.scatter(np.arange(len(signal)),signal,s=1,marker='o',c='k')\n",
    "    if len(overlay_color):\n",
    "        plt.scatter(np.arange(len(signal)),signal,s=1,marker='o',c=overlay_color)\n",
    "    fig.suptitle(text, fontsize=16)\n",
    "    plt.xlabel('time (secs)', fontsize=18)\n",
    "    plt.ylabel('signal strength', fontsize=14);\n",
    "    plt.axis([0,len(signal),-0.5,+0.5])\n",
    "    time_axis,_ = plt.xticks();\n",
    "    plt.xticks(time_axis[:-1],time_axis[:-1]/sample_rate);\n",
    "    \n",
    "COLORS=\"b g c m y\".split()\n",
    "\n",
    "def get_color(signal,speech_labels,sample_rate=16000):\n",
    "    c=np.array(['k']*len(signal))\n",
    "    for time_stamp in speech_labels:\n",
    "        start,end,label=time_stamp.split()\n",
    "        start,end = int(float(start)*16000),int(float(end)*16000),\n",
    "        if label == \"speech\":\n",
    "            code = 'red'\n",
    "        else:\n",
    "            code = COLORS[int(label.split('_')[-1])]\n",
    "        c[start:end]=code\n",
    "    \n",
    "    return c "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the above function, we can display the waveform of the example audio clip. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display_waveform(signal)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parameter setting for ASR and diarization\n",
    "First, we need to setup the following parameters for ASR and diarization. We start our demonstration by first transcribing the audio recording using our pretrained ASR model `QuartzNet15x5Base-En` and use the CTC output probabilities to get timestamps for the spoken words. We then use these timestamps to get speaker label information using speaker diarizer model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CONFIG_URL = \"https://raw.githubusercontent.com/NVIDIA/NeMo/main/examples/speaker_tasks/diarization/conf/speaker_diarization.yaml\"\n",
    "\n",
    "params = {\n",
    "    \"time_stride\": 0.02,  # This should not be changed if you are using QuartzNet15x5Base.\n",
    "    \"offset\": -0.18,  # This should not be changed if you are using QuartzNet15x5Base.\n",
    "    \"round_float\": 2,\n",
    "    \"window_length_in_sec\": 1.5,\n",
    "    \"shift_length_in_sec\": 0.25,\n",
    "    \"print_transcript\": False,\n",
    "    \"threshold\": 50,  # minimun width to consider non-speech activity\n",
    "    \"external_oracle_vad\": False,\n",
    "    \"diar_config_url\": CONFIG_URL,\n",
    "    \"ASR_model_name\": 'QuartzNet15x5Base-En',\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's create an instance from ASR_DIAR_OFFLINE class. We pass the ``params`` variable to setup the parameters for both ASR and diarization. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "asr_diar_offline = ASR_DIAR_OFFLINE(params)\n",
    "asr_model = asr_diar_offline.set_asr_model(params['ASR_model_name'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will create folders that we need for storing VAD stamps and ASR/diarization results.\n",
    "\n",
    "Under the folder named ``asr_with_diar``, the following folders will be created.\n",
    "\n",
    "- ``oracle_vad``\n",
    "- ``json_result``\n",
    "- ``transcript_with_speaker_labels``\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "asr_diar_offline.create_directories()\n",
    "\n",
    "print(\"Folders are created as below.\")\n",
    "print(\"VAD file path: \\n\", asr_diar_offline.oracle_vad_dir)\n",
    "print(\"JSON result path: \\n\", asr_diar_offline.json_result_dir)\n",
    "print(\"Transcript result path: \\n\", asr_diar_offline.trans_with_spks_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run ASR and get word timestamps\n",
    "Before we run speaker diarization, we should run ASR and get the ASR output to generate VAD timestamps and decoded words. `run_ASR()` function extracts word sequence, logit values for each frame and timestamps for each token (character). These three types of results are included in ``ASR_output`` variable. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ASR_output = asr_diar_offline.run_ASR(asr_model, audio_file_list)\n",
    "\n",
    "print(\"Decoded word output: \\n\", ASR_output[0][0])\n",
    "print(\"Logit values for each frame:  \\n\",ASR_output[0][1].shape, ASR_output[0][1])\n",
    "print(\"Framelevel timestamps for each token: \\n\", ASR_output[0][2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following three variables are obtained from `get_speech_labels_list()` function.\n",
    "\n",
    "- words List[str]: contains the sequence of words.\n",
    "- spaces List[int]: contains frame level index of the end of the last word and the start time of the next word. \n",
    "- word_ts List[int]: contains frame level index of the start and the end of each word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "words, spaces, word_ts = asr_diar_offline.get_speech_labels_list(ASR_output, audio_file_list)\n",
    "\n",
    "print(\"Transcribed words: \\n\", words[0])\n",
    "print(\"Spaces between words: \\n\", spaces[0])\n",
    "print(\"Timestamps for the words: \\n\", word_ts[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Then we multiply `params['time_stride']=0.02` to get timestamp in second.\n",
    "\n",
    "### Run diarization with extracted VAD timestamp\n",
    "\n",
    "\n",
    "We need to convert ASR based VAD output (*.rttm format) to VAD manifest (*.json format) file. The following function converts the rttm files into manifest file and returns the path for manifest file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vad_manifest_path = asr_diar_offline.write_VAD_rttm(asr_diar_offline.oracle_vad_dir, audio_file_list)\n",
    "\n",
    "print(\"VAD manifest file path: \\n\", vad_manifest_path)\n",
    "print(\"VAD Manifest file content:\")\n",
    "pp.pprint(read_file(vad_manifest_path))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that all the components for diarization is ready, let's run diarization by calling `run_diarization()` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_speakers = None # If we know the number of speakers, we can assign \"2\".\n",
    "pretrained_speaker_model = 'ecapa_tdnn'\n",
    "\n",
    "asr_diar_offline.run_diarization(audio_file_list, vad_manifest_path, num_speakers, pretrained_speaker_model)\n",
    "\n",
    "print(nemo.__file__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`run_diarization()` function will create `./asr_with_diar/oracle_vad/pred_rttm/an4_diarize_test.rttm` file. Let's see what is written in this `rttm` file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_speaker_label_rttm_path = f\"{ROOT}/asr_with_diar/oracle_vad/pred_rttms/an4_diarize_test.rttm\"\n",
    "pred_rttm = read_file(predicted_speaker_label_rttm_path)\n",
    "\n",
    "pp.pprint(pred_rttm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check out the diarization output. `get_diarization_labels()` function extracts the estimated speaker label information with timestamps from the predicted rttm file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "diar_labels = asr_diar_offline.get_diarization_labels(audio_file_list)\n",
    "\n",
    "print(\"Diarization Labels:\")\n",
    "pp.pprint(diar_labels[0])\n",
    "\n",
    "color = get_color(signal, diar_labels[0])\n",
    "display_waveform(signal,'Audio with Speaker Labels',color)\n",
    "display(Audio(signal,rate=16000))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check the transcription output\n",
    "Now we've done all the processes for running ASR and diarization, let's match the diarization result with ASR result and get the final output. `write_json_and_transcript()` function matches diarization output `diar_labels` with `words` using the timestamp information `word_ts`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "asr_output_dict = asr_diar_offline.write_json_and_transcript(audio_file_list, diar_labels, words, word_ts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After running `write_json_and_transcript()` function, the transcription output will be located in `./asr_with_diar/transcript_with_speaker_labels` folder, which shows **start time to end time of the utterance, speaker ID, and words spoken** during the notified time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transcription_path_to_file = f\"{ROOT}/asr_with_diar/transcript_with_speaker_labels/an4_diarize_test.txt\"\n",
    "transcript = read_file(transcription_path_to_file)\n",
    "\n",
    "pp.pprint(transcript)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another output is transcription output in JSON format, which is saved in `./asr_with_diar/json_result`. \n",
    "\n",
    "In the JSON format output, we include information such as **transcription, estimated number of speakers (variable named `speaker_count`), start and end time of each word and most importantly, speaker label for each word.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "transcription_path_to_file = f\"{ROOT}/asr_with_diar/json_result/an4_diarize_test.json\"\n",
    "json_contents = read_file(transcription_path_to_file)\n",
    "\n",
    "pp.pprint(json_contents)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
