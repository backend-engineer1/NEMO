{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "Text_Normalization_Tutorial.ipynb",
      "private_outputs": true,
      "provenance": [],
      "collapsed_sections": [
        "lcvT3P2lQ_GS"
      ],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.9"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "x0DJqotopcyb"
      },
      "source": [
        "\"\"\"\n",
        "You can run either this notebook locally (if you have all the dependencies and a GPU) or on Google Colab.\n",
        "\n",
        "Instructions for setting up Colab are as follows:\n",
        "1. Open a new Python 3 notebook.\n",
        "2. Import this notebook from GitHub (File -> Upload Notebook -> \"GITHUB\" tab -> copy/paste GitHub URL)\n",
        "3. Connect to an instance with a GPU (Runtime -> Change runtime type -> select \"GPU\" for hardware accelerator)\n",
        "4. Run this cell to set up dependencies.\n",
        "\"\"\"\n",
        "# If you're using Google Colab and not running locally, run this cell\n",
        "\n",
        "# install NeMo\n",
        "BRANCH = 'main'\n",
        "!python -m pip install git+https://github.com/NVIDIA/NeMo.git@$BRANCH#egg=nemo_toolkit[nlp]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CH7yR7cSwPKr"
      },
      "source": [
        "import os\n",
        "import wget\n",
        "import nemo_text_processing\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F-IrnmXMTevr"
      },
      "source": [
        "# Task Description\n",
        "\n",
        "Text normalization (TN) is a part of the Text-To-Speech (TTS) pre-processing pipeline. It could also be used for pre-processing Automatic Speech Recognition (ASR) training transcripts.\n",
        "\n",
        "TN is the task of converting text in written form to its spoken form to improve TTS. For example, `10:00` should be changed to `ten o'clock` and `10kg` to `ten kilograms`."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xXRARM8XtK_g"
      },
      "source": [
        "# NeMo Text Normalization\n",
        "\n",
        "NeMo TN is based on Python Regex.\n",
        "\n",
        "Currently, NeMo TN provides support for English and the following semiotic classes from the [Google Text normalization dataset](https://www.kaggle.com/richardwilliamsproat/text-normalization-for-english-russian-and-polish):\n",
        "DATE, CARDINAL, MEASURE, DECIMAL, ORDINAL, MONEY, TIME, PLAIN. We additionally added the class `WHITELIST` for all whitelisted tokens whose verbalizations are directly looked up from a user-defined list.\n",
        "\n",
        "The toolkit is modular. The rule-based system is divided into a tagger and a verbalizer following  [Google's Kestrel](https://www.researchgate.net/profile/Richard_Sproat/publication/277932107_The_Kestrel_TTS_text_normalization_system/links/57308b1108aeaae23f5cc8c4/The-Kestrel-TTS-text-normalization-system.pdf) design: the tagger is responsible for detecting and classifying semiotic classes in the underlying text, the verbalizer takes the output of the tagger and carries out the normalization. \n",
        "In the example `The alarm goes off at 10:30 a.m.`, the tagger for TIME detects `10:30 a.m.` as a valid time data with `hour=10`, `minutes=30`, `suffix=a.m.`, the verbalizer then turns this into `ten thirty a m`.\n",
        "\n",
        "\n",
        "This tool offers prediction on text files and evaluation on [Google Text normalization dataset](https://www.kaggle.com/richardwilliamsproat/text-normalization-for-english-russian-and-polish). It reaches 81% in sentence accuracy on the first file of `output-00001-of-00100` of Google text normalization dataset, 97.4% in token accuracy.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-IT1Xr9iW2Xr"
      },
      "source": [
        "# Quick Start\n",
        "\n",
        "## Add TN to your Python TTS pre-processing workflow\n",
        "\n",
        "TN is a part of the `nemo_text_processing` package which is installed with `nemo_toolkit`. Installation instructions could be found [here](https://github.com/NVIDIA/NeMo/tree/main/README.rst)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Bfs7fa9lXDDh"
      },
      "source": [
        "from nemo_text_processing.text_normalization.normalize import normalize\n",
        "\n",
        "raw_text = \"we paid $123 for this desk.\"\n",
        "normalize(raw_text, verbose=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w5sX0SXbXoZp"
      },
      "source": [
        "In the above cell, `$123` would be converted to `one hundred twenty three dollars`, and the rest of the words remain the same.\n",
        "\n",
        "## Run Text Normalization on an input from a file\n",
        "\n",
        "Use `run_predict.py` to convert a spoken text from a file `INPUT_FILE` to a written format and save the output to `OUTPUT_FILE`. Under the hood, `run_predict.py` is calling `normalize()` (see the above section)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UD-OuFmEOX3T"
      },
      "source": [
        "# If you're running the notebook locally, update the NEMO_TEXT_PROCESSING_PATH below\n",
        "# In Colab, a few required scripts will be downloaded from NeMo github\n",
        "\n",
        "NEMO_TOOLS_PATH = '<UPDATE_PATH_TO_NeMo_root>/nemo_text_processing/text_normalization'\n",
        "DATA_DIR = 'data_dir'\n",
        "os.makedirs(DATA_DIR, exist_ok=True)\n",
        "\n",
        "if 'google.colab' in str(get_ipython()):\n",
        "    NEMO_TOOLS_PATH = '.'\n",
        "\n",
        "    required_files = ['run_predict.py',\n",
        "                      'run_evaluate.py']\n",
        "    for file in required_files:\n",
        "        if not os.path.exists(file):\n",
        "            file_path = 'https://raw.githubusercontent.com/NVIDIA/NeMo/' + BRANCH + '/nemo_text_processing/text_normalization/' + file\n",
        "            print(file_path)\n",
        "            wget.download(file_path)\n",
        "elif not os.path.exists(NEMO_TOOLS_PATH):\n",
        "      raise ValueError(f'update path to NeMo root directory')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d4T0gXHwY3JZ"
      },
      "source": [
        "INPUT_FILE = f'{DATA_DIR}/test.txt'\n",
        "OUTPUT_FILE = f'{DATA_DIR}/test_tn.txt'\n",
        "\n",
        "! echo \"The alarm went off at 10:00.\" > $DATA_DIR/test.txt\n",
        "! cat $INPUT_FILE\n",
        "! python $NEMO_TOOLS_PATH/run_predict.py --input=$INPUT_FILE --output=$OUTPUT_FILE"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F5wSJTI8ZFRg"
      },
      "source": [
        "# check that the raw text was converted to the spoken form\n",
        "! cat $OUTPUT_FILE"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RMT5lkPYzZHK"
      },
      "source": [
        "## Run evaluation\n",
        "The data for Evaluation needs to be segmented and labeled by semiotic class, following the format of [Google Text normalization dataset](https://www.kaggle.com/richardwilliamsproat/text-normalization-for-english-russian-and-polish).\n",
        "That is, every line of the file needs to have the format `<semiotic class>\\t<unnormalized text>\\t<self>` if it's trivial class or `<semiotic class>\\t<unnormalized text>\\t<normalized text>` in case of a semiotic class.\n",
        "\n",
        "\n",
        "We will create a simple example file to show how evaluation works:\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u4zjeVVv-UXR"
      },
      "source": [
        "eval_text =  \"\"\"LETTERS\\tA & E\\ta and e\n",
        "PUNCT\\t.\\tsil\n",
        "PLAIN\\tRetrieved\\t<self>\n",
        "DATE\\t2006-08-05\\tthe fifth of august two thousand six\n",
        "PUNCT\\t.\\tsil\n",
        "<eos>\\t<eos>\n",
        "PLAIN\\tDownloaded\\t<self>\n",
        "PLAIN\\ton\\t<self>\n",
        "DATE\\t7 August 2007\\tthe seventh of august two thousand seven\n",
        "PUNCT\\t.\\tsil\"\"\"\n",
        "INPUT_FILE_EVAL = f\"{DATA_DIR}/test_eval.txt\"\n",
        "with open(INPUT_FILE_EVAL, 'w') as fp:\n",
        "  fp.write(eval_text)\n",
        "! cat $INPUT_FILE_EVAL"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G7_5oXpObizP"
      },
      "source": [
        "! python $NEMO_TOOLS_PATH/run_evaluate.py --input=$INPUT_FILE_EVAL"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bIvKBwRcH_9W"
      },
      "source": [
        "`run_evaluate.py` call will output both **sentence level** and **token level** accuracies. \n",
        "For our example, the expected output is the following:\n",
        "\n",
        "```\n",
        "Loading training data: test_eval.text\n",
        "Sentence level evaluation...\n",
        "- Data: 1 sentences\n",
        "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 578.76it/s]\n",
        "- Normalized. Evaluating...\n",
        "- Accuracy: 1.0\n",
        "Token level evaluation...\n",
        "- Token type: LETTERS\n",
        "  - Data: 1 tokens\n",
        "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 2549.73it/s]\n",
        "  - Normalized. Evaluating...\n",
        "  - Accuracy: 1.0\n",
        "- Token type: PUNCT\n",
        "  - Data: 3 tokens\n",
        "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 3/3 [00:00<00:00, 3933.39it/s]\n",
        "  - Normalized. Evaluating...\n",
        "  - Accuracy: 1.0\n",
        "- Token type: PLAIN\n",
        "  - Data: 3 tokens\n",
        "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 3/3 [00:00<00:00, 3576.72it/s]\n",
        "  - Normalized. Evaluating...\n",
        "  - Accuracy: 1.0\n",
        "- Token type: DATE\n",
        "  - Data: 2 tokens\n",
        "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 1891.46it/s]\n",
        "  - Normalized. Evaluating...\n",
        "  - Accuracy: 1.0\n",
        "- Accuracy: 1.0\n",
        " - Total: 9 \n",
        "\n",
        "Class      | Num Tokens | nemo \n",
        "sent level | 1          | 1.0  \n",
        "PLAIN      | 3          | 1.0  \n",
        "PUNCT      | 3          | 1.0  \n",
        "DATE       | 2          | 1.0  \n",
        "CARDINAL   | 0          | 0    \n",
        "LETTERS    | 1          | 1.0  \n",
        "VERBATIM   | 0          | 0    \n",
        "MEASURE    | 0          | 0    \n",
        "DECIMAL    | 0          | 0    \n",
        "ORDINAL    | 0          | 0    \n",
        "DIGIT      | 0          | 0    \n",
        "MONEY      | 0          | 0    \n",
        "TELEPHONE  | 0          | 0    \n",
        "ELECTRONIC | 0          | 0    \n",
        "FRACTION   | 0          | 0    \n",
        "TIME       | 0          | 0    \n",
        "ADDRESS    | 0          | 0 \n",
        "\n",
        "```\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lcvT3P2lQ_GS"
      },
      "source": [
        "# Notes\n",
        "\n",
        "The current system expects well-formed sentences and word boundaries. The default expects a semiotic token to be surrounded by a non-word token. E.g. `A & E` will be detected as `VERBATIM`, however `A&E` will not be detected due to missing spaces around `&`. As an exercise, adjust the word boundary definition in [nemo_text_processing/text_normalization/tagger.py](https://github.com/NVIDIA/NeMo/blob/main/nemo_text_processing/text_normalization/tagger.py) to accommodate this too."
      ]
    }
  ]
}